---
title: "Welcome to Machine Learning Algorithms"
---

This website is a comprehensive resource for machine learning algorithms. Here, you will find explanations of the most popular algorithms, as well as code implementations in Python.

The website is divided into two main sections: supervised learning and unsupervised learning. Supervised learning algorithms are used to learn from labeled data, while unsupervised learning algorithms are used to learn from unlabeled data.

Within each section, the algorithms are further divided into regression, classification, and clustering. Regression algorithms are used to predict continuous values, while classification algorithms are used to predict discrete values. Clustering algorithms are used to group data points together.

Each algorithm page includes the following information:

* A brief overview of the algorithm
* The math behind the algorithm
* A Python implementation of the algorithm
* Examples of how the algorithm can be used

I hope you find this website to be a valuable resource for learning about machine learning algorithms.

**Here is a more detailed overview of the algorithms covered on this website:**

### Unsupervised Learning

* Representation learning
    * [Principal component analysis (PCA)](/pages/PCA.html): PCA is a dimensionality reduction algorithm that projects data points onto a lower-dimensional subspace while preserving as much of the variance as possible.
    * [Kernel PCA](/pages/Kernel_PCA.html): Kernel PCA is a generalization of PCA that can be used to project data points onto a nonlinear subspace.
* Clustering
    * [K-means clustering](/pages/kmeans.html): K-means clustering is a simple and efficient algorithm for clustering data points into a predefined number of clusters.
    * Kernel K-means clustering: Kernel K-means clustering is a generalization of K-means clustering that can be used to cluster data points into nonlinear clusters.
* Estimation
    * [Maximum likelihood estimation (MLE)](/pages/mle.html): MLE is a statistical method for estimating the parameters of a probability distribution from data.
    * [Bayesian estimation](/pages/bayes_est.html): Bayesian estimation is a statistical method for estimating the parameters of a probability distribution using prior knowledge about the parameters.
    * [Gaussian mixture model (GMM)](/pages/gmm.html): GMM is a probabilistic model that can be used to represent data that is generated by a mixture of Gaussian distributions.
    * [Expectation-maximization (EM) algorithm](/pages/em.html): EM is an iterative algorithm that can be used to fit a GMM to data.
    
### Supervised Learning

* Regression
    * [Least squares regression](/pages/lin_reg.html): Least squares regression is a linear regression model that minimizes the sum of the squares of the errors between the predicted values and the observed values.
    * [Bayesian view of least squares regression](/pages/klin_reg.html): The Bayesian view of least squares regression provides a probabilistic interpretation of the model.
    * [Ridge regression](/pages/bayes_lin_reg.html): Ridge regression is a linear regression model that penalizes the model coefficients to prevent overfitting.
    * [LASSO regression](/pages/lasso_lin_reg.html): LASSO regression is a linear regression model that shrinks the model coefficients to zero to prevent overfitting.
* Classification
    * [K-nearest neighbors (KNN)](/pages/knn.html): KNN is a non-parametric classification algorithm that classifies a new data point by finding the K nearest neighbors of the data point and then assigning the new data point to the class that is most common among its neighbors.
    * [Decision tree](/pages/dec_trees.html): Decision trees are a type of supervised learning algorithm that can be used for both classification and regression tasks. Decision trees are constructed by recursively partitioning the data space until all of the data points in each leaf node belong to the same class.
* Generative models
    * [Naive Bayes](/pages/naive_bayes.html): Naive Bayes is a simple probabilistic classification algorithm that assumes that the features of a data point are independent of each other.
* Discriminative models
    * [Perceptron](/pages/percp.html): The perceptron model is a discriminative model that uses a linear decision boundary to classify data points into different categories.
    * [Logistic regression](/pages/log_reg.html): Logistic regression is a probabilistic classification algorithm that is used to model the probability of a binary outcome.
    * [Support vector machines (SVMs)](/pages/svm.html): SVMs are a type of supervised learning algorithm that can be used for both classification and regression tasks. SVMs are constructed by finding a hyperplane that separates the data points into two classes.
* Ensemble methods
    * [Bagging](/pages/bag.html): Bagging is an ensemble method that combines the predictions of multiple models to improve the overall accuracy of the predictions.
    * [Boosting](/pages/boost.html): Boosting is an ensemble method that trains a sequence of models, each of which is trained to correct the mistakes of the previous models.
* [Artificial neural networks (ANNs)](/pages/ann.html): ANNs are a type of machine learning algorithm that is inspired by the human brain. ANNs are composed of artificial neurons that are connected to each other in a network. Each neuron receives input from other neurons, and it outputs a signal that is determined by a nonlinear function of the input. ANNs can be used to solve a variety of problems, including classification, regression, and time series forecasting.

## Credits

I would like to express my gratitude to Professor Arun Rajkumar for his valuable content and notations, which have greatly influenced my understanding. Additionally, I would like to acknowledge the contribution of IIT Madras, where I had the opportunity to learn from Professor Arun Rajkumar in the Machine Learning Techniques Course.