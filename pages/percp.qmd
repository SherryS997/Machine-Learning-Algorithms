---
title: "Perceptron"
---

The **Perceptron Learning Algorithm** is a type of supervised learning algorithm used for binary classification tasks. It involves iteratively adjusting the weights of a linear combination of input features until a decision boundary that separates the two classes is found. The algorithm is a type of discriminative classification as it directly models the boundary between classes rather than modeling the underlying probability distribution of each class.

Let $D=\{(x_1, y_1), \ldots, (x_n,y_n)\}$ be the dataset, where $x_i \in \mathbb{R}^d$ and $y_i \in \{0, 1\}$.

Assumptions made in the algorithm:

1. $P(y=1|x) = 1$ if $w^Tx\ge0$, $0$ otherwise.
2. **Linear Separability Assumption**: The Linear Separability Assumption is the assumption made in some machine learning algorithms, including the Perceptron Learning Algorithm, that the classes to be classified can be separated by a linear decision boundary. This means that there exists a hyperplane in the feature space that can separate the data points of the two classes.

The objective function is given by,
$$
\min _{h \in \mathcal{H}} \sum _{i=1} ^n \mathbb{1}\left ( h(x_i) \ne y_i \right )
$$
In general, this is an NP-Hard Problem even if $\mathcal{H}$ only accounts for the Linear Hypotheses.

Under the Linear Separability Assumption, let $\exists w \in \mathbb{R}^d$ s.t. $\text{sign}(w^Tx_i)=y_i$ $\forall i \in [n]$.

We solve this problem of convergence using an iterative algorithm. The algorithm is as follows:

* Assign $w^0 = 0 \in \mathbb{R}^d$
* Until Convergence:
	* Pick $(x_i, y_i)$ pair from the dataset
	* if $\text{sign}(w^Tx_i)==y_i$
		* do nothing
	* else
		* $w^{(t+1)} = w^t + x_iy_i \hspace{2em} \leftarrow$ update rule.
	* end

## Analysis of the Update Rule 
For a training example $(x, y)$, where $x$ is the input and $y$ is the correct output (either $1$ or $-1$), the perceptron algorithm updates its weight vector $w$ as follows:

* If the prediction of the perceptron on $x$ is correct (i.e., $\text{sign}(w^Tx_i)==y_i$), then no update is performed.
* If the prediction of the perceptron on $x$ is incorrect (i.e., $\text{sign}(w^Tx_i)\ne y_i$), then the weights are updated by adding the product of the input vector and the correct output to the current weight vector: $w^{(t+1)} = w^t + x_iy_i$.

This update rule effectively moves the decision boundary in the direction of the correct classification for the misclassified example. It is guaranteed to converge to a linearly separable solution if the data is linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution.

\newpage
## Further Assumptions
We make three further assumptions:

1. **Linear Separability with $\gamma$-Margin**: A dataset $D=\{(x_1, y_1), \ldots, (x_n,y_n)\}$ is linearly separable with $\gamma$-margin if $\exists w^* \in \mathbb{R}^d$ s.t. $(w^{*T}x_i)y_i\ge\gamma$ $\forall i$ for some $\gamma>0$.

![Linear Separability with $\gamma$-Margin](../images/Linear Separability with gamma-Margin.png)

2. **Radius Assumption**: Let some $R>0 \in \mathbb{R}$, $\forall i \in D$ $||x_i||\le R$. In short, let $R$ be the length of the datapoint furthest from the center.
3. **Normal Length for $w^*$**: Let $w^*$ be of unit length.

# Proof of Convergence of Perceptron Algorithm
Let $l$ denote the current mistake number. Using everything upto now, we can see,
\begin{align*}
w^{l+1} &= w^l + xy \\
||w^{l+1}||^2&=||w^l + xy||^2 \\
&=(w^l + xy)^T(w^l + xy) \\
&=||w^l||^2 + 2(w^{lT}x)y+ ||x||^2y^2 \\
\therefore ||w^{l+1}||^2&\le ||w^l||^2 + R^2 \\
&\le (||w^{l-1}||^2 + R^2)  + R^2 \\
&\le ||w^0||^2 + lR^2 \\
\therefore ||w^{l+1}||^2&\le lR^2 \hspace{6em} \ldots[1]
\end{align*}
We can also see,
\begin{align*}
w^{l+1} &= w^l + xy \\
(w^{l+1})^Tw^* &= (w^l + xy)^Tw^* \\
&= w^{lT}w^* + (w^{*T}x)y \\
\therefore (w^{l+1})^Tw^* &\ge w^{lT}w^* + \gamma \\
&\ge (w^{l-1T}w^* + \gamma) + \gamma \\
&\ge w^{0T}w^* + l\gamma \\
\therefore (w^{l+1})^Tw^* &\ge l\gamma \\
((w^{l+1})^Tw^*)^2 &\ge l^2\gamma^2 \\
||w^{l+1}||^2||w^*||^2 &\ge l^2\gamma^2 \hspace{0.5em}\ldots\text{Using Cauchy-Schwartz}\\
\therefore ||w^{l+1}||^2 &\ge l^2\gamma^2 \hspace{6em} \ldots[2]
\end{align*}
Therefore, from $[1]$ and $[2]$, we get,
\begin{align*}
l^2\gamma^2 &\le ||w^{l+1}||^2 \le lR^2 \\
l^2\gamma^2 &\le  lR^2 \\
\therefore l &\le \frac{R^2}{\gamma^2}
\end{align*}
Hence, from the above equation, we can conclude that for a dataset that follows Linear Separability with $\gamma$-Margin, and has a finite Radius $R$, the perceptron algorithm has an upper bound for the number of mistakes.

Therefore, perceptron converges.