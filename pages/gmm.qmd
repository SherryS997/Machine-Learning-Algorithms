---
title: "Gaussian mixture model (GMM)"
---

Gaussian Mixture Models are a type of probabilistic model used to represent complex data distributions by combining multiple Gaussian distributions.

The procedure is as follows:

* Step 1: Generate a mixture component among $\{1, 2, \ldots, K\}$ where $z_i \in \{1, 2, \ldots, K\}$. We obtain,
$$
P(z_i=k) = \pi_k \hspace{2em} \left [ \sum _{i=1} ^K \pi_i = 1 \hspace{1em} 0 \le \pi_i \le 1 \hspace{1em} \forall i \right ]
$$
* Step 2: Generate $\mathbf{x}_i \sim \mathcal{N}(\boldsymbol{\mu}_{z_i}, \boldsymbol{\sigma}^2_{z_i})$

Hence, there are $3K$ parameters. However, since $\displaystyle \sum _{i=1} ^K \pi_i = 1$, the number of parameters to be estimated becomes $3K-1$ for a GMM with $K$ components.

# Likelihood of GMM's
\begin{align*}
\mathcal{L}\left( \begin{array}{cccc}
\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \ldots, \boldsymbol{\mu}_K \\
\boldsymbol{\sigma}^2_1, \boldsymbol{\sigma}^2_2, \ldots, \boldsymbol{\sigma}^2_K\\
\pi_1, \pi_2, \ldots, \pi_K
\end{array}; \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \right )
&= \prod _{i=1} ^n f_{\text{mix}} \left( \mathbf{x}_i; \begin{array}{cccc}
\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \ldots, \boldsymbol{\mu}_K \\
\boldsymbol{\sigma}^2_1, \boldsymbol{\sigma}^2_2, \ldots, \boldsymbol{\sigma}^2_K\\
\pi_1, \pi_2, \ldots, \pi_K
\end{array} \right ) \\
&= \prod _{i=1} ^n \left [ \sum _{k=1} ^K \pi_k * f_{\text{mix}}(\mathbf{x}_i; \boldsymbol{\mu}_k, \boldsymbol{\sigma}_k) \right ] \\
\therefore \log\mathcal{L}(\boldsymbol{\theta}) &= \sum _{i=1} ^n \log \left [ \sum _{k=1} ^K \pi_k * \frac{1}{\sqrt{2\pi}\boldsymbol{\sigma}_k} e^{\frac{-(\mathbf{x}_i-\boldsymbol{\mu}_k)^2}{2\boldsymbol{\sigma}^2_k}} \right ] \\
\end{align*}
To solve the above equation, we need to understand convexity.

