---
title: "Linear Regression"
---

Linear regression is a supervised learning algorithm used to predict a continuous output variable based on one or more input features, assuming a linear relationship between the input and output variables. The goal of linear regression is to find the line of best fit that minimizes the sum of squared errors between the predicted and actual output values.

Given a dataset $\{x_1, \ldots, x_n\}$ where $x_i \in \mathbb{R}^d$, let $\{y_1, \ldots, y_n\}$ be the labels, where $y_i \in \mathbb{R}$.

The goal of linear regression is to find the mapping between input and output variables. i.e.
$$
h: \mathbb{R}^d \rightarrow \mathbb{R}
$$
The error for the above mapping function is given by,
$$
\text{error}(h) = \sum _{i=1} ^n (h(x_i) - y_i) ^2)
$$
This error can be as small as zero, and this is achieved when $h(x_i)=y_i \hspace{0.5em} \forall i$. But this may not be the desired output as it may represent nothing more than memorizing the data and its outputs.

The memorization problem can be mitigated by introducing a structure to the mapping. The simplest structure being of the linear kind, we shall use it as the underlying structure for our data.

Let $\mathcal{H}_{\text{linear}}$ represent the solution space for the mapping in the linear space.
$$
\mathcal{H}_{\text{linear}}=\Biggr \lbrace{h_w: \mathbb{R}^d \rightarrow \mathbb{R} \hspace{0.5em} s.t. \hspace{0.5em} h_w(x)=w^Tx \hspace{0.5em} \forall w \in \mathbb{R}^d } \Biggr \rbrace
$$
Therefore, our goal is,
\begin{align*}
\min _{h \in \mathcal{H}_{\text{linear}}} \sum _{i=1} ^n (h(x_i) - y_i) ^2 \\
\text{Equivalently} \\
\min _{w \in \mathbb{R}^d} \sum _{i=1} ^n (w^Tx_i - y_i) ^2
\end{align*} 
Optimizing the above is the entire point of the linear regression algorithm.

# Optimizing the Error Function
The minimization equation can be rewritten in the vectorized form as,
$$
\min _{w \in \mathbb{R}^d} || X^Tw - y || ^2 _2
$$
Let this be a function of $w$ and as follows:
\begin{align*}
f(w) &= \min _{w \in \mathbb{R}^d} || X^Tw - y || ^2 _2 \\
f(w) &= (X^Tw - y)^T(X^Tw - y) \\
\therefore\bigtriangledown f(w) &= 2(XX^T)w - 2(Xy) \\
\end{align*}
Setting the above equation to zero, we get
\begin{align*}
(XX^T)w^* &= Xy \\
\therefore w^* &= (XX^T)^+Xy
\end{align*}
where $(XX^T)^+$ is the pseudo-inverse of $XX^T$.

On further analysis of the above equation, we can say that $X^Tw$	 is the projection of the labels onto the subspace spanned by the features.

#### Implementing Linear Regression Algorithm (Closed Form) in Python

We will be using the `FuelConsumption.csv` dataset.

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.read_csv("FuelConsumption.csv")

# take a look at the dataset
df.head()
```
::: {.callout-note collapse="true" appearance="simple"}
# Output

| MODELYEAR | MAKE | MODEL | VEHICLECLASS | ENGINESIZE | CYLINDERS | TRANSMISSION | FUELTYPE | FUELCONSUMPTION_CITY | FUELCONSUMPTION_HWY | FUELCONSUMPTION_COMB | FUELCONSUMPTION_COMB_MPG | CO2EMISSIONS |
|-----------|------|-------|--------------|------------|-----------|--------------|----------|-----------------------|----------------------|-----------------------|--------------------------|--------------|
| 2014      | ACURA | ILX   | COMPACT      | 2.0        | 4         | AS5          | Z        | 9.9                   | 6.7                  | 8.5                   | 33                       | 196          |
| 2014      | ACURA | ILX   | COMPACT      | 2.4        | 4         | M6           | Z        | 11.2                  | 7.7                  | 9.6                   | 29                       | 221          |
| 2014      | ACURA | ILX HYBRID   | COMPACT      | 1.5        | 4         | AV7          | Z        | 6.0                   | 5.8                  | 5.9                   | 48                       | 136          |
:::
Let's select some features to explore more.
```python
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
cdf.head()
```
::: {.callout-note collapse="true" appearance="simple"}
# Output

| ENGINESIZE | CYLINDERS | FUELCONSUMPTION_COMB | CO2EMISSIONS |
|------------|-----------|----------------------|--------------|
| 2.0        | 4         | 8.5                  | 196          |
| 2.4        | 4         | 9.6                  | 221          |
| 1.5        | 4         | 5.9                  | 136          |
| 3.5        | 6         | 11.1                 | 255          |
| 3.5        | 6         | 10.6                 | 244          |
:::

Let's split the dataset into features and labels and convert them into numpy arrays for computation.
```python
X = np.asanyarray(cdf[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB', 'Ones']]).T
y = np.asanyarray(cdf[['CO2EMISSIONS']])

X.shape, y.shape
```
::: {.callout-note appearance="minimal"}
((4, 825), (825, 1))
:::
Let compute the $w$ vector and print the coefficients and intercept.
```python
w = np.linalg.pinv(X@X.T)@X@y

print ('Coefficients: ', w.reshape((-1,))[:3])
print ('Intercept: ', w.reshape((-1,))[-1])
```
::: {.callout-note appearance="minimal"}
Coefficients:  [10.59881197, 7.4005805, 9.48207228]

Intercept:  67.82612794918894
:::

# Gradient Descent
The normal equation for linear regression, as seen before, needs the calculation of $(XX^T)^+$ which can computationally be of $O(d^3)$ complexity. 

As we know $w^*$ is the solution of an unconstrained optimization problem, we can solve it using gradient descent. It is given by,
\begin{align*}
w^{t+1} &= w^t - \eta^t \bigtriangledown f(w^t) \\
\therefore w^{t+1} &= w^t - \eta^t \left [ 2(XX^T)w^t - 2(Xy) \right ]
\end{align*}
where $\eta$ is a scalar used to control the step-size of the descent and $t$ is the current iteration.

#### Implementing Gradient Descent in Python

Let compute the $w$ vector using Gradient Descent and print the coefficients and intercept.
```python
w = np.ones((X.shape[0], 1))
eta = 1e-6
t = 100000

for i in range(t):
    grad = 2*(X @ X.T @ w) - 2*(X @ y)
    w = w - eta*grad
    
print ('Coefficients: ', w.reshape((-1,))[:3])
print ('Intercept: ', w.reshape((-1,))[-1])
```
::: {.callout-note appearance="minimal"}
Coefficients:  [10.85347843, 7.51750331, 9.59591605]

Intercept:  65.21825985969691
:::

Even in the above equation, the calculation of $XX^T$ is needed which is a computational expensive task. Is there a way to adapt this further?

## Stochastic Gradient Descent
Stochastic gradient descent (SGD) is an optimization algorithm used in machine learning for finding the parameters that minimize the loss function of a model. In contrast to traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters based on a randomly selected subset of the data, known as a batch. This results in faster training times and makes SGD particularly useful for large datasets.

For every step $t$, rather than updating $w$ using the entire dataset, we use a small randomly selected ($k$) data points to update $w$. Therefore, the new gradient is $2(\tilde{X}\tilde{X}^Tw^t - \tilde{X}\tilde{y})$ where $\tilde{X}$ and $\tilde{y}$ is the small sample randomly selected from the dataset. This is manageable because $\tilde{X} \in \mathbb{R}^{d \times k}$ which is considerably smaller than $X$.

After T rounds, we use,
$$
w ^T _{SGD} = \frac{1}{T}  \sum _{i=1} ^T w^i
$$
This has a certain guarantee to have optimal convergence partly because of the randomness involved in it.

#### Implementing Stochastic Gradient Descent in Python

Let compute the $w$ vector using Stochastic Gradient Descent and print the coefficients and intercept.
```python
w = np.ones((X.shape[0], 1))
eta = 1e-4
t = 100000
n = X.shape[1]
b = 10

for i in range(t):
    # randomly select a batch of samples
    idx = np.random.choice(n, b, replace=False)
    X_b = X[:, idx]
    y_b = y[idx]
    # compute the gradient for the batch
    grad = 2*(X_b @ X_b.T @ w) - 2*(X_b @ y_b)
    # update the weights
    w = w - eta*grad
    
print ('Coefficients: ', w.reshape((-1,))[:3])
print ('Intercept: ', w.reshape((-1,))[-1])
```
::: {.callout-note appearance="minimal"}
Coefficients:  [10.72000912, 8.3366805, 9.13970723]

Intercept:  65.2366350549217
:::

# Kernel Regression
What if the data points lie in a non-linear sub-space? Just like in the case of clustering non-linear data, we will use kernel functions here too.

Let $w^* = X\alpha^*$ for some $\alpha^* \in \mathbb{R}^d$.
\begin{align*}
X\alpha^*&=w^*\\
\therefore X\alpha^* &=(XX^T)^+Xy \\
(XX^T)X\alpha^* &=(XX^T)(XX^T)^+Xy \\
(XX^T)X\alpha^* &=Xy \\
X^T(XX^T)X\alpha^* &=X^TXy \\
(X^TX)^2\alpha^* &=X^TXy \\
K^2\alpha^* &=Ky \\
\therefore \alpha^* &=K^{-1}y
\end{align*}
where $K \in \mathbb{R}^{n \times n}$ and $K$ can be obtained using a kernel function like the Polynomial Kernel or RBF Kernel.

How to predict using alpha and the kernel function? Let $X_{test} \in R^{d \times m}$ be the test dataset. We predict by,
\begin{align*}
w^*\phi(X_{test}) &=  \sum _{i=1} ^n \alpha_i^* k(x_i, x_{test_i})
\end{align*}
where $\alpha_i^*$ gives the importance of the $i^{th}$ datapoint towards $w^*$ and $k(x_i, x_{test_i})$ shows how similar $x_{test_i}$ is to $x_i$.

# Probabilistic View of Linear Regression
Given a dataset $\{x_1, \ldots, x_n\}$ where $x_i \in \mathbb{R}^d$, let $\{y_1, \ldots, y_n\}$ be the labels, where $y_i \in \mathbb{R}$. The probability of $y_i$ given $x_i$ is given by,
$$
P(y|X) \sim w^TX + \epsilon
$$
where $w \in R^d$ is an unknown but fixed value and $\epsilon$ is the noise corresponding to distribution $\mathcal{N}(0, \sigma^2)$.

Because of the probabilistic nature of this problem, this can be viewed as an estimation problem and the best solution can be approached using Maximum Likelihood. This is given by,
\begin{align*}
\mathcal{L}\left(w; \hspace{0.5em} \begin{array}{cccc}
x_1, x_2, \ldots, x_n \\
y_1, y_2, \ldots, y_n\\
\end{array} \right )
&= \prod _{i=1} ^n \left [ \frac{1}{\sqrt{2\pi}\sigma}  e^{\frac{-(w^Tx_i - y_i)^2}{2\sigma^2}} \right ] \\
\log\mathcal{L}\left(w; \hspace{0.5em} \begin{array}{cccc}
x_1, x_2, \ldots, x_n \\
y_1, y_2, \ldots, y_n\\
\end{array} \right )
&= \sum _{i=1} ^n \left [ \frac{-(w^Tx_i - y_i)^2}{2\sigma^2} + \log \left ( \frac{1}{\sqrt{2\pi}\sigma} \right ) \right ]
\end{align*}
$$
\text{Taking gradients w.r.t. } w \text{ on both sides}
$$
$$
\hat{w}_{ML} = w^* = (XX^T)^+Xy
$$
Therefore, Maximum Likelihood Estimator assuming Zero mean Gaussian Noise is the same as linear regression with square error.