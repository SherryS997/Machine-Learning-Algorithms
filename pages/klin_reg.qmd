---
title: "Kernel Least squares regression"
---

What if the data points lie in a non-linear sub-space? Just like in the case of clustering non-linear data, we will use kernel functions here too.

Let $w^* = X\alpha^*$ for some $\alpha^* \in \mathbb{R}^d$.
\begin{align*}
X\alpha^*&=w^*\\
\therefore X\alpha^* &=(XX^T)^+Xy \\
(XX^T)X\alpha^* &=(XX^T)(XX^T)^+Xy \\
(XX^T)X\alpha^* &=Xy \\
X^T(XX^T)X\alpha^* &=X^TXy \\
(X^TX)^2\alpha^* &=X^TXy \\
K^2\alpha^* &=Ky \\
\therefore \alpha^* &=K^{-1}y
\end{align*}
where $K \in \mathbb{R}^{n \times n}$ and $K$ can be obtained using a kernel function like the Polynomial Kernel or RBF Kernel.

How to predict using alpha and the kernel function? Let $X_{test} \in R^{d \times m}$ be the test dataset. We predict by,
\begin{align*}
w^*\phi(X_{test}) &=  \sum _{i=1} ^n \alpha_i^* k(x_i, x_{test_i})
\end{align*}
where $\alpha_i^*$ gives the importance of the $i^{th}$ datapoint towards $w^*$ and $k(x_i, x_{test_i})$ shows how similar $x_{test_i}$ is to $x_i$.