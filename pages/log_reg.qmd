---
title: "Logistic regression"
---

## Sigmoid Function
Until now, we used the $\text{sign}$ function to get the class for the output. But can we also provide the probabilities for these outputs?

Let $z=w^Tx$ and $z \in \mathbb{R}$. How can we map $[-\infty, \infty]\rightarrow[0,1]$? For this, we use the **Sigmoid Function**. It is given by,
$$
g(z) = \frac{1}{1+e^{-z}}
$$

![Sigmoid Function](../images/sigmoid-function.png)

The sigmoid function is often used in machine learning as an activation function for neural networks. It has a characteristic S-shaped curve, which makes it useful for modeling processes that have a threshold or saturation point, such as logistic growth or binary classification problems.

When the input value is large and positive, the sigmoid function output approaches 1, and when the input value is large and negative, the sigmoid function output approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.

The term "sigmoid" comes from the Greek word "sigmoides," which means "shaped like the letter sigma" ($\Sigma$). The letter sigma has a similar shape to the sigmoid function's characteristic S-shaped curve, which is likely the reason for the function's name.

## Logistic Regression
Logistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables, which can be either continuous or categorical. The goal of logistic regression is to estimate the probability that the dependent variable is one of the two possible values, given the values of the independent variables.

In logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic(sigmoid) function, which produces an S-shaped curve that ranges between 0 and 1. The logistic function transforms the output of a linear combination of the independent variables into a probability estimate, which can then be used to classify new observations.

Let $D=\{(x_1, y_1), \ldots, (x_n,y_n)\}$ be the dataset, where $x_i \in \mathbb{R}^d$ and $y_i \in \{0, 1\}$.

We know,
$$
P(y=1|x) = g(w^Tx_i) = \frac{1}{1+e^{-w^Tx}}
$$
Using Maximum Likelihood, we get
\begin{align*}
\mathcal{L}(w;\text{Data}) &= \prod _{i=1} ^{n} (g(w^Tx_i))^{y_i}(1- g(w^Tx_i))^{1-y_i} \\
\log(\mathcal{L}(w;\text{Data})) &= \sum _{i=1} ^{n} y_i\log(g(w^Tx_i))+(1-y_i)\log(1- g(w^Tx_i)) \\
&= \sum _{i=1} ^{n} y_i\log\left(\frac{1}{1+e^{-w^Tx_i}}\right)+(1-y_i)\log\left(\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}}\right) \\
&= \sum _{i=1} ^{n} \left [ (1-y_i)(-w^Tx_i) - \log(1+e^{-w^Tx_i}) \right ]
\end{align*}
Therefore, our objective, which is maximizing the log-likelihood function, is given by,
$$
\max _{w}\sum _{i=1} ^{n} \left [ (1-y_i)(-w^Tx_i) - \log(1+e^{-w^Tx_i}) \right ]
$$
But, there is no closed form solution for this. And hence, we use gradient descent for convergence.

The gradient is given by,
\begin{align*}
\nabla \log(\mathcal{L}(w;\text{Data})) &= \sum _{i=1} ^{n} \left [ (1-y_i)(-x_i) - \left( \frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \right ) (-x_i) \right ] \\
&= \sum _{i=1} ^{n} \left [ -x_i + x_iy_i + x_i \left( \frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \right ) \right ] \\
&= \sum _{i=1} ^{n} \left [ x_iy_i - x_i \left( \frac{1}{1+e^{-w^Tx_i}} \right ) \right ] \\
\nabla \log(\mathcal{L}(w;\text{Data})) &= \sum _{i=1} ^{n} \left [ x_i \left(y_i - \frac{1}{1+e^{-w^Tx_i}} \right ) \right ]
\end{align*}
Using the Gradient Descent update rule, we get,
\begin{align*}
w_{t+1} &= w_t + \eta_t\nabla \log(\mathcal{L}(w;\text{Data})) \\
&= w_t + \eta_t  \left ( \sum _{i=1} ^{n} x_i \left(y_i - \frac{1}{1+e^{-w^Tx_i}} \right ) \right )
\end{align*}

### Kernel and Regularized Versions
We can argue that $w^*=\displaystyle\sum _{i=1} ^{n}\alpha_ix_i$, and therefore, can be Kernelized. For further details, refer to this [link.](https://cs229.stanford.edu/extra-notes/representer-function.pdf#section.2)

The regularized version is given by,
$$
\min _{w}\sum _{i=1} ^{n} \left [ \log(1+e^{-w^Tx_i}) + w^Tx_i(1-y_i) \right ] + \frac{\lambda}{2}||w||^2
$$
where $\frac{\lambda}{2}||w||^2$ is the regualizer and $\lambda$ is found using cross-validation.