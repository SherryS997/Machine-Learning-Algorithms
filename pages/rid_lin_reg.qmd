---
title: "Ridge regression"
---

Ridge regression is a type of linear regression that adds a penalty term to the ordinary least squares method to mitigate multicollinearity and overfitting.

Its objective function is given by,
$$
\min_{w\in \mathbb{R}^d} \sum^n_{i=1}(w^Tx_i-y_i)^2 + \lambda||w||_2^2
$$
where $\lambda||w||_2^2$ is the regularizer, and $||w||_2^2$ is the squared L2 Norm of $w$. Let this equation be given by $f(w)$.

Subsequently, this is also equivalent to,
$$
\min_{w\in \mathbb{R}^d} \sum^n_{i=1}(w^Tx_i-y_i)^2 \hspace{1em}\text{s.t.}||w||_2^2\le\theta
$$
where $\theta$ is dependent on $\lambda$.

In conclusion, for every choice of $\lambda>0$, $\exists \theta$ s.t. there are optimal solutions to our objective function.

The loss function of the linear regression of $w_{ML}$ is given by,
$$
f(w_{ML}) = \sum^n_{i=1}(w_{ML}^Tx_i-y_i)^2
$$
Consider the set of all $w$ s.t. $f(w_{ML}) = f(w) + c$ where $c>0$. This set is given by,
$$
S_c = \left \{w: f(w_{ML}) = f(w) + c \right \} 
$$
i.e. every $w \in S_c$ satisfies,
$$
||X^Tw-y||^2=||X^Tw_{ML}-y||^2 + c
$$
$$
\text{On Simplification, we get}
$$
$$
(w-w_{ML})^T(XX^T)(w-w_{ML}) = c'
$$
where $c'$ depends on $c,XX^T,$ and $w_{ML}$, but **not on** $w$.

![Pictoral Representation of what Ridge Regression does.](../images/ridge_regression.png)

**Conclusion**: Ridge Regression pushes feature values to zero but not necessarily zero.