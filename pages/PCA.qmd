---
title: "Principal Component Analysis"
---


# Representation Learning
Representation learning is a sub-field of machine learning that focuses on learning meaningful and compact representations of complex data for tasks such as dimensionality reduction, clustering, and classification.

Given a dataset $\{x_1, x_2, \ldots, x_n\}$ where $x_i \in \mathbb{R}^{d}$, we need to find a representation for it with the minimum reconstruction error.

Let $w$, where $||w||=1$, be the best linear representation of the dataset.

The representation is given by,
\begin{align*}
    \frac{(x_i^Tw)}{w^Tw}&w \\
    \text{But }||w||&=1\\
    \therefore \text{ Projection } &= (x_i^Tw)w
\end{align*}
The reconstruction error is given by,
$$
\text{Reconstruction Error}(f(w)) = \frac{1}{n} \sum _{i=1} ^{n} || x_i - (x_i^Tw)w || ^ 2
$$
where $x_i - (x_i^Tw)w$ is the residue and can be given by $x'$.

Our objective is to minimize the reconstruction error. Minimizing it, we get,
\begin{align*}
    \min _{w \in ||w|| = 1} f(w) &= \frac{1}{n} \sum _{i=1} ^{n} -(x_i^Tw)^2 \\
    \therefore \max _{w \in ||w|| = 1} f(w) &= \frac{1}{n} \sum _{i=1} ^{n} (x_i^Tw)^2 \\
    &= w^T(\frac{1}{n} \sum _{i=1} ^{n} x_ix_i^T)w \\
    \max _{w \in ||w|| = 1} f(w) &= w^TCw
\end{align*}
where $C=\displaystyle \frac{1}{n} \displaystyle \sum _{i=1} ^{n} x_ix_i^T$ is the Covariance Matrix and $C \in \mathbb{R}^{d \times d}$.

From the above equation we find that $w$ is the eigenvector corresponding to the largest eigenvalue $\lambda$ of $C$.

$w$ is also called the First Principal Component of the dataset.

## Potential Algorithm
With this information, we can give the following algorithm:

Given a dataset $\{x_1, x_2, \ldots, x_n\}$ where $x_i \in \mathbb{R}^{d}$,

* Center the dataset
$$
\mu = \frac{1}{n} \sum _{i=1} ^{n} x_i
$$
$$
x_i = x_i - \mu  \hspace{2em} \forall i
$$
* Find the best representation $w \in \mathbb{R}^d$ and $||w|| = 1$.
* Replace $x_i = x_i - (x_i^Tw)w  \hspace{1em} \forall i$
* Repeat the first three steps until residues become zero and we obtain $w_2, w_3, \ldots, w_d$.

But is this the best way? How many $w$ do we need for optimal compression?

# Principal Component Analysis
Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by identifying the most important features, or principal components, that explain the maximum amount of variance in the data. PCA accomplishes this by transforming the original dataset into a new set of variables that are uncorrelated and ordered by their importance in explaining the variance in the data. This can be helpful for visualizing high-dimensional data or for preprocessing data before performing machine learning tasks. 

Following the potential algorithm from above, using the $\{w_1, w_2, \ldots, w_d\}$, we get,
$$
\forall i \hspace{1em} x_i - ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + \ldots +(x_i^Tw_d)w_d) = 0
$$
$$
\therefore x_i = (x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + \ldots +(x_i^Tw_d)w_d
$$
From the above equation, we can say that we can represent the data using $\{x_i^Tw_1, x_i^Tw_2, \ldots, x_i^Tw_d\}$, which are constants, and $\{w_1, w_2, \ldots, w_d\}$ which are vectors.

Therefore, data which was initially $d \times n$ can now be represented with $d (d + n)$ elements which doesn't seem great at first.

But if the data lies in a lower sub-space, the residues can be zero without the need for $d$ principal components.

If the data can be represented using only $k$ principal components, where $k << d$, the data now can be compressed from $d \times n$ to $k(d + n)$.

## Approximate Representation
If the data can be approximately represented by a lower sub-space, is it enough that we use only those $k$ projections? How much variance should be covered?

Given a centered dataset $\{x_1, x_2, \ldots, x_n\}$ where $x_i \in \mathbb{R}^{d}$, let $C$ be its covariance matrix, and let $\{\lambda_1, \lambda_2, \ldots, \lambda_d \}$ be its eigenvalues, which are non-negative because the covariance matrix is a positive semi-definite matrix, placed in descending order, and let $\{w_1, w_2, \ldots, w_d \}$ be its corresponding eigenvectors of unit length.

The eigen equation for the covariance matrix can be given by,
\begin{align*}
    Cw &= \lambda w \\
    w^TCw &= w^T\lambda w\\
    \therefore \lambda &= w^TCw \hspace{2em} \{w^Tw = 1\} \\
    \lambda &= \frac{1}{n} \sum _{i=1} ^{n} (x_i^Tw)^2 \\
\end{align*}
Therefore, as the mean is zero, $\lambda$ gives the variance captured by the eigenvector $w$.

A good rule of thumb is that the variance captured by P.C.A. should be at least 95\%. 
If the first $K$ eigenvectors capture the required variance, this is given by,
$$
\frac{\displaystyle \sum _{k=1} ^{K} \lambda_k}{\displaystyle \sum _{i=1} ^{d} \lambda_i} \ge 0.95
$$ 
Hence, the higher the variance captured, the lower is the error obtained.

## P.C.A. Algorithm
Given a centered dataset $\{x_1, x_2, \ldots, x_n\}$ where $x_i \in \mathbb{R}^{d}$, let $C$ be its covariance matrix.

The algorithm is as follows:

* Step 1: Find the eigenvalues and eigenvectors of $C$. Let $\{\lambda_1, \lambda_2, \ldots, \lambda_d \}$ be its eigenvalues placed in the descending order, and let $\{w_1, w_2, \ldots, w_d \}$ be its corresponding eigenvectors of unit length.
* Step 2: Calculate $K$, where $K$ is the number of required top eigenvalues and eigenvectors, according to the required variance that needs to be covered.
* Step 3: Project the data onto the eigenvectors, and obtain the required representation as a linear combination of those projections. 

In short, we can say that P.C.A. is a dimensionality reduction technique that finds combination of features that are de-correlated (independent of each other).