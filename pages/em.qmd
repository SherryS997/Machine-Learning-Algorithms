---
title: "Expectation-maximization (EM) algorithm"
---

## Convexity and Jensen's Inequality
Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as,
$$
f \left (\sum _{k=1} ^K \lambda_k a_k \right ) \ge \sum _{k=1} ^K \lambda_k f(a_k)
$$
where
$$
\sum _{k=1} ^K \lambda _k = 1 
$$
$$
a_k \text{ are points of the function}
$$
This is also known as **Jensen's Inequality**.

# Estimating the Parameters
As log is a concave function, using the above equation, we can also approximate the likelihood function for GMM's as follows,
$$
\log\mathcal{L}(\theta) = \sum _{i=1} ^n \log \left [ \sum _{k=1} ^K \pi_k * \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ]
$$
Introduce for data point $x_i$, the parameters $\{\lambda_1^i, \lambda_2^i, \ldots, \lambda_k^i\}$ s.t. $\forall i,k \displaystyle \sum _{k=1} ^K \lambda _k^i = 1; 0 \le \lambda _k^i \le 1$.
$$
\log\mathcal{L}(\theta) = \sum _{i=1} ^n \log \left [ \sum _{k=1} ^K \lambda _k ^i \left ( \pi_k * \frac{1}{\lambda _k ^i\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ) \right ]
$$
$$ \text{Using Jensen's Inequality, we get} $$
$$
\log\mathcal{L}(\theta) \ge \text{modified\_log}\mathcal{L}(\theta)
$$
\begin{align}
\therefore \text{modified\_log}\mathcal{L}(\theta) 
&= \sum _{i=1} ^n \sum _{i=k} ^K \lambda _k ^i \log \left ( \pi_k * \frac{1}{\lambda _k ^i\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right )
\end{align} 
Note that the modified-log likelihood function gives a lower bound for the true log likelihood function at $\theta$.
Finally, to get the parameters, we do the following,

* To get $\theta$: Fix $\lambda$ and maximize over $\theta$.
$$
\underset{\theta} {\max} \sum _{i=1} ^n \sum _{i=k} ^K \lambda _k ^i \log \left ( \pi_k * \frac{1}{\lambda _k ^i\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right )
$$
$$
\text{Differentiate w.r.t. }\mu,\sigma^2,\text{ and }\pi \text{ to get the following}
$$
\begin{align*}
\hat{\mu_k^{\scriptsize{MML}}} &= \frac{\displaystyle\sum _{i=1} ^n \lambda_k^i x_i}{\displaystyle\sum _{i=1} ^n \lambda_k^i} \\
\hat{\sigma_k^{2^{\scriptsize{MML}}}} &= \frac{\displaystyle\sum _{i=1} ^n \lambda_k^i (x_i-\hat{\mu_k^{\scriptsize{MML}}})^2}{\displaystyle\sum _{i=1} ^n \lambda_k^i} \\
\hat{\pi_k^{\scriptsize{MML}}} &= \frac{\displaystyle\sum _{i=1} ^n \lambda_k^i}{n} \\
\end{align*}
* To get $\lambda$: Fix $\theta$ and maximize over $\lambda$. For any $i$:
$$
\underset{\lambda_1^i, \lambda_2^i, \ldots, \lambda_k^i} {\max} 
\sum _{k=1} ^K \left [ \lambda _k ^i \log \left ( \pi_k * \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ) -  \lambda _k ^i \log( \lambda _k ^i) \right ] \hspace{1em} s.t. \hspace{1em} \sum _{k=1} ^K \lambda _k ^i = 1;  0 \le \lambda _k ^i \le 1
$$
$$
\text{Solving the above constrained optimization problem analytically, we get}
$$
\begin{align*}
\hat{\lambda_k^{i^{\scriptsize{MML}}}}
&= \frac{\left ( \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} \right ) * \pi_k}{ \displaystyle \sum _{k=1} ^K \left ( \frac{1}{\sqrt{2\pi}\sigma_k} e^{\frac{-(x_i-\mu_k)^2}{2\sigma^2_k}} * \pi_k \right )} \\
\text{i.e. }\hat{\lambda_k^{i^{\scriptsize{MML}}}}
&= \frac{f(x_i|z_i=k)*P(z_i=k)}{P(x_i)} \\
\hat{\lambda_k^{i^{\scriptsize{MML}}}}
&= \frac{f(x_i|z_i=k)*P(z_i=k)}{\displaystyle \sum _{k=1} ^K f(x_i|z_i=k)*P(z_i=k)} \\
\end{align*}

# EM Algorithm
The EM (Expectation-Maximization) algorithm is a popular method for estimating the parameters of statistical models with incomplete data by iteratively alternating between expectation and maximization steps until convergence to a stable solution.

The algorithm is as follows:

* Initialize $\theta^0 =
\left \{ \begin{array}{cccc}
\mu_1, \mu_2, \ldots, \mu_K \\
\sigma^2_1, \sigma^2_2, \ldots, \sigma^2_K\\
\pi_1, \pi_2, \ldots, \pi_K
\end{array} \right \}$ using Lloyd's algorithm.
* Until convergence ($||\theta^{t+1}-\theta^{t} || \le \epsilon$ where $\epsilon$ is the tolerance parameter) do the following:
\begin{align*}
\lambda^{t+1} &= \underset{\lambda}{\arg \max} \text{ modified\_log}(\theta^t, \textcolor{red}{\lambda}) &\rightarrow \text{ Expectation Step}\\
\theta^{t+1} &= \underset{\theta}{\arg \max} \text{ modified\_log}(\textcolor{red}{\theta}, \lambda^{t+1}) &\rightarrow \text{ Maximization Step}\\
\end{align*}

EM algorithm produces soft clustering. For hard clustering using EM, a further step is involved:

* For a point $x_i$, assign it to a cluster using the following equation:
$$
z_i = \underset{k}{\arg\max} \lambda_k^i
$$