[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Machine Learning Algorithms",
    "section": "",
    "text": "Warning\n\n\n\nThe site is not complete and is still under development.\nThis website is a comprehensive resource for machine learning algorithms. Here, you will find explanations of the most popular algorithms, as well as code implementations in Python.\nThe website is divided into two main sections: supervised learning and unsupervised learning. Supervised learning algorithms are used to learn from labeled data, while unsupervised learning algorithms are used to learn from unlabeled data.\nWithin each section, the algorithms are further divided into regression, classification, and clustering. Regression algorithms are used to predict continuous values, while classification algorithms are used to predict discrete values. Clustering algorithms are used to group data points together.\nEach algorithm page includes the following information:\nI hope you find this website to be a valuable resource for learning about machine learning algorithms.\nHere is a more detailed overview of the algorithms covered on this website:"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Welcome to Machine Learning Algorithms",
    "section": "Credits",
    "text": "Credits\nI would like to express my gratitude to Professor Arun Rajkumar for his valuable content and notations, which have greatly influenced my understanding. Additionally, I would like to acknowledge the contribution of IIT Madras, where I had the opportunity to learn from Professor Arun Rajkumar in the Machine Learning Techniques Course."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there, I’m Sherry Thomas, and I’m passionate about making a difference in the field of data science. I have a Bachelor’s degree in Philosophy from Christ University, Bengaluru and I’m currently pursuing a Bachelor’s degree in Data Science and Applications at IIT-Madras.\nI have a diverse academic background that includes philosophy, programming, computer science, and statistics. My philosophy studies have helped me develop critical thinking and problem-solving skills, while my data science coursework has provided me with the technical expertise needed to tackle complex challenges.\nI believe in taking a holistic approach to data analysis and decision-making, considering the ethical and societal implications of technology and data. My goal is to use my knowledge and skills to make a positive impact in the field of data science.\nI’m constantly learning and growing, and I’m committed to expanding my knowledge base and contributing to the ever-evolving world of technology and data."
  },
  {
    "objectID": "pages/lin_reg.html",
    "href": "pages/lin_reg.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a supervised learning algorithm used to predict a continuous output variable based on one or more input features, assuming a linear relationship between the input and output variables. The goal of linear regression is to find the line of best fit that minimizes the sum of squared errors between the predicted and actual output values.\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels, where \\(y_i \\in \\mathbb{R}\\).\nThe goal of linear regression is to find the mapping between input and output variables. i.e. \\[\nh: \\mathbb{R}^d \\rightarrow \\mathbb{R}\n\\] The error for the above mapping function is given by, \\[\n\\text{error}(h) = \\sum _{i=1} ^n (h(x_i) - y_i) ^2)\n\\] This error can be as small as zero, and this is achieved when \\(h(x_i)=y_i \\hspace{0.5em} \\forall i\\). But this may not be the desired output as it may represent nothing more than memorizing the data and its outputs.\nThe memorization problem can be mitigated by introducing a structure to the mapping. The simplest structure being of the linear kind, we shall use it as the underlying structure for our data.\nLet \\(\\mathcal{H}_{\\text{linear}}\\) represent the solution space for the mapping in the linear space. \\[\n\\mathcal{H}_{\\text{linear}}=\\Biggr \\lbrace{h_w: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\hspace{0.5em} s.t. \\hspace{0.5em} h_w(x)=w^Tx \\hspace{0.5em} \\forall w \\in \\mathbb{R}^d } \\Biggr \\rbrace\n\\] Therefore, our goal is, \\[\\begin{align*}\n\\min _{h \\in \\mathcal{H}_{\\text{linear}}} \\sum _{i=1} ^n (h(x_i) - y_i) ^2 \\\\\n\\text{Equivalently} \\\\\n\\min _{w \\in \\mathbb{R}^d} \\sum _{i=1} ^n (w^Tx_i - y_i) ^2\n\\end{align*}\\] Optimizing the above is the entire point of the linear regression algorithm."
  },
  {
    "objectID": "pages/lin_reg.html#stochastic-gradient-descent",
    "href": "pages/lin_reg.html#stochastic-gradient-descent",
    "title": "Linear Regression",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm used in machine learning for finding the parameters that minimize the loss function of a model. In contrast to traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters based on a randomly selected subset of the data, known as a batch. This results in faster training times and makes SGD particularly useful for large datasets.\nFor every step \\(t\\), rather than updating \\(w\\) using the entire dataset, we use a small randomly selected (\\(k\\)) data points to update \\(w\\). Therefore, the new gradient is \\(2(\\tilde{X}\\tilde{X}^Tw^t - \\tilde{X}\\tilde{y})\\) where \\(\\tilde{X}\\) and \\(\\tilde{y}\\) is the small sample randomly selected from the dataset. This is manageable because \\(\\tilde{X} \\in \\mathbb{R}^{d \\times k}\\) which is considerably smaller than \\(X\\).\nAfter T rounds, we use, \\[\nw ^T _{SGD} = \\frac{1}{T}  \\sum _{i=1} ^T w^i\n\\] This has a certain guarantee to have optimal convergence partly because of the randomness involved in it.\n\nImplementing Stochastic Gradient Descent in Python\nLet compute the \\(w\\) vector using Stochastic Gradient Descent and print the coefficients and intercept.\nw = np.ones((X.shape[0], 1))\neta = 1e-4\nt = 100000\nn = X.shape[1]\nb = 10\n\nfor i in range(t):\n    # randomly select a batch of samples\n    idx = np.random.choice(n, b, replace=False)\n    X_b = X[:, idx]\n    y_b = y[idx]\n    # compute the gradient for the batch\n    grad = 2*(X_b @ X_b.T @ w) - 2*(X_b @ y_b)\n    # update the weights\n    w = w - eta*grad\n    \nprint ('Coefficients: ', w.reshape((-1,))[:3])\nprint ('Intercept: ', w.reshape((-1,))[-1])\n\n\n\n\n\n\nCoefficients: [10.72000912, 8.3366805, 9.13970723]\nIntercept: 65.2366350549217"
  },
  {
    "objectID": "pages/intro.html",
    "href": "pages/intro.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "What is Machine Learning\nMachine learning (ML) is a field of computer science that gives computers the ability to learn without being explicitly programmed. ML algorithms are able to learn from data and make predictions or decisions without being explicitly told how to do so. This makes ML a powerful tool for automating tasks, making predictions, and uncovering patterns in data.\n\n\nWhy Machine Learning?\nThere are many reasons why machine learning is becoming increasingly popular. Some of the key benefits of ML include:\n\nAutomation: ML can be used to automate tasks that would otherwise require human intelligence. This can free up human workers to focus on more creative and strategic tasks.\nAccuracy: ML algorithms can often make more accurate predictions than humans. This is because ML algorithms can learn from large amounts of data and identify patterns that humans would not be able to see.\nScalability: ML algorithms can be scaled to handle large amounts of data. This makes them ideal for applications such as fraud detection, natural language processing, and image recognition.\n\n\n\nWhere is Machine Learning Used?\nMachine learning is used in a wide variety of fields, including:\n\nFinance: ML is used to predict stock prices, identify fraud, and manage risk.\nHealthcare: ML is used to diagnose diseases, develop new treatments, and personalize care.\nRetail: ML is used to personalize recommendations, predict demand, and prevent fraud.\nManufacturing: ML is used to optimize production, improve quality control, and predict maintenance needs.\nLogistics: ML is used to optimize shipping routes, manage inventory, and predict demand.\n\n\n\nTypes of Machine Learning\nThere are many different types of machine learning, each with its own strengths and weaknesses. Some of the most common types of machine learning include:\n\nSupervised learning: In supervised learning, the algorithm is trained on a dataset of labeled data. This means that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data.\nUnsupervised learning: In unsupervised learning, the algorithm is trained on a dataset of unlabeled data. This means that the data only includes inputs and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance.\nReinforcement learning: In reinforcement learning, the algorithm is trained by trial and error. The algorithm is given a goal and a set of actions that it can take. The algorithm then tries different actions and observes the results. Over time, the algorithm learns which actions are most likely to lead to the desired goal.\n\n\n\nThe Future of Machine Learning\nMachine learning is a rapidly growing field with the potential to revolutionize many industries. As ML algorithms become more powerful and sophisticated, they will be able to automate more tasks, make more accurate predictions, and uncover more patterns in data. This will lead to new and innovative applications in a wide variety of fields."
  },
  {
    "objectID": "pages/PCA.html",
    "href": "pages/PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Representation learning is a sub-field of machine learning that focuses on learning meaningful and compact representations of complex data for tasks such as dimensionality reduction, clustering, and classification.\nGiven a dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\), we need to find a representation for it with the minimum reconstruction error.\nLet \\(w\\), where \\(||w||=1\\), be the best linear representation of the dataset.\nThe representation is given by, \\[\\begin{align*}\n    \\frac{(x_i^Tw)}{w^Tw}&w \\\\\n    \\text{But }||w||&=1\\\\\n    \\therefore \\text{ Projection } &= (x_i^Tw)w\n\\end{align*}\\] The reconstruction error is given by, \\[\n\\text{Reconstruction Error}(f(w)) = \\frac{1}{n} \\sum _{i=1} ^{n} || x_i - (x_i^Tw)w || ^ 2\n\\] where \\(x_i - (x_i^Tw)w\\) is the residue and can be given by \\(x'\\).\nOur objective is to minimize the reconstruction error. Minimizing it, we get, \\[\\begin{align*}\n    \\min _{w \\in ||w|| = 1} f(w) &= \\frac{1}{n} \\sum _{i=1} ^{n} -(x_i^Tw)^2 \\\\\n    \\therefore \\max _{w \\in ||w|| = 1} f(w) &= \\frac{1}{n} \\sum _{i=1} ^{n} (x_i^Tw)^2 \\\\\n    &= w^T(\\frac{1}{n} \\sum _{i=1} ^{n} x_ix_i^T)w \\\\\n    \\max _{w \\in ||w|| = 1} f(w) &= w^TCw\n\\end{align*}\\] where \\(C=\\displaystyle \\frac{1}{n} \\displaystyle \\sum _{i=1} ^{n} x_ix_i^T\\) is the Covariance Matrix and \\(C \\in \\mathbb{R}^{d \\times d}\\).\nFrom the above equation we find that \\(w\\) is the eigenvector corresponding to the largest eigenvalue \\(\\lambda\\) of \\(C\\).\n\\(w\\) is also called the First Principal Component of the dataset.\n\n\nWith this information, we can give the following algorithm:\nGiven a dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset \\[\n\\mu = \\frac{1}{n} \\sum _{i=1} ^{n} x_i\n\\] \\[\nx_i = x_i - \\mu  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(w \\in \\mathbb{R}^d\\) and \\(||w|| = 1\\).\nReplace \\(x_i = x_i - (x_i^Tw)w \\hspace{1em} \\forall i\\)\nRepeat the first three steps until residues become zero and we obtain \\(w_2, w_3, \\ldots, w_d\\).\n\nBut is this the best way? How many \\(w\\) do we need for optimal compression?"
  },
  {
    "objectID": "pages/PCA.html#potential-algorithm",
    "href": "pages/PCA.html#potential-algorithm",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "With this information, we can give the following algorithm:\nGiven a dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset \\[\n\\mu = \\frac{1}{n} \\sum _{i=1} ^{n} x_i\n\\] \\[\nx_i = x_i - \\mu  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(w \\in \\mathbb{R}^d\\) and \\(||w|| = 1\\).\nReplace \\(x_i = x_i - (x_i^Tw)w \\hspace{1em} \\forall i\\)\nRepeat the first three steps until residues become zero and we obtain \\(w_2, w_3, \\ldots, w_d\\).\n\nBut is this the best way? How many \\(w\\) do we need for optimal compression?"
  },
  {
    "objectID": "pages/PCA.html#approximate-representation",
    "href": "pages/PCA.html#approximate-representation",
    "title": "Principal Component Analysis",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nIf the data can be approximately represented by a lower sub-space, is it enough that we use only those \\(k\\) projections? How much variance should be covered?\nGiven a centered dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\), let \\(C\\) be its covariance matrix, and let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be its eigenvalues, which are non-negative because the covariance matrix is a positive semi-definite matrix, placed in descending order, and let \\(\\{w_1, w_2, \\ldots, w_d \\}\\) be its corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be given by, \\[\\begin{align*}\n    Cw &= \\lambda w \\\\\n    w^TCw &= w^T\\lambda w\\\\\n    \\therefore \\lambda &= w^TCw \\hspace{2em} \\{w^Tw = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (x_i^Tw)^2 \\\\\n\\end{align*}\\] Therefore, as the mean is zero, \\(\\lambda\\) gives the variance captured by the eigenvector \\(w\\).\nA good rule of thumb is that the variance captured by P.C.A. should be at least 95%. If the first \\(K\\) eigenvectors capture the required variance, this is given by, \\[\n\\frac{\\displaystyle \\sum _{k=1} ^{K} \\lambda_k}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\] Hence, the higher the variance captured, the lower is the error obtained."
  },
  {
    "objectID": "pages/PCA.html#p.c.a.-algorithm",
    "href": "pages/PCA.html#p.c.a.-algorithm",
    "title": "Principal Component Analysis",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nGiven a centered dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\), let \\(C\\) be its covariance matrix.\nThe algorithm is as follows:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(C\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be its eigenvalues placed in the descending order, and let \\(\\{w_1, w_2, \\ldots, w_d \\}\\) be its corresponding eigenvectors of unit length.\nStep 2: Calculate \\(K\\), where \\(K\\) is the number of required top eigenvalues and eigenvectors, according to the required variance that needs to be covered.\nStep 3: Project the data onto the eigenvectors, and obtain the required representation as a linear combination of those projections.\n\nIn short, we can say that P.C.A. is a dimensionality reduction technique that finds combination of features that are de-correlated (independent of each other)."
  },
  {
    "objectID": "pages/Kernel_PCA.html",
    "href": "pages/Kernel_PCA.html",
    "title": "Kernel PCA",
    "section": "",
    "text": "For a given dataset \\(D \\in \\mathbb{R} ^{d \\times n}\\), the covariance matrix is \\(C \\in \\mathbb{R} ^{d \\times d}\\). PCA for this dataset can have the following two problems:\n\nTime Complexity: The algorithimic complexity of finding the eigenvalues and eigenvectors of \\(C\\) is \\(O(d ^3)\\). Hence, as \\(d\\) grows, the time taken becomes very large.\nNon-Linear Dataset: The dataset may lie in a non-linear subspace. As PCA tries to get linear combination of Principal Components, non-linear datasets may result in non-optimal outputs."
  },
  {
    "objectID": "pages/Kernel_PCA.html#transforming-features",
    "href": "pages/Kernel_PCA.html#transforming-features",
    "title": "Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nWe solve the problem of non-linear relationships by mapping them to higher dimensions. \\[\nx \\to \\phi(x) \\hspace{2em} \\mathbb{R} ^d \\to \\mathbb{R} ^D \\hspace{2em} \\text{where } [D &gt;&gt; d]\n\\] To compute \\(D\\):\nLet \\(x=\\left [ \\begin{array} {cc}  f_1 & f_2 \\end{array} \\right ]\\) be features of a dataset containing datapoints lying on a curve of degree two in a two-dimensional space.\nTo make it linear from quadratic, we map the features to \\(\\phi(x)=\\left [ \\begin{array} {cccccc}  1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2 \\end{array} \\right ]\\)\nMapping \\(d\\) features to the polygonial power \\(p\\) gives \\(^{d+p} C_d\\) new features.\nIssue: Finding \\(\\phi(x)\\) may be very hard.\nSolution for this issue is in the next point."
  },
  {
    "objectID": "pages/Kernel_PCA.html#kernel-functions",
    "href": "pages/Kernel_PCA.html#kernel-functions",
    "title": "Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function that maps \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\), and is a “valid”, is called a Kernel Function.\nProof of a “Valid” Kernel:\n\nMethod 1: Exhibit the map to \\(\\phi\\) explicitly. [may be hard]\nMethod 2: Using Mercer’s Theorem:\n\n\\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a “valid” kernel if and only if:\n\n\\(k\\) is symmetric i.e \\(k(x,x') = k(x',x)\\)\nFor any dataset \\(\\{x_1,x_2,\\ldots,x_n \\}\\), the matrix \\(K \\in \\mathbb{R}^{n \\times n}\\), where \\(K_{ij} = k(i,j)\\), is Positive Semi-Definite.\n\n\n\nTwo Popular Kernel Functions:\n\nPolynomial Kernel: \\(k(x,x') = (x^Tx + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(exp(-\\frac{||x-x'||^2}{2\\sigma^2})\\)"
  },
  {
    "objectID": "pages/kmeans.html",
    "href": "pages/kmeans.html",
    "title": "K Means",
    "section": "",
    "text": "Clustering is a method of unsupervised machine learning that groups similar objects into clusters, discovering structure in data for exploratory analysis or as a pre-processing step for other algorithms.\nOur objective is to group \\(n\\) datapoints into \\(k\\) clusters.\nNotation: \\[\n\\{x_1, x_2, \\dots, x_n \\} \\hspace{6em} x_i \\in \\mathbb{R}^d\n\\] \\[\n\\{z_1, z_2, \\dots, z_n\\} \\hspace{2em} z_i \\in \\{1, 2, \\dots, k\\}\n\\] Objective Function: \\[\nF(z_1, z_2, \\dots, z_n) = \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}_2 ^2\n\\] where \\[\n\\mu _k = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {x_i \\cdot \\mathbf{1}(z_i=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{1}(z_i=k)}}\n\\] Goal: \\[\n\\min _{\\{z_1, z_2, \\ldots, z_n\\}} \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2\n\\] Unfortunately, finding a solution manually is an NP-Hard problem due to the existence of \\(k^n\\) possibilities. As a result, alternative approaches must be considered to address this challenge."
  },
  {
    "objectID": "pages/kmeans.html#the-algorithm",
    "href": "pages/kmeans.html#the-algorithm",
    "title": "K Means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm is as follows:\nStep 1: Initialization: Assign \\(z_1^0, z_2^0, \\ldots, z_n^0\\) where \\(z_i^0 \\in \\{1, 2, \\ldots, k\\}\\). The approach on how to initialize them is discussed later.\nStep 2: Compute Means: \\[\n\\mu _k ^t = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {x_i \\cdot \\mathbf{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 3: Reassignment Step: \\[\nz _i ^{t+1} = \\underset{k}{\\arg \\min} {|| x_i - \\mu _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until convergence for \\(t\\) iterations."
  },
  {
    "objectID": "pages/kmeans.html#fact-regarding-lloyds-algorithm",
    "href": "pages/kmeans.html#fact-regarding-lloyds-algorithm",
    "title": "K Means",
    "section": "Fact regarding Lloyd’s Algorithm",
    "text": "Fact regarding Lloyd’s Algorithm\nLloyd’s Algorithm, also known as K-means, is guaranteed to converge to a solution. While the converged solution may not be the optimal one, it has been observed to produce acceptable clustering results in practice."
  },
  {
    "objectID": "pages/kkmeans.html",
    "href": "pages/kkmeans.html",
    "title": "Kernel K-means",
    "section": "",
    "text": "What if the dataset is as follow:\n\nThe standard k-means algorithm may not perform well when the underlying clusters in the dataset have a non-linear structure. In such cases, alternative methods such as Kernel K-means or Spectral Clustering can be employed to improve clustering accuracy. However, the intricacies of these methods will not be covered in this session.\nOne possible way to initialize the centroids is to randomly assign datapoints from the dataset as centroids.\nThe other method is K-means++.\n\n\nThe premise is to select centroids that are as far as possible from each other.\n\nStep 1: Choose \\(\\mu _1 ^0\\) randomly from the dataset.\nStep 2: For \\(l \\in \\{2, 3, \\ldots, k\\}\\), choose \\(\\mu _l ^0\\) probablistically proportional to score(\\(S\\)) where \\(S\\) is, \\[\n  S(x) = \\min _{\\{j=1, 2, \\ldots, l-1\\}} {|| x - \\mu _{j} ^0 ||}^2 \\hspace{2em} \\forall x\n\\] The probabilistic aspect of the algorithm provides an expected guarantee of optimal convergence in K-means. The guarantee is given by, \\[\n  \\mathbb{E} \\left[ \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2 \\right ]\n  \\le O(\\log k) \\left [ \\min _{\\{z_1, z_2, \\ldots, z_n\\}} \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2 \\right ]\n\\] where \\(O(\\log k)\\) is a constant of order \\(\\log k\\).\nStep 3: Once the centroids are determined, we proceed with Lloyd’s Algorithm."
  },
  {
    "objectID": "pages/kkmeans.html#k-means",
    "href": "pages/kkmeans.html#k-means",
    "title": "Kernel K-means",
    "section": "",
    "text": "The premise is to select centroids that are as far as possible from each other.\n\nStep 1: Choose \\(\\mu _1 ^0\\) randomly from the dataset.\nStep 2: For \\(l \\in \\{2, 3, \\ldots, k\\}\\), choose \\(\\mu _l ^0\\) probablistically proportional to score(\\(S\\)) where \\(S\\) is, \\[\n  S(x) = \\min _{\\{j=1, 2, \\ldots, l-1\\}} {|| x - \\mu _{j} ^0 ||}^2 \\hspace{2em} \\forall x\n\\] The probabilistic aspect of the algorithm provides an expected guarantee of optimal convergence in K-means. The guarantee is given by, \\[\n  \\mathbb{E} \\left[ \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2 \\right ]\n  \\le O(\\log k) \\left [ \\min _{\\{z_1, z_2, \\ldots, z_n\\}} \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2 \\right ]\n\\] where \\(O(\\log k)\\) is a constant of order \\(\\log k\\).\nStep 3: Once the centroids are determined, we proceed with Lloyd’s Algorithm."
  },
  {
    "objectID": "pages/mle.html",
    "href": "pages/mle.html",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "",
    "text": "Estimators in machine learning are algorithms or models used to estimate unknown parameters or predict outcomes based on data. The aim of the method is to find/predict the unknow parameters describing the distribution of the data.\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\). We assume that the datapoints are independent and identically distributed.\nIndependence means \\(P(x_i|x_j) = P(x_i)\\). Identically distributed means \\(P(x_i)=P(x_j)=p\\)."
  },
  {
    "objectID": "pages/mle.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/mle.html#fishers-principle-of-maximum-likelihood",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate the parameters of a statistical model by choosing the parameter values that maximize the likelihood function, which measures how well the model fits the observed data.\nApplying the likelihood function on the above dataset, we get \\[\\begin{align*}\n\\mathcal{L}(p;\\{x_1, x_2, \\ldots, x_n\\}) &= P(x_1, x_2, \\ldots, x_n;p)\\\\\n&=p(x_1;p)p(x_2;p)\\ldots p(x_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{x_1, x_2, \\ldots, x_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{ML} &= \\frac{1}{n}\\sum _{i=1} ^n x_i\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/mle.html#likelihood-estimation-on-gaussian-distributions",
    "href": "pages/mle.html#likelihood-estimation-on-gaussian-distributions",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Likelihood Estimation on Gaussian Distributions",
    "text": "Likelihood Estimation on Gaussian Distributions\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\). We assume that the datapoints are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\mu, \\sigma^2;\\{x_1, x_2, \\ldots, x_n\\}) &= f_{x_1, x_2, \\ldots, x_n}(x_1, x_2, \\ldots, x_n;\\mu, \\sigma^2) \\\\\n&=\\prod _{i=1} ^n  f_{x_i}(x_i;\\mu, \\sigma^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{\\frac{-(x_i-\\mu)^2}{2\\sigma^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{x_1, x_2, \\ldots, x_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\sigma}  \\right ) - \\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{Differentiating wrt $\\mu$ and $\\sigma$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\mu}_{ML} &= \\frac{1}{n}\\sum _{i=1} ^n x_i \\\\\n\\hat{\\sigma^2}_{ML} &= \\frac{1}{n}\\sum _{i=1} ^n (x_i-\\mu)^2\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/bayes_est.html",
    "href": "pages/bayes_est.html",
    "title": "Bayesian estimation",
    "section": "",
    "text": "Bayesian estimation is a statistical method that updates the estimates of model parameters by combining prior knowledge or beliefs with observed data to calculate the posterior probability distribution of the parameters.\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i\\) belongs to a distribution with parameters \\(\\theta\\). We assume that the datapoints are independent and identically distributed. We also assume that \\(\\theta\\) is also from a probability distribution.\nOur goal is to update the parameters using the data.\nie. \\[\nP(\\theta)=&gt;P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})\n\\] where, using Bayes Law, we get \\[\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\})=\\left ( \\frac{P(\\{x_1, x_2, \\ldots, x_n\\}|\\theta)}{P(\\{x_1, x_2, \\ldots, x_n\\})} \\right )*P(\\theta)\n\\]\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\) with parameter \\(\\theta\\). What distribution can be suited for \\(P(\\theta)\\).\nA commonly used distributions for priors is the Beta Distribution. \\[\nf(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{z} \\hspace{2em} \\forall p \\in [0,1]\n\\] \\[\n\\text{where $z$ is a normalizing factor}\n\\]\nHence, using the Beta Distribution as the Prior, we get, \\[\\begin{align*}\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\}) &\\propto P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})*P(\\theta) \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto \\left [ \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ]*\\left [ p^{\\alpha-1}(1-p)^{\\beta-1} \\right ] \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto p^{\\sum _{i=1} ^n x_i + \\alpha - 1}(1-p)^{\\sum _{i=1} ^n(1-x_i) + \\beta - 1}\n\\end{align*}\\] i.e. we get, \\[\n\\text{BETA PRIOR }(\\alpha, \\beta) \\xrightarrow[Bernoulli]{\\{x_1, x_2, \\ldots, x_n\\}} \\text{BETA POSTERIOR }(\\alpha + n_h, \\beta + n_t)\n\\] \\[\n\\therefore \\hat{p_{ML}} = \\mathbb{E}[Posterior]=\\mathbb{E}[Beta(\\alpha +n_h, \\beta + n_t)]= \\frac{\\alpha + n_h}{\\alpha + n_h + \\beta + n_t}\n\\]"
  },
  {
    "objectID": "pages/gmm.html",
    "href": "pages/gmm.html",
    "title": "Gaussian mixture model (GMM)",
    "section": "",
    "text": "Gaussian Mixture Models are a type of probabilistic model used to represent complex data distributions by combining multiple Gaussian distributions.\nThe procedure is as follows:\n\nStep 1: Generate a mixture component among \\(\\{1, 2, \\ldots, K\\}\\) where \\(z_i \\in \\{1, 2, \\ldots, K\\}\\). We get, \\[\nP(z_i=k) = \\pi_k \\hspace{2em} \\left [ \\sum _{i=1} ^K \\pi_i = 1 \\hspace{1em} 0 \\le \\pi_i \\le 1 \\hspace{1em} \\forall i \\right ]\n\\]\nStep 2: Generate \\(x_i \\sim \\mathcal{N}(\\mu_{z_i}, \\sigma^2_{z_i})\\)\n\nHence, there are \\(3K\\) parameters. Let these parameters be represented by \\(\\theta\\).\n\nLikelihood of GMM’s\n\\[\\begin{align*}\n\\mathcal{L}\\left( \\begin{array}{cccc}\n\\mu_1, \\mu_2, \\ldots, \\mu_K \\\\\n\\sigma^2_1, \\sigma^2_2, \\ldots, \\sigma^2_K\\\\\n\\pi_1, \\pi_2, \\ldots, \\pi_K\n\\end{array}; x_1, x_2, \\ldots, x_n \\right )\n&= \\prod _{i=1} ^n f_{mix} \\left( x_i; \\begin{array}{cccc}\n\\mu_1, \\mu_2, \\ldots, \\mu_K \\\\\n\\sigma^2_1, \\sigma^2_2, \\ldots, \\sigma^2_K\\\\\n\\pi_1, \\pi_2, \\ldots, \\pi_K\n\\end{array} \\right ) \\\\\n&= \\prod _{i=1} ^n \\left [ \\sum _{k=1} ^K \\pi_k * f_{mix}(x_i; \\mu_k, \\sigma_k) \\right ] \\\\\n\\therefore \\log\\mathcal{L}(\\theta) &= \\sum _{i=1} ^n \\log \\left [ \\sum _{k=1} ^K \\pi_k * \\frac{1}{\\sqrt{2\\pi}\\sigma_k} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma^2_k}} \\right ] \\\\\n\\end{align*}\\] To solve the above equation, we need to understand convexity."
  },
  {
    "objectID": "pages/em.html",
    "href": "pages/em.html",
    "title": "Expectation-maximization (EM) algorithm",
    "section": "",
    "text": "Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/em.html#convexity-and-jensens-inequality",
    "href": "pages/em.html#convexity-and-jensens-inequality",
    "title": "Expectation-maximization (EM) algorithm",
    "section": "",
    "text": "Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/klin_reg.html",
    "href": "pages/klin_reg.html",
    "title": "Kernel Least squares regression",
    "section": "",
    "text": "What if the data points lie in a non-linear sub-space? Just like in the case of clustering non-linear data, we will use kernel functions here too.\nLet \\(w^* = X\\alpha^*\\) for some \\(\\alpha^* \\in \\mathbb{R}^d\\). \\[\\begin{align*}\nX\\alpha^*&=w^*\\\\\n\\therefore X\\alpha^* &=(XX^T)^+Xy \\\\\n(XX^T)X\\alpha^* &=(XX^T)(XX^T)^+Xy \\\\\n(XX^T)X\\alpha^* &=Xy \\\\\nX^T(XX^T)X\\alpha^* &=X^TXy \\\\\n(X^TX)^2\\alpha^* &=X^TXy \\\\\nK^2\\alpha^* &=Ky \\\\\n\\therefore \\alpha^* &=K^{-1}y\n\\end{align*}\\] where \\(K \\in \\mathbb{R}^{n \\times n}\\) and \\(K\\) can be obtained using a kernel function like the Polynomial Kernel or RBF Kernel.\nHow to predict using alpha and the kernel function? Let \\(X_{test} \\in R^{d \\times m}\\) be the test dataset. We predict by, \\[\\begin{align*}\nw^*\\phi(X_{test}) &=  \\sum _{i=1} ^n \\alpha_i^* k(x_i, x_{test_i})\n\\end{align*}\\] where \\(\\alpha_i^*\\) gives the importance of the \\(i^{th}\\) datapoint towards \\(w^*\\) and \\(k(x_i, x_{test_i})\\) shows how similar \\(x_{test_i}\\) is to \\(x_i\\)."
  },
  {
    "objectID": "pages/bayes_lin_reg.html",
    "href": "pages/bayes_lin_reg.html",
    "title": "Bayesian view of least squares regression",
    "section": "",
    "text": "Probabilistic View of Linear Regression\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels, where \\(y_i \\in \\mathbb{R}\\). The probability of \\(y_i\\) given \\(x_i\\) is given by, \\[\nP(y|X) \\sim w^TX + \\epsilon\n\\] where \\(w \\in R^d\\) is an unknown but fixed value and \\(\\epsilon\\) is the noise corresponding to distribution \\(\\mathcal{N}(0, \\sigma^2)\\).\nBecause of the probabilistic nature of this problem, this can be viewed as an estimation problem and the best solution can be approached using Maximum Likelihood. This is given by, \\[\\begin{align*}\n\\mathcal{L}\\left(w; \\hspace{0.5em} \\begin{array}{cccc}\nx_1, x_2, \\ldots, x_n \\\\\ny_1, y_2, \\ldots, y_n\\\\\n\\end{array} \\right )\n&= \\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\sigma}  e^{\\frac{-(w^Tx_i - y_i)^2}{2\\sigma^2}} \\right ] \\\\\n\\log\\mathcal{L}\\left(w; \\hspace{0.5em} \\begin{array}{cccc}\nx_1, x_2, \\ldots, x_n \\\\\ny_1, y_2, \\ldots, y_n\\\\\n\\end{array} \\right )\n&= \\sum _{i=1} ^n \\left [ \\frac{-(w^Tx_i - y_i)^2}{2\\sigma^2} + \\log \\left ( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right ) \\right ]\n\\end{align*}\\] \\[\n\\text{Taking gradients w.r.t. } w \\text{ on both sides}\n\\] \\[\n\\hat{w}_{ML} = w^* = (XX^T)^+Xy\n\\] Therefore, Maximum Likelihood Estimator assuming Zero mean Gaussian Noise is the same as linear regression with square error.\n\n\nGoodness of Maximum Likelihood Estimator for Linear Regression\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels, where \\(y_i \\in \\mathbb{R}\\). \\[\ny|X = w^Tx + \\epsilon\n\\] where \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) and \\(w \\in \\mathbb{R}^d\\). Let \\(\\hat{w}_{ML}\\) signify the maximum likelihood parameter for linear regression. \\[\n\\hat{w}_{ML}=w^*=(XX^T)^+Xy\n\\] \\(\\therefore\\) To measure how good our parameter is, we use the follow: \\[\n\\mathbb{E} [|| \\hat{w}_{ML} - w ||^2_2]\n\\] This is known as the Mean Squared Error (MSE) and turns out to be equal to \\[\n\\mathbb{E} [|| \\hat{w}_{ML} - w ||^2_2] = \\sigma^2 *trace((XX^T)^{-1})\n\\]\n\n\nCross-Validation for Minimizing MSE\nLet the eigenvalues of \\(XX^T\\) be \\(\\{\\lambda_1, \\ldots, \\lambda_d\\}\\). Hence the eigenvalues of \\((XX^T)^{-1}\\) are \\(\\{\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_d}\\}\\).\n\\(\\therefore\\) The MSE is, \\[\n\\mathbb{E} [|| \\hat{w}_{ML} - w ||^2_2] = \\sigma^2 \\sum_{i=1}^d  \\frac{1}{\\lambda_i}\n\\] Consider the following estimator, \\[\n\\hat{w}_{new}=(XX^T + \\lambda I)^{-1}Xy\n\\] where \\(\\lambda \\in \\mathbb{R}\\) and \\(I \\in \\mathbb{R}^{d\\times d}\\) is the Identity Matrix. Using this we get, \\[\ntrace((XX^T + \\lambda I)^{-1}) = \\sum_{i=1}^d  \\frac{1}{\\lambda_i + \\lambda}\n\\] According to the Existence Theorem, \\(\\exists\\lambda\\) s.t. \\(\\hat{w}_{new}\\) has lesser means square error than \\(\\hat{w}_{ML}\\).\nIn practice, we find \\(\\lambda\\) using cross validation.\nThree popular techniques of Cross Validation are:\n\nTraining-Validation Split: The training set is randomly split into training and validation set, usually in the ratio \\(80:20\\). From among various \\(\\lambda\\)s, we choose the one with gives the least error.\nK-Fold Cross Validation: It is done by dividing the training set into K equally-sized parts, training the model K times on different (K-1) parts, and evaluating it on the remaining part. From among various \\(\\lambda\\)s, we choose the one with gives the least average error.\nLeave One Out Cross Validation: It is done by training the model on all but one of the samples in the training set and evaluating it on the left-out sample, repeating this process for each sample in the dataset. From among various \\(\\lambda\\)s, we choose the one with gives the least average error.\n\n\n\nBayesian Modeling\nAn alternate way to understand \\(\\hat{w}_{ML}\\) is through Bayesian Modeling.\nLet \\(P(y|X)\\sim \\mathcal{N}(w^Tx,I)\\). We use \\(I\\), the identity matrix, instead of \\(\\sigma^2\\) for simplicity.\nA good choice of prior for \\(w\\) is \\(\\mathcal{N}(0,\\gamma^2I)\\), where \\(\\gamma\\in\\mathbb{R}^d\\).\nTherefore, we get, \\[\\begin{align*}\nP(w|\\{(x_1, y_1), \\ldots, (x_n,y_n)\\})&\\propto P(\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}|w)*P(w)\\\\\n&\\propto \\left ( \\prod _{i=1} ^n e^{\\frac{-(y_i - w^Tx_i)^2}{2}}  \\right ) * \\left ( \\prod _{i=1} ^d  e^{\\frac{-(w_i - 0)^2}{2\\gamma^2}} \\right )\\\\\n&\\propto \\left ( \\prod _{i=1} ^n e^{\\frac{-(y_i - w^Tx_i)^2}{2}}  \\right ) * \\left ( e^{-\\sum _{i=1} ^d\\frac{w_i^2}{2\\gamma^2}} \\right ) \\\\\n&\\propto \\left ( \\prod _{i=1} ^n e^{\\frac{-(y_i - w^Tx_i)^2}{2}}  \\right ) * e^{\\frac{-||w||^2}{2\\gamma^2}} \\\\\n\\log(P(w|\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}))&\\propto \\frac{-(y_i - w^Tx_i)^2}{2}-\\frac{||w||^2}{2\\gamma^2}\n\\end{align*}\\] \\[\n\\text{Taking the gradient, we get}\n\\] \\[\\begin{align*}\n\\nabla \\log(P(w|\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}))&\\propto  (XX^T)\\hat{w}_{MAP} - Xy + \\frac{\\hat{w}_{MAP}}{\\gamma^2} \\\\\n\\therefore \\hat{w}_{MAP}&=(XX^T + \\frac{1}{\\gamma^2} I)^{-1}Xy\n\\end{align*}\\] where \\(\\hat{w}_{MAP}\\) is the Maximum a posteriori Estimate. In practice, the value for \\(\\frac{1}{\\gamma^2}\\) is acquired using cross validation.\nHence, Maximum a posteriori Estimation for linear regression with a Gaussian Prior \\(\\mathcal{N}(0,\\gamma^2I)\\) for \\(w\\) is equivalent to the “new” estimator we used previously."
  },
  {
    "objectID": "pages/rid_lin_reg.html",
    "href": "pages/rid_lin_reg.html",
    "title": "Ridge regression",
    "section": "",
    "text": "Ridge regression is a type of linear regression that adds a penalty term to the ordinary least squares method to mitigate multicollinearity and overfitting.\nIts objective function is given by, \\[\n\\min_{w\\in \\mathbb{R}^d} \\sum^n_{i=1}(w^Tx_i-y_i)^2 + \\lambda||w||_2^2\n\\] where \\(\\lambda||w||_2^2\\) is the regularizer, and \\(||w||_2^2\\) is the squared L2 Norm of \\(w\\). Let this equation be given by \\(f(w)\\).\nSubsequently, this is also equivalent to, \\[\n\\min_{w\\in \\mathbb{R}^d} \\sum^n_{i=1}(w^Tx_i-y_i)^2 \\hspace{1em}\\text{s.t.}||w||_2^2\\le\\theta\n\\] where \\(\\theta\\) is dependent on \\(\\lambda\\).\nIn conclusion, for every choice of \\(\\lambda&gt;0\\), \\(\\exists \\theta\\) s.t. there are optimal solutions to our objective function.\nThe loss function of the linear regression of \\(w_{ML}\\) is given by, \\[\nf(w_{ML}) = \\sum^n_{i=1}(w_{ML}^Tx_i-y_i)^2\n\\] Consider the set of all \\(w\\) s.t. \\(f(w_{ML}) = f(w) + c\\) where \\(c&gt;0\\). This set is given by, \\[\nS_c = \\left \\{w: f(w_{ML}) = f(w) + c \\right \\}\n\\] i.e. every \\(w \\in S_c\\) satisfies, \\[\n||X^Tw-y||^2=||X^Tw_{ML}-y||^2 + c\n\\] \\[\n\\text{On Simplification, we get}\n\\] \\[\n(w-w_{ML})^T(XX^T)(w-w_{ML}) = c'\n\\] where \\(c'\\) depends on \\(c,XX^T,\\) and \\(w_{ML}\\), but not on \\(w\\).\n\n\n\nPictoral Representation of what Ridge Regression does.\n\n\nConclusion: Ridge Regression pushes feature values to zero but not necessarily zero."
  },
  {
    "objectID": "pages/lasso_lin_reg.html",
    "href": "pages/lasso_lin_reg.html",
    "title": "Lasso Regression",
    "section": "",
    "text": "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a type of linear regression that uses a regularization technique to shrink the coefficients of the less important features to zero, effectively performing feature selection and preventing overfitting.\nIts objective function is given by, \\[\n\\min_{w\\in \\mathbb{R}^d} \\sum^n_{i=1}(w^Tx_i-y_i)^2 + \\lambda||w||_1^2\n\\]\nAs you can see, it is almost the same as Ridge Regression. The only difference is that it uses \\(||w||_1^2\\), instead of \\(||w||_2^2\\), which is the squared L1 norm of \\(w\\).\n\n\n\nPictoral Representation of what Lasso Regression does.\n\n\nLasso Regression does not have a closed form solution and is often solved using Sub-gradients. For further info on sub-gradients, see here.\nConclusion: Lasso Regression pushes less important features to zero."
  },
  {
    "objectID": "pages/knn.html",
    "href": "pages/knn.html",
    "title": "K-nearest neighbors (KNN)",
    "section": "",
    "text": "Binary classification is a machine learning task where the goal is to classify objects into one of two categories. It is a fundamental problem in various fields, including computer vision, natural language processing, and bioinformatics.\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels, where \\(y_i \\in \\{0, 1\\}\\). The goal is given by \\(h: \\mathbb{R}^d \\rightarrow \\{0, 1\\}\\).\nThe loss for this function is given by, \\[\nloss(h)=\\frac{1}{n}\\sum ^n _{i=1}\\mathbb{1}\\left ( h(x_i) \\ne y_i \\right )\n\\] Let \\(\\mathcal{H}_{\\text{linear}}\\) represent the solution space for the mapping in the linear space. \\[\n\\mathcal{H}_{\\text{linear}}=\\Biggr \\lbrace{h_w: \\mathbb{R}^d \\rightarrow \\{1, 0\\} \\hspace{0.5em} s.t. \\hspace{0.5em} h_w(x)=sign(w^Tx) \\hspace{0.5em} \\forall w \\in \\mathbb{R}^d } \\Biggr \\rbrace\n\\] Therefore, the objective function is given by, \\[\n\\min _{h \\in \\mathcal{H}_{\\text{linear}}} \\sum _{i=1} ^n \\mathbb{1}\\left ( h(x_i) \\ne y_i \\right )\n\\] This objective function presents an NP-Hard Problem, indicating the challenge in arriving at optimal and sufficient parameters. Therefore, improved implementations are necessary to address this complexity and achieve satisfactory results."
  },
  {
    "objectID": "pages/knn.html#issues-with-k-nn",
    "href": "pages/knn.html#issues-with-k-nn",
    "title": "K-nearest neighbors (KNN)",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nFollowing are the issues with the algorithm:\n\nThe choice of distance function itself can give different results. The Euclidean distance might not always be the best fit!\nIt can be computationally demanding. When making a prediction for a single test datapoint, the distances between that datapoint and all training points must be calculated and sorted. As a result, the algorithm has a complexity of \\(O(nlog(n))\\), where \\(n\\) represents the size of the dataset.\nNo model is learned by this algorithm. It always needs the training dataset to make proper predictions."
  },
  {
    "objectID": "pages/dec_trees.html",
    "href": "pages/dec_trees.html",
    "title": "Decision tree",
    "section": "",
    "text": "Decision trees are a popular machine learning algorithm that operates by recursively splitting the data based on the most informative features until a stopping criterion is met. They are widely used for classification and regression tasks and can be visualized as a tree-like structure.\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels where \\(y_i \\in \\{0, 1\\}\\). The output of the algorithm will be a decision tree.\nPrediction: Given \\(x_{test}\\), traverse through the tree to reach a leaf node. \\(y_{test} = \\text{value in the leaf node}\\).\nPictorial depiction of the decision tree:\n\n\n\nDecision Tree\n\n\nwhere a question is a (feature, value) pair. Example: \\(height\\le180cm\\)?\n\n\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset. We partition it using a question into \\(D_{yes}\\) and \\(D_{no}\\).\nWhat we need is a measure of “Impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). This measure can be given by various ways, but we will use the Entropy Function.\nThe Entropy function is given by, \\[\nEntropy(\\{y_1, \\ldots, y_n\\}) = Entropy(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\] where conventionally \\(\\log(0)\\) is treated as \\(0\\).\nPictorial Representation of the Entropy function:\n\n\n\nEntropy Function\n\n\nThen, we use Information Gain to measure the goodness of the split.\nInformation gain is a commonly used criterion in decision tree algorithms that measures the reduction in entropy or impurity of a dataset after splitting based on a given feature. By selecting features with high information gain, decision trees can effectively differentiate between the different classes of data and make accurate predictions.\nInformation gain is given by, \\[\n\\text{Information Gain}(feature,value)=Entropy(D) - \\left [ \\gamma Entropy(D_{yes})+(1-\\gamma)Entropy(D_{no}) \\right ]\n\\] where \\(\\gamma\\) is given by, \\[\n\\gamma=\\frac{|D_{yes}|}{|D|}\n\\]\n\n\n\nThe algorithm is as follows:\n\nDiscretize each feature in [min,max] range.\nPick the question that has the largest information gain.\nRepeat the procedure for \\(D_{yes}\\) and \\(D_{no}\\).\nStop growing the tree if a node becomes sufficiently “pure”.\n\nThe goodness of a question can also be measured using different methods like the Gini Index, etc.\nPictorial Depiction of decision boundary and its decision tree:\n\n\n\nDecision Boundary"
  },
  {
    "objectID": "pages/dec_trees.html#goodness-of-a-question",
    "href": "pages/dec_trees.html#goodness-of-a-question",
    "title": "Decision tree",
    "section": "",
    "text": "Let \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset. We partition it using a question into \\(D_{yes}\\) and \\(D_{no}\\).\nWhat we need is a measure of “Impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). This measure can be given by various ways, but we will use the Entropy Function.\nThe Entropy function is given by, \\[\nEntropy(\\{y_1, \\ldots, y_n\\}) = Entropy(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\] where conventionally \\(\\log(0)\\) is treated as \\(0\\).\nPictorial Representation of the Entropy function:\n\n\n\nEntropy Function\n\n\nThen, we use Information Gain to measure the goodness of the split.\nInformation gain is a commonly used criterion in decision tree algorithms that measures the reduction in entropy or impurity of a dataset after splitting based on a given feature. By selecting features with high information gain, decision trees can effectively differentiate between the different classes of data and make accurate predictions.\nInformation gain is given by, \\[\n\\text{Information Gain}(feature,value)=Entropy(D) - \\left [ \\gamma Entropy(D_{yes})+(1-\\gamma)Entropy(D_{no}) \\right ]\n\\] where \\(\\gamma\\) is given by, \\[\n\\gamma=\\frac{|D_{yes}|}{|D|}\n\\]"
  },
  {
    "objectID": "pages/dec_trees.html#decision-tree-algorithm",
    "href": "pages/dec_trees.html#decision-tree-algorithm",
    "title": "Decision tree",
    "section": "",
    "text": "The algorithm is as follows:\n\nDiscretize each feature in [min,max] range.\nPick the question that has the largest information gain.\nRepeat the procedure for \\(D_{yes}\\) and \\(D_{no}\\).\nStop growing the tree if a node becomes sufficiently “pure”.\n\nThe goodness of a question can also be measured using different methods like the Gini Index, etc.\nPictorial Depiction of decision boundary and its decision tree:\n\n\n\nDecision Boundary"
  },
  {
    "objectID": "pages/naive_bayes.html",
    "href": "pages/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "A Generative Model Based Algorithm tries to model the probability distribution of the input data and then generates new samples from this distribution. It involves learning the joint probability distribution of the features and labels of the training data, and then using this model to make predictions for new, unseen data.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nGeneral Steps in the algorithm are:\n\nDecide the labels by tossing a coin \\(P(y_i=1)=p\\).\nDecide features using the labels in step 1 using \\(P(x_i|y_i)\\).\n\nThe parameters in the model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(x|y=1)\\): \\(2^d-1\\)\nParameters for \\(P(x|y=0)\\): \\(2^d-1\\)\n\nHence, the total number of parameters \\[\\begin{align*}\n    &=1 + (2^d-1) + (2^d-1)\\\\\n    &=1 + 2(2^d-1)\\\\\n    &=2^{d+1}-1\n\\end{align*}\\]\nIssues:\n\nToo many parameters.\nNot a reasonable model.\n\n\n\nAn alternate model begins with the class conditional independence assumption. The class conditional independence assumption is a common assumption made in machine learning algorithms that assumes the features of an object are conditionally independent given its class label.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nGeneral Steps in the algorithm are:\n\nDecide the labels by tossing a coin \\(P(y_i=1)=p\\).\nDecide features for \\(x\\) given \\(y\\) using: \\[\nP(x = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^y_i)^{f_i}(1-p^y_i)^{1-f_i}\n\\]\n\nThe parameters in the model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(x|y=1)\\): \\(d\\)\nParameters for \\(P(x|y=0)\\): \\(d\\)\n\nHence, the total number of parameters \\[\\begin{align*}\n    &=1 + d + d\\\\\n    &=2d+1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/naive_bayes.html#alternate-generative-model",
    "href": "pages/naive_bayes.html#alternate-generative-model",
    "title": "Naive Bayes",
    "section": "",
    "text": "An alternate model begins with the class conditional independence assumption. The class conditional independence assumption is a common assumption made in machine learning algorithms that assumes the features of an object are conditionally independent given its class label.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nGeneral Steps in the algorithm are:\n\nDecide the labels by tossing a coin \\(P(y_i=1)=p\\).\nDecide features for \\(x\\) given \\(y\\) using: \\[\nP(x = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^y_i)^{f_i}(1-p^y_i)^{1-f_i}\n\\]\n\nThe parameters in the model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(x|y=1)\\): \\(d\\)\nParameters for \\(P(x|y=0)\\): \\(d\\)\n\nHence, the total number of parameters \\[\\begin{align*}\n    &=1 + d + d\\\\\n    &=2d+1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/naive_bayes.html#prediction-using-the-parameters",
    "href": "pages/naive_bayes.html#prediction-using-the-parameters",
    "title": "Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(x^{test}\\in\\{0,1\\}^d\\), prediction for \\(\\hat{y}^{test}\\) is done using the following: \\[\nP(\\hat{y}^{test}=1|x^{test}) \\ge P(\\hat{y}^{test}=0|x^{test})\n\\] If the above is true, \\(\\hat{y}^{test}=1\\), otherwise \\(0\\).\nUsing Bayes rule, we can get the values for \\(P(\\hat{y}^{test}=1|x^{test})\\) and \\(P(\\hat{y}^{test}=0|x^{test})\\): \\[\\begin{align*}\nP(\\hat{y}^{test}=1|x^{test})&=\\frac{P(x^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(x^{test})}\\\\\nP(\\hat{y}^{test}=0|x^{test})&=\\frac{P(x^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(x^{test})}\n\\end{align*}\\] As we predict by comparing the two values, we can do this without actually solving for \\(P(x^{test})\\).\nSolving for \\(P(x^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we get, \\[\\begin{align*}\n&=P(x^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1)\\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\] Similarly we solve for \\(P(x^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, if \\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\] we predict \\(y^{test}=1\\), othewise \\(y^{test}=0\\).\nThe model implements two main things:\n\nClass Conditional Independence Assumption\nBayes Rule\n\nTherefore, we call this algorithm Naive Bayes.\nIn short, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It calculates the probability of a sample belonging to a class by estimating the conditional probability of each feature given the class and then multiplying them together using Bayes’ theorem. Despite its simple assumption, Naive Bayes is known to perform well in various applications, particularly when there are many features but relatively few training examples."
  },
  {
    "objectID": "pages/naive_bayes.html#pitfalls-of-naive-bayes",
    "href": "pages/naive_bayes.html#pitfalls-of-naive-bayes",
    "title": "Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nThe most prominent issue with Naive Bayes is that if a feature is not seen in the training set but seen in the testing set, the prediction probability for both the classes would be zero. \\[\\begin{align*}\nP(\\hat{y}^{test}=1|x^{test} = [f_1, f_2, \\ldots, f_d])&\\propto\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\\\\\nP(\\hat{y}^{test}=0|x^{test} = [f_1, f_2, \\ldots, f_d])&\\propto\\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\] Even if one feature \\(f_i\\) was zero in training set, we get \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), which ultimately results in \\(P(\\hat{y}^{test}=0|x^{test})=P(\\hat{y}^{test}=1|x^{test})=0\\).\nA popular fix for this is to introduce two “pseudo” datapoints with labels \\(1\\) and \\(0\\) each into the dataset whose features are all ones. This technique is also known as Laplace smoothing.\nBriefly speaking, Laplace smoothing is a technique used to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By adding this smoothing term, the model becomes more robust and can handle unseen data more effectively."
  },
  {
    "objectID": "pages/naive_bayes.html#prediction-using-bayes-rule",
    "href": "pages/naive_bayes.html#prediction-using-bayes-rule",
    "title": "Naive Bayes",
    "section": "Prediction using Bayes Rule",
    "text": "Prediction using Bayes Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|x_{test})\\propto P(x_{test}|y_{test})*P(y_{test})\n\\] where \\(P(x_{test}|y_{test})\\equiv f(x_{test};\\hat{\\mu}_{y_{test}}, \\hat{\\Sigma})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nPredict \\(y_{test}=1\\) if: \\[\\begin{align*}\nf(x_{test};\\hat{\\mu}_1, \\hat{\\Sigma})*\\hat{p}&\\ge f(x_{test};\\hat{\\mu}_0, \\hat{\\Sigma})*(1-\\hat{p}) \\\\\ne^{-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_1)}*\\hat{p}&\\ge e^{-(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_0)}*(1-\\hat{p}) \\\\\n-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_1)+\\log(\\hat{p})&\\ge -(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] \\[\n\\left( (\\hat{\\mu}_1-\\hat{\\mu}_0)^T\\hat{\\Sigma}^{-1} \\right)x_{test} + \\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0 - \\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, we can say that the decision function is of the form \\(w^Tx+b\\ge0\\) where \\(w\\in\\mathbb{R}^d\\), \\(w_i= (\\hat{\\mu}_1-\\hat{\\mu}_0)^T\\hat{\\Sigma}^{-1}\\) and \\(b=\\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0 - \\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + log(\\frac{1-\\hat{p}}{\\hat{p}})\\).\nTherefore, the decision function of Gaussian Naive Bayes when the covariance matrix is equal for both classes is Linear."
  },
  {
    "objectID": "pages/naive_bayes.html#decision-boundaries-for-different-covariances",
    "href": "pages/naive_bayes.html#decision-boundaries-for-different-covariances",
    "title": "Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As seen previously, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is linear as well as the perpendicular bisector of the line drawn from \\(\\hat{\\mu}_1\\) to \\(\\hat{\\mu}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\Sigma}_1\\) and \\(\\hat{\\Sigma}_0\\) be the covariance matrices for classes \\(1\\) and \\(0\\) respectively. They are given by, \\[\\begin{align*}\n\\hat{\\Sigma}_1 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)^T \\\\\n\\hat{\\Sigma}_0 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)^T\n\\end{align*}\\] Predict \\(y_{test}=1\\) if: \\[\\begin{align*}\nf(x_{test};\\hat{\\mu}_1, \\hat{\\Sigma_1})*\\hat{p}&\\ge f(x_{test};\\hat{\\mu}_0, \\hat{\\Sigma_0})*(1-\\hat{p}) \\\\\ne^{-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_1)}*\\hat{p}&\\ge e^{-(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_0)}*(1-\\hat{p}) \\\\\n-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_1)+\\log(\\hat{p})&\\ge -(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma_0}(x_{test}-\\hat{\\mu}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] \\[\nx_{test}^T(\\hat{\\Sigma}_1^{-1}-\\hat{\\Sigma}_0^{-1})x_{test}-2(\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}-\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1})x_{test}+(\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\hat{\\mu}_0-\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}\\hat{\\mu}_1) + log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, we can say that the decision function is of the form \\(x^TQx-2b^Tx+c\\ge0\\) where \\(Q=\\hat{\\Sigma}_1^{-1}-\\hat{\\Sigma}_0^{-1}\\), \\(b=\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}-\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\), and \\(c=(\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\hat{\\mu}_0-\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}\\hat{\\mu}_1) + log(\\frac{1-\\hat{p}}{\\hat{p}})\\). Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes"
  },
  {
    "objectID": "pages/percp.html",
    "href": "pages/percp.html",
    "title": "Perceptron",
    "section": "",
    "text": "The Perceptron Learning Algorithm is a type of supervised learning algorithm used for binary classification tasks. It involves iteratively adjusting the weights of a linear combination of input features until a decision boundary that separates the two classes is found. The algorithm is a type of discriminative classification as it directly models the boundary between classes rather than modeling the underlying probability distribution of each class.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nAssumptions made in the algorithm:\nThe objective function is given by, \\[\n\\min _{h \\in \\mathcal{H}} \\sum _{i=1} ^n \\mathbb{1}\\left ( h(x_i) \\ne y_i \\right )\n\\] In general, this is an NP-Hard Problem even if \\(\\mathcal{H}\\) only accounts for the Linear Hypotheses.\nUnder the Linear Separability Assumption, let \\(\\exists w \\in \\mathbb{R}^d\\) s.t. \\(\\text{sign}(w^Tx_i)=y_i\\) \\(\\forall i \\in [n]\\).\nWe solve this problem of convergence using an iterative algorithm. The algorithm is as follows:"
  },
  {
    "objectID": "pages/percp.html#analysis-of-the-update-rule",
    "href": "pages/percp.html#analysis-of-the-update-rule",
    "title": "Perceptron",
    "section": "Analysis of the Update Rule",
    "text": "Analysis of the Update Rule\nFor a training example \\((x, y)\\), where \\(x\\) is the input and \\(y\\) is the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates its weight vector \\(w\\) as follows:\n\nIf the prediction of the perceptron on \\(x\\) is correct (i.e., \\(\\text{sign}(w^Tx_i)==y_i\\)), then no update is performed.\nIf the prediction of the perceptron on \\(x\\) is incorrect (i.e., \\(\\text{sign}(w^Tx_i)\\ne y_i\\)), then the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(w^{(t+1)} = w^t + x_iy_i\\).\n\nThis update rule effectively moves the decision boundary in the direction of the correct classification for the misclassified example. It is guaranteed to converge to a linearly separable solution if the data is linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution."
  },
  {
    "objectID": "pages/percp.html#further-assumptions",
    "href": "pages/percp.html#further-assumptions",
    "title": "Perceptron",
    "section": "Further Assumptions",
    "text": "Further Assumptions\nWe make three further assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) is linearly separable with \\(\\gamma\\)-margin if \\(\\exists w^* \\in \\mathbb{R}^d\\) s.t. \\((w^{*T}x_i)y_i\\ge\\gamma\\) \\(\\forall i\\) for some \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let some \\(R&gt;0 \\in \\mathbb{R}\\), \\(\\forall i \\in D\\) \\(||x_i||\\le R\\). In short, let \\(R\\) be the length of the datapoint furthest from the center.\nNormal Length for \\(w^*\\): Let \\(w^*\\) be of unit length."
  },
  {
    "objectID": "pages/log_reg.html",
    "href": "pages/log_reg.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Until now, we used the \\(\\text{sign}\\) function to get the class for the output. But can we also provide the probabilities for these outputs?\nLet \\(z=w^Tx\\) and \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? For this, we use the Sigmoid Function. It is given by, \\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is often used in machine learning as an activation function for neural networks. It has a characteristic S-shaped curve, which makes it useful for modeling processes that have a threshold or saturation point, such as logistic growth or binary classification problems.\nWhen the input value is large and positive, the sigmoid function output approaches 1, and when the input value is large and negative, the sigmoid function output approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” comes from the Greek word “sigmoides,” which means “shaped like the letter sigma” (\\(\\Sigma\\)). The letter sigma has a similar shape to the sigmoid function’s characteristic S-shaped curve, which is likely the reason for the function’s name."
  },
  {
    "objectID": "pages/log_reg.html#sigmoid-function",
    "href": "pages/log_reg.html#sigmoid-function",
    "title": "Logistic regression",
    "section": "",
    "text": "Until now, we used the \\(\\text{sign}\\) function to get the class for the output. But can we also provide the probabilities for these outputs?\nLet \\(z=w^Tx\\) and \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? For this, we use the Sigmoid Function. It is given by, \\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is often used in machine learning as an activation function for neural networks. It has a characteristic S-shaped curve, which makes it useful for modeling processes that have a threshold or saturation point, such as logistic growth or binary classification problems.\nWhen the input value is large and positive, the sigmoid function output approaches 1, and when the input value is large and negative, the sigmoid function output approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” comes from the Greek word “sigmoides,” which means “shaped like the letter sigma” (\\(\\Sigma\\)). The letter sigma has a similar shape to the sigmoid function’s characteristic S-shaped curve, which is likely the reason for the function’s name."
  },
  {
    "objectID": "pages/log_reg.html#logistic-regression",
    "href": "pages/log_reg.html#logistic-regression",
    "title": "Logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables, which can be either continuous or categorical. The goal of logistic regression is to estimate the probability that the dependent variable is one of the two possible values, given the values of the independent variables.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic(sigmoid) function, which produces an S-shaped curve that ranges between 0 and 1. The logistic function transforms the output of a linear combination of the independent variables into a probability estimate, which can then be used to classify new observations.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know, \\[\nP(y=1|x) = g(w^Tx_i) = \\frac{1}{1+e^{-w^Tx}}\n\\] Using Maximum Likelihood, we get \\[\\begin{align*}\n\\mathcal{L}(w;\\text{Data}) &= \\prod _{i=1} ^{n} (g(w^Tx_i))^{y_i}(1- g(w^Tx_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(w^Tx_i))+(1-y_i)\\log(1- g(w^Tx_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-w^Tx_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-w^Tx_i) - \\log(1+e^{-w^Tx_i}) \\right ]\n\\end{align*}\\] Therefore, our objective, which is maximizing the log-likelihood function, is given by, \\[\n\\max _{w}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-w^Tx_i) - \\log(1+e^{-w^Tx_i}) \\right ]\n\\] But, there is no closed form solution for this. And hence, we use gradient descent for convergence.\nThe gradient is given by, \\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-x_i) - \\left( \\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \\right ) (-x_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -x_i + x_iy_i + x_i \\left( \\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ x_iy_i - x_i \\left( \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ x_i \\left(y_i - \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right ]\n\\end{align*}\\] Using the Gradient Descent update rule, we get, \\[\\begin{align*}\nw_{t+1} &= w_t + \\eta_t\\nabla \\log(\\mathcal{L}(w;\\text{Data})) \\\\\n&= w_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} x_i \\left(y_i - \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right )\n\\end{align*}\\]\n\nKernel and Regularized Versions\nWe can argue that \\(w^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_ix_i\\), and therefore, can be Kernelized. For further details, refer to this link.\nThe regularized version is given by, \\[\n\\min _{w}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-w^Tx_i}) + w^Tx_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||w||^2\n\\] where \\(\\frac{\\lambda}{2}||w||^2\\) is the regualizer and \\(\\lambda\\) is found using cross-validation."
  },
  {
    "objectID": "pages/svm.html",
    "href": "pages/svm.html",
    "title": "Support vector machines (SVMs)",
    "section": "",
    "text": "Let dataset \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be linearly separable with \\(\\gamma\\)-margin where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{-1, 1\\}\\).\nLet \\(w* \\in \\mathbb{R}^d\\) be the weight vector s.t. \\((w^{*T}x_i)y_i\\ge\\gamma\\) \\(\\forall i\\).\nLet some \\(R&gt;0 \\in \\mathbb{R}\\), s.t. \\(\\forall i\\) \\(||x_i||\\le R\\).\nTherefore, the number of mistakes made by the algorithm is given by, \\[\n\\text{\\#mistakes} \\le \\frac{R^2}{\\gamma^2}\n\\]\nObservations\nLet \\(w_{perc}\\) be any weight vector which can linearly separate the dataset.\nTherefore, we observe the following:\n\n“Quality” of the solution depends on the margin.\nNumber of mistakes depend on \\(w^*\\)’s margin.\n\\(w_{perc}\\) need not necessarily be \\(w^*\\).\n\nHence, our goal should be to find the solution that maximizes the margin.\n\n\nFrom the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\)."
  },
  {
    "objectID": "pages/svm.html#margin-maximization",
    "href": "pages/svm.html#margin-maximization",
    "title": "Support vector machines (SVMs)",
    "section": "",
    "text": "From the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\)."
  },
  {
    "objectID": "pages/svm.html#hard-margin-svm-algorithm",
    "href": "pages/svm.html#hard-margin-svm-algorithm",
    "title": "Support vector machines (SVMs)",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThis algorithm only works if the dataset is linearly separable with a \\(\\gamma &gt; 0\\).\n\nCalculate \\(Q=X^TX\\) directly or using a kernel as per the dataset.\nUse the gradient of the dual formula (\\(\\alpha^T1 - \\frac{1}{2}\\alpha^TY^TQY\\alpha\\)), in the gradient descent algorithm to find a satisfactory \\(\\alpha\\). Let the intial \\(\\alpha\\) be a zero vector \\(\\in \\mathbb{R}^n_+\\).\nTo predict:\n\nFor non-kernelized SVM: \\(\\text{label}(x_{test}) = w^Tx_{test} = \\sum _{i=1} ^n \\alpha _i y_i(x_i^Tx_{test})\\)\nFor kernelized SVM: \\(\\text{label}(x_{test}) = w^T\\phi(x_{test}) = \\sum _{i=1} ^n \\alpha _i y_ik(x_i^Tx_{test})\\)"
  }
]