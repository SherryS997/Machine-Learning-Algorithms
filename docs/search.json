[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLA",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/Wk01.html",
    "href": "pages/Wk01.html",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "href": "pages/Wk01.html#broad-paradigms-of-machine-learning",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Broad Paradigms of Machine Learning",
    "text": "Broad Paradigms of Machine Learning\n\nSupervised Learning:Supervised Machine Learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data. Few examples:\n\n\nLinear regression for predicting a continuous output\nLogistic regression for binary classification problems\nDecision trees for non-linear classification and regression problems\nSupport Vector Machines for binary and multi-class classification problems\nNeural Networks for complex non-linear problems in various domains such as computer vision, natural language processing, and speech recognition\n\n\nUnsupervised Learning: Unsupervised Machine Learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning that only the inputs are provided and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance. Few examples:\n\n\nClustering algorithms such as K-means, hierarchical clustering, and density-based clustering, used to group similar data points together into clusters\nDimensionality reduction techniques such as Principal Component Analysis (PCA), used to reduce the number of features in a dataset while preserving the maximum amount of information\nAnomaly detection algorithms used to identify unusual data points that deviate from the normal patterns in the data\n\n\nSequential learning: Sequential Machine Learning (also known as time-series prediction) is a type of machine learning that is focused on making predictions based on sequences of data. It involves training the model on a sequence of inputs, such that the predictions for each time step depend on the previous time steps. Few examples:\n\n\nTime series forecasting, used to predict future values based on past trends and patterns in data such as stock prices, weather patterns, and energy consumption\nSpeech recognition, used to transcribe speech into text by recognizing patterns in audio signals\nNatural language processing, used to analyze and make predictions about sequences of text data"
  },
  {
    "objectID": "pages/Wk01.html#potential-algorithm",
    "href": "pages/Wk01.html#potential-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Potential Algorithm",
    "text": "Potential Algorithm\nWith this information, we can give the following algorithm:\nGiven a dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset \\[\n\\mu = \\frac{1}{n} \\sum _{i=1} ^{n} x_i\n\\] \\[\nx_i = x_i - \\mu  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(w \\in \\mathbb{R}^d\\) and \\(||w|| = 1\\).\nReplace \\(x_i = x_i - (x_i^Tw)w \\hspace{1em} \\forall i\\)\nRepeat the first three steps until residues become zero and we obtain \\(w_2, w_3, \\ldots, w_d\\).\n\nBut is this the best way? How many \\(w\\) do we need for optimal compression?"
  },
  {
    "objectID": "pages/Wk01.html#approximate-representation",
    "href": "pages/Wk01.html#approximate-representation",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nIf the data can be approximately represented by a lower sub-space, is it enough that we use only those \\(k\\) projections? How much variance should be covered?\nGiven a centered dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\), let \\(C\\) be its covariance matrix, and let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be its eigenvalues, which are non-negative because the covariance matrix is a positive semi-definite matrix, placed in descending order, and let \\(\\{w_1, w_2, \\ldots, w_d \\}\\) be its corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be given by, \\[\\begin{align*}\n    Cw &= \\lambda w \\\\\n    w^TCw &= w^T\\lambda w\\\\\n    \\therefore \\lambda &= w^TCw \\hspace{2em} \\{w^Tw = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (x_i^Tw)^2 \\\\\n\\end{align*}\\] Therefore, as the mean is zero, \\(\\lambda\\) gives the variance captured by the eigenvector \\(w\\).\nA good rule of thumb is that the variance captured by P.C.A. should be at least 95%. If the first \\(K\\) eigenvectors capture the required variance, this is given by, \\[\n\\frac{\\displaystyle \\sum _{k=1} ^{K} \\lambda_k}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\] Hence, the higher the variance captured, the lower is the error obtained."
  },
  {
    "objectID": "pages/Wk01.html#p.c.a.-algorithm",
    "href": "pages/Wk01.html#p.c.a.-algorithm",
    "title": "Introduction; Unsupervised Learning - Representation learning - PCA",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nGiven a centered dataset \\(\\{x_1, x_2, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{d}\\), let \\(C\\) be its covariance matrix.\nThe algorithm is as follows:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(C\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be its eigenvalues placed in the descending order, and let \\(\\{w_1, w_2, \\ldots, w_d \\}\\) be its corresponding eigenvectors of unit length.\nStep 2: Calculate \\(K\\), where \\(K\\) is the number of required top eigenvalues and eigenvectors, according to the required variance that needs to be covered.\nStep 3: Project the data onto the eigenvectors, and obtain the required representation as a linear combination of those projections.\n\nIn short, we can say that P.C.A. is a dimensionality reduction technique that finds combination of features that are de-correlated (independent of each other)."
  },
  {
    "objectID": "pages/Wk02.html",
    "href": "pages/Wk02.html",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk02.html#transforming-features",
    "href": "pages/Wk02.html#transforming-features",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nWe solve the problem of non-linear relationships by mapping them to higher dimensions. \\[\nx \\to \\phi(x) \\hspace{2em} \\mathbb{R} ^d \\to \\mathbb{R} ^D \\hspace{2em} \\text{where } [D &gt;&gt; d]\n\\] To compute \\(D\\):\nLet \\(x=\\left [ \\begin{array} {cc}  f_1 & f_2 \\end{array} \\right ]\\) be features of a dataset containing datapoints lying on a curve of degree two in a two-dimensional space.\nTo make it linear from quadratic, we map the features to \\(\\phi(x)=\\left [ \\begin{array} {cccccc}  1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2 \\end{array} \\right ]\\)\nMapping \\(d\\) features to the polygonial power \\(p\\) gives \\(^{d+p} C_d\\) new features.\nIssue: Finding \\(\\phi(x)\\) may be very hard.\nSolution for this issue is in the next point."
  },
  {
    "objectID": "pages/Wk02.html#kernel-functions",
    "href": "pages/Wk02.html#kernel-functions",
    "title": "Unsupervised Learning - Representation learning - Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function that maps \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\), and is a “valid”, is called a Kernel Function.\nProof of a “Valid” Kernel:\n\nMethod 1: Exhibit the map to \\(\\phi\\) explicitly. [may be hard]\nMethod 2: Using Mercer’s Theorem:\n\n\\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a “valid” kernel if and only if:\n\n\\(k\\) is symmetric i.e \\(k(x,x') = k(x',x)\\)\nFor any dataset \\(\\{x_1,x_2,\\ldots,x_n \\}\\), the matrix \\(K \\in \\mathbb{R}^{n \\times n}\\), where \\(K_{ij} = k(i,j)\\), is Positive Semi-Definite.\n\n\n\nTwo Popular Kernel Functions:\n\nPolynomial Kernel: \\(k(x,x') = (x^Tx + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(exp(-\\frac{||x-x'||^2}{2\\sigma^2})\\)"
  },
  {
    "objectID": "pages/Wk03.html",
    "href": "pages/Wk03.html",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk03.html#the-algorithm",
    "href": "pages/Wk03.html#the-algorithm",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm is as follows:\nStep 1: Initialization: Assign \\(z_1^0, z_2^0, \\ldots, z_n^0\\) where \\(z_i^0 \\in \\{1, 2, \\ldots, k\\}\\). The approach on how to initialize them is discussed later.\nStep 2: Compute Means: \\[\n\\mu _k ^t = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {x_i \\cdot \\mathbf{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 3: Reassignment Step: \\[\nz _i ^{t+1} = \\underset{k}{\\arg \\min} {|| x_i - \\mu _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until convergence for \\(t\\) iterations."
  },
  {
    "objectID": "pages/Wk03.html#fact-regarding-lloyds-algorithm",
    "href": "pages/Wk03.html#fact-regarding-lloyds-algorithm",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "Fact regarding Lloyd’s Algorithm",
    "text": "Fact regarding Lloyd’s Algorithm\nLloyd’s Algorithm, also known as K-means, is guaranteed to converge to a solution. While the converged solution may not be the optimal one, it has been observed to produce acceptable clustering results in practice."
  },
  {
    "objectID": "pages/Wk03.html#k-means",
    "href": "pages/Wk03.html#k-means",
    "title": "Unsupervised Learning - Clustering - K-means/Kernel K-means",
    "section": "K-means++",
    "text": "K-means++\nThe premise is to select centroids that are as far as possible from each other.\n\nStep 1: Choose \\(\\mu _1 ^0\\) randomly from the dataset.\nStep 2: For \\(l \\in \\{2, 3, \\ldots, k\\}\\), choose \\(\\mu _l ^0\\) probablistically proportional to score(\\(S\\)) where \\(S\\) is, \\[\n  S(x) = \\min _{\\{j=1, 2, \\ldots, l-1\\}} {|| x - \\mu _{j} ^0 ||}^2 \\hspace{2em} \\forall x\n\\] The probabilistic aspect of the algorithm provides an expected guarantee of optimal convergence in K-means. The guarantee is given by, \\[\n  \\mathbb{E} \\left[ \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2 \\right ]\n  \\le O(\\log k) \\left [ \\min _{\\{z_1, z_2, \\ldots, z_n\\}} \\sum _{i=1} ^{n} {|| x_i - \\mu _{z_i} ||}^2 \\right ]\n\\] where \\(O(\\log k)\\) is a constant of order \\(\\log k\\).\nStep 3: Once the centroids are determined, we proceed with Lloyd’s Algorithm."
  },
  {
    "objectID": "pages/Wk04.html",
    "href": "pages/Wk04.html",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/Wk04.html#fishers-principle-of-maximum-likelihood",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate the parameters of a statistical model by choosing the parameter values that maximize the likelihood function, which measures how well the model fits the observed data.\nApplying the likelihood function on the above dataset, we get \\[\\begin{align*}\n\\mathcal{L}(p;\\{x_1, x_2, \\ldots, x_n\\}) &= P(x_1, x_2, \\ldots, x_n;p)\\\\\n&=p(x_1;p)p(x_2;p)\\ldots p(x_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{x_1, x_2, \\ldots, x_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{ML} &= \\frac{1}{n}\\sum _{i=1} ^n x_i\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#likelihood-estimation-on-gaussian-distributions",
    "href": "pages/Wk04.html#likelihood-estimation-on-gaussian-distributions",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Likelihood Estimation on Gaussian Distributions",
    "text": "Likelihood Estimation on Gaussian Distributions\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\). We assume that the datapoints are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\mu, \\sigma^2;\\{x_1, x_2, \\ldots, x_n\\}) &= f_{x_1, x_2, \\ldots, x_n}(x_1, x_2, \\ldots, x_n;\\mu, \\sigma^2) \\\\\n&=\\prod _{i=1} ^n  f_{x_i}(x_i;\\mu, \\sigma^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{\\frac{-(x_i-\\mu)^2}{2\\sigma^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{x_1, x_2, \\ldots, x_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\sigma}  \\right ) - \\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{Differentiating wrt $\\mu$ and $\\sigma$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\mu}_{ML} &= \\frac{1}{n}\\sum _{i=1} ^n x_i \\\\\n\\hat{\\sigma^2}_{ML} &= \\frac{1}{n}\\sum _{i=1} ^n (x_i-\\mu)^2\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/Wk04.html#convexity-and-jensens-inequality",
    "href": "pages/Wk04.html#convexity-and-jensens-inequality",
    "title": "Unsupervised Learning - Estimation - Recap of MLE + Bayesian estimation, Gaussian Mixture Model - EM algorithm",
    "section": "Convexity and Jensen’s Inequality",
    "text": "Convexity and Jensen’s Inequality\nConvexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/Wk05.html",
    "href": "pages/Wk05.html",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk05.html#stochastic-gradient-descent",
    "href": "pages/Wk05.html#stochastic-gradient-descent",
    "title": "Supervised Learning - Regression - Least Squares; Bayesian view",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm used in machine learning for finding the parameters that minimize the loss function of a model. In contrast to traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters based on a randomly selected subset of the data, known as a batch. This results in faster training times and makes SGD particularly useful for large datasets.\nFor every step \\(t\\), rather than updating \\(w\\) using the entire dataset, we use a small randomly selected (\\(k\\)) data points to update \\(w\\). Therefore, the new gradient is \\(2(\\tilde{X}\\tilde{X}^Tw^t - \\tilde{X}\\tilde{y})\\) where \\(\\tilde{X}\\) and \\(\\tilde{y}\\) is the small sample randomly selected from the dataset. This is manageable because \\(\\tilde{X} \\in \\mathbb{R}^{d \\times k}\\) which is considerably smaller than \\(X\\).\nAfter T rounds, we use, \\[\nw ^T _{SGD} = \\frac{1}{T}  \\sum _{i=1} ^T w^i\n\\] This has a certain guarantee to have optimal convergence partly because of the randomness involved in it."
  },
  {
    "objectID": "pages/Wk06.html",
    "href": "pages/Wk06.html",
    "title": "Supervised Learning - Regression - Ridge/LASSO",
    "section": "",
    "text": "PDF Link: notes\n\nGoodness of Maximum Likelihood Estimator for Linear Regression\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels, where \\(y_i \\in \\mathbb{R}\\). \\[\ny|X = w^Tx + \\epsilon\n\\] where \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) and \\(w \\in \\mathbb{R}^d\\). Let \\(\\hat{w}_{ML}\\) signify the maximum likelihood parameter for linear regression. \\[\n\\hat{w}_{ML}=w^*=(XX^T)^+Xy\n\\] \\(\\therefore\\) To measure how good our parameter is, we use the follow: \\[\n\\mathbb{E} [|| \\hat{w}_{ML} - w ||^2_2]\n\\] This is known as the Mean Squared Error (MSE) and turns out to be equal to \\[\n\\mathbb{E} [|| \\hat{w}_{ML} - w ||^2_2] = \\sigma^2 *trace((XX^T)^{-1})\n\\]\n\n\nCross-Validation for Minimizing MSE\nLet the eigenvalues of \\(XX^T\\) be \\(\\{\\lambda_1, \\ldots, \\lambda_d\\}\\). Hence the eigenvalues of \\((XX^T)^{-1}\\) are \\(\\{\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_d}\\}\\).\n\\(\\therefore\\) The MSE is, \\[\n\\mathbb{E} [|| \\hat{w}_{ML} - w ||^2_2] = \\sigma^2 \\sum_{i=1}^d  \\frac{1}{\\lambda_i}\n\\] Consider the following estimator, \\[\n\\hat{w}_{new}=(XX^T + \\lambda I)^{-1}Xy\n\\] where \\(\\lambda \\in \\mathbb{R}\\) and \\(I \\in \\mathbb{R}^{d\\times d}\\) is the Identity Matrix. Using this we get, \\[\ntrace((XX^T + \\lambda I)^{-1}) = \\sum_{i=1}^d  \\frac{1}{\\lambda_i + \\lambda}\n\\] According to the Existence Theorem, \\(\\exists\\lambda\\) s.t. \\(\\hat{w}_{new}\\) has lesser means square error than \\(\\hat{w}_{ML}\\).\nIn practice, we find \\(\\lambda\\) using cross validation.\nThree popular techniques of Cross Validation are:\n\nTraining-Validation Split: The training set is randomly split into training and validation set, usually in the ratio \\(80:20\\). From among various \\(\\lambda\\)s, we choose the one with gives the least error.\nK-Fold Cross Validation: It is done by dividing the training set into K equally-sized parts, training the model K times on different (K-1) parts, and evaluating it on the remaining part. From among various \\(\\lambda\\)s, we choose the one with gives the least average error.\nLeave One Out Cross Validation: It is done by training the model on all but one of the samples in the training set and evaluating it on the left-out sample, repeating this process for each sample in the dataset. From among various \\(\\lambda\\)s, we choose the one with gives the least average error.\n\n\n\nBayesian Modeling\nAn alternate way to understand \\(\\hat{w}_{ML}\\) is through Bayesian Modeling.\nLet \\(P(y|X)\\sim \\mathcal{N}(w^Tx,I)\\). We use \\(I\\), the identity matrix, instead of \\(\\sigma^2\\) for simplicity.\nA good choice of prior for \\(w\\) is \\(\\mathcal{N}(0,\\gamma^2I)\\), where \\(\\gamma\\in\\mathbb{R}^d\\).\nTherefore, we get, \\[\\begin{align*}\nP(w|\\{(x_1, y_1), \\ldots, (x_n,y_n)\\})&\\propto P(\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}|w)*P(w)\\\\\n&\\propto \\left ( \\prod _{i=1} ^n e^{\\frac{-(y_i - w^Tx_i)^2}{2}}  \\right ) * \\left ( \\prod _{i=1} ^d  e^{\\frac{-(w_i - 0)^2}{2\\gamma^2}} \\right )\\\\\n&\\propto \\left ( \\prod _{i=1} ^n e^{\\frac{-(y_i - w^Tx_i)^2}{2}}  \\right ) * \\left ( e^{-\\sum _{i=1} ^d\\frac{w_i^2}{2\\gamma^2}} \\right ) \\\\\n&\\propto \\left ( \\prod _{i=1} ^n e^{\\frac{-(y_i - w^Tx_i)^2}{2}}  \\right ) * e^{\\frac{-||w||^2}{2\\gamma^2}} \\\\\n\\log(P(w|\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}))&\\propto \\frac{-(y_i - w^Tx_i)^2}{2}-\\frac{||w||^2}{2\\gamma^2}\n\\end{align*}\\] \\[\n\\text{Taking the gradient, we get}\n\\] \\[\\begin{align*}\n\\nabla \\log(P(w|\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}))&\\propto  (XX^T)\\hat{w}_{MAP} - Xy + \\frac{\\hat{w}_{MAP}}{\\gamma^2} \\\\\n\\therefore \\hat{w}_{MAP}&=(XX^T + \\frac{1}{\\gamma^2} I)^{-1}Xy\n\\end{align*}\\] where \\(\\hat{w}_{MAP}\\) is the Maximum a posteriori Estimate. In practice, the value for \\(\\frac{1}{\\gamma^2}\\) is acquired using cross validation.\nHence, Maximum a posteriori Estimation for linear regression with a Gaussian Prior \\(\\mathcal{N}(0,\\gamma^2I)\\) for \\(w\\) is equivalent to the “new” estimator we used previously.\n\n\nRidge Regression\nRidge regression is a type of linear regression that adds a penalty term to the ordinary least squares method to mitigate multicollinearity and overfitting.\nIts objective function is given by, \\[\n\\min_{w\\in \\mathbb{R}^d} \\sum^n_{i=1}(w^Tx_i-y_i)^2 + \\lambda||w||_2^2\n\\] where \\(\\lambda||w||_2^2\\) is the regularizer, and \\(||w||_2^2\\) is the squared L2 Norm of \\(w\\). Let this equation be given by \\(f(w)\\).\nSubsequently, this is also equivalent to, \\[\n\\min_{w\\in \\mathbb{R}^d} \\sum^n_{i=1}(w^Tx_i-y_i)^2 \\hspace{1em}\\text{s.t.}||w||_2^2\\le\\theta\n\\] where \\(\\theta\\) is dependent on \\(\\lambda\\).\nIn conclusion, for every choice of \\(\\lambda&gt;0\\), \\(\\exists \\theta\\) s.t. there are optimal solutions to our objective function.\nThe loss function of the linear regression of \\(w_{ML}\\) is given by, \\[\nf(w_{ML}) = \\sum^n_{i=1}(w_{ML}^Tx_i-y_i)^2\n\\] Consider the set of all \\(w\\) s.t. \\(f(w_{ML}) = f(w) + c\\) where \\(c&gt;0\\). This set is given by, \\[\nS_c = \\left \\{w: f(w_{ML}) = f(w) + c \\right \\}\n\\] i.e. every \\(w \\in S_c\\) satisfies, \\[\n||X^Tw-y||^2=||X^Tw_{ML}-y||^2 + c\n\\] \\[\n\\text{On Simplification, we get}\n\\] \\[\n(w-w_{ML})^T(XX^T)(w-w_{ML}) = c'\n\\] where \\(c'\\) depends on \\(c,XX^T,\\) and \\(w_{ML}\\), but not on \\(w\\).\n\n\n\nPictoral Representation of what Ridge Regression does.\n\n\nConclusion: Ridge Regression pushes feature values to zero but not necessarily zero.\n\n\nLasso Regression\nLasso (Least Absolute Shrinkage and Selection Operator) regression is a type of linear regression that uses a regularization technique to shrink the coefficients of the less important features to zero, effectively performing feature selection and preventing overfitting.\nIts objective function is given by, \\[\n\\min_{w\\in \\mathbb{R}^d} \\sum^n_{i=1}(w^Tx_i-y_i)^2 + \\lambda||w||_1^2\n\\]\nAs you can see, it is almost the same as Ridge Regression. The only difference is that it uses \\(||w||_1^2\\), instead of \\(||w||_2^2\\), which is the squared L1 norm of \\(w\\).\n\n\n\nPictoral Representation of what Lasso Regression does.\n\n\nLasso Regression does not have a closed form solution and is often solved using Sub-gradients. For further info on sub-gradients, see here.\nConclusion: Lasso Regression pushes less important features to zero.\n\n\nCredits\nProfessor Arun Rajkumar: The content as well as the notations are from his slides and lecture."
  },
  {
    "objectID": "pages/Wk07.html",
    "href": "pages/Wk07.html",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk07.html#issues-with-k-nn",
    "href": "pages/Wk07.html#issues-with-k-nn",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nFollowing are the issues with the algorithm:\n\nThe choice of distance function itself can give different results. The Euclidean distance might not always be the best fit!\nIt can be computationally demanding. When making a prediction for a single test datapoint, the distances between that datapoint and all training points must be calculated and sorted. As a result, the algorithm has a complexity of \\(O(nlog(n))\\), where \\(n\\) represents the size of the dataset.\nNo model is learned by this algorithm. It always needs the training dataset to make proper predictions."
  },
  {
    "objectID": "pages/Wk07.html#goodness-of-a-question",
    "href": "pages/Wk07.html#goodness-of-a-question",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Goodness of a Question",
    "text": "Goodness of a Question\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset. We partition it using a question into \\(D_{yes}\\) and \\(D_{no}\\).\nWhat we need is a measure of “Impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). This measure can be given by various ways, but we will use the Entropy Function.\nThe Entropy function is given by, \\[\nEntropy(\\{y_1, \\ldots, y_n\\}) = Entropy(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\] where conventionally \\(\\log(0)\\) is treated as \\(0\\).\nPictorial Representation of the Entropy function:\n\n\n\nEntropy Function\n\n\nThen, we use Information Gain to measure the goodness of the split.\nInformation gain is a commonly used criterion in decision tree algorithms that measures the reduction in entropy or impurity of a dataset after splitting based on a given feature. By selecting features with high information gain, decision trees can effectively differentiate between the different classes of data and make accurate predictions.\nInformation gain is given by, \\[\n\\text{Information Gain}(feature,value)=Entropy(D) - \\left [ \\gamma Entropy(D_{yes})+(1-\\gamma)Entropy(D_{no}) \\right ]\n\\] where \\(\\gamma\\) is given by, \\[\n\\gamma=\\frac{|D_{yes}|}{|D|}\n\\]"
  },
  {
    "objectID": "pages/Wk07.html#decision-tree-algorithm",
    "href": "pages/Wk07.html#decision-tree-algorithm",
    "title": "Supervised Learning - Classification - K-NN, Decision tree",
    "section": "Decision Tree Algorithm",
    "text": "Decision Tree Algorithm\nThe algorithm is as follows:\n\nDiscretize each feature in [min,max] range.\nPick the question that has the largest information gain.\nRepeat the procedure for \\(D_{yes}\\) and \\(D_{no}\\).\nStop growing the tree if a node becomes sufficiently “pure”.\n\nThe goodness of a question can also be measured using different methods like the Gini Index, etc.\nPictorial Depiction of decision boundary and its decision tree:\n\n\n\nDecision Boundary"
  },
  {
    "objectID": "pages/Wk08.html",
    "href": "pages/Wk08.html",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk08.html#alternate-generative-model",
    "href": "pages/Wk08.html#alternate-generative-model",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Alternate Generative Model",
    "text": "Alternate Generative Model\nAn alternate model begins with the class conditional independence assumption. The class conditional independence assumption is a common assumption made in machine learning algorithms that assumes the features of an object are conditionally independent given its class label.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nGeneral Steps in the algorithm are:\n\nDecide the labels by tossing a coin \\(P(y_i=1)=p\\).\nDecide features for \\(x\\) given \\(y\\) using: \\[\nP(x = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^y_i)^{f_i}(1-p^y_i)^{1-f_i}\n\\]\n\nThe parameters in the model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(x|y=1)\\): \\(d\\)\nParameters for \\(P(x|y=0)\\): \\(d\\)\n\nHence, the total number of parameters \\[\\begin{align*}\n    &=1 + d + d\\\\\n    &=2d+1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-the-parameters",
    "href": "pages/Wk08.html#prediction-using-the-parameters",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(x^{test}\\in\\{0,1\\}^d\\), prediction for \\(\\hat{y}^{test}\\) is done using the following: \\[\nP(\\hat{y}^{test}=1|x^{test}) \\ge P(\\hat{y}^{test}=0|x^{test})\n\\] If the above is true, \\(\\hat{y}^{test}=1\\), otherwise \\(0\\).\nUsing Bayes rule, we can get the values for \\(P(\\hat{y}^{test}=1|x^{test})\\) and \\(P(\\hat{y}^{test}=0|x^{test})\\): \\[\\begin{align*}\nP(\\hat{y}^{test}=1|x^{test})&=\\frac{P(x^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(x^{test})}\\\\\nP(\\hat{y}^{test}=0|x^{test})&=\\frac{P(x^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(x^{test})}\n\\end{align*}\\] As we predict by comparing the two values, we can do this without actually solving for \\(P(x^{test})\\).\nSolving for \\(P(x^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we get, \\[\\begin{align*}\n&=P(x^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1)\\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\] Similarly we solve for \\(P(x^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, if \\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\] we predict \\(y^{test}=1\\), othewise \\(y^{test}=0\\).\nThe model implements two main things:\n\nClass Conditional Independence Assumption\nBayes Rule\n\nTherefore, we call this algorithm Naive Bayes.\nIn short, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It calculates the probability of a sample belonging to a class by estimating the conditional probability of each feature given the class and then multiplying them together using Bayes’ theorem. Despite its simple assumption, Naive Bayes is known to perform well in various applications, particularly when there are many features but relatively few training examples."
  },
  {
    "objectID": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "href": "pages/Wk08.html#pitfalls-of-naive-bayes",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nThe most prominent issue with Naive Bayes is that if a feature is not seen in the training set but seen in the testing set, the prediction probability for both the classes would be zero. \\[\\begin{align*}\nP(\\hat{y}^{test}=1|x^{test} = [f_1, f_2, \\ldots, f_d])&\\propto\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\\\\\nP(\\hat{y}^{test}=0|x^{test} = [f_1, f_2, \\ldots, f_d])&\\propto\\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\] Even if one feature \\(f_i\\) was zero in training set, we get \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), which ultimately results in \\(P(\\hat{y}^{test}=0|x^{test})=P(\\hat{y}^{test}=1|x^{test})=0\\).\nA popular fix for this is to introduce two “pseudo” datapoints with labels \\(1\\) and \\(0\\) each into the dataset whose features are all ones. This technique is also known as Laplace smoothing.\nBriefly speaking, Laplace smoothing is a technique used to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By adding this smoothing term, the model becomes more robust and can handle unseen data more effectively."
  },
  {
    "objectID": "pages/Wk08.html#prediction-using-bayes-rule",
    "href": "pages/Wk08.html#prediction-using-bayes-rule",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Prediction using Bayes Rule",
    "text": "Prediction using Bayes Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|x_{test})\\propto P(x_{test}|y_{test})*P(y_{test})\n\\] where \\(P(x_{test}|y_{test})\\equiv f(x_{test};\\hat{\\mu}_{y_{test}}, \\hat{\\Sigma})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nPredict \\(y_{test}=1\\) if: \\[\\begin{align*}\nf(x_{test};\\hat{\\mu}_1, \\hat{\\Sigma})*\\hat{p}&\\ge f(x_{test};\\hat{\\mu}_0, \\hat{\\Sigma})*(1-\\hat{p}) \\\\\ne^{-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_1)}*\\hat{p}&\\ge e^{-(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_0)}*(1-\\hat{p}) \\\\\n-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_1)+\\log(\\hat{p})&\\ge -(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma}(x_{test}-\\hat{\\mu}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] \\[\n\\left( (\\hat{\\mu}_1-\\hat{\\mu}_0)^T\\hat{\\Sigma}^{-1} \\right)x_{test} + \\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0 - \\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, we can say that the decision function is of the form \\(w^Tx+b\\ge0\\) where \\(w\\in\\mathbb{R}^d\\), \\(w_i= (\\hat{\\mu}_1-\\hat{\\mu}_0)^T\\hat{\\Sigma}^{-1}\\) and \\(b=\\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0 - \\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + log(\\frac{1-\\hat{p}}{\\hat{p}})\\).\nTherefore, the decision function of Gaussian Naive Bayes when the covariance matrix is equal for both classes is Linear."
  },
  {
    "objectID": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "href": "pages/Wk08.html#decision-boundaries-for-different-covariances",
    "title": "Supervised Learning - Classification - Generative Models - Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As seen previously, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is linear as well as the perpendicular bisector of the line drawn from \\(\\hat{\\mu}_1\\) to \\(\\hat{\\mu}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\Sigma}_1\\) and \\(\\hat{\\Sigma}_0\\) be the covariance matrices for classes \\(1\\) and \\(0\\) respectively. They are given by, \\[\\begin{align*}\n\\hat{\\Sigma}_1 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)(\\mathbb{1}(y_i=1)*x_i-\\hat{\\mu}_1)^T \\\\\n\\hat{\\Sigma}_0 &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)(\\mathbb{1}(y_i=0)*x_i-\\hat{\\mu}_0)^T\n\\end{align*}\\] Predict \\(y_{test}=1\\) if: \\[\\begin{align*}\nf(x_{test};\\hat{\\mu}_1, \\hat{\\Sigma_1})*\\hat{p}&\\ge f(x_{test};\\hat{\\mu}_0, \\hat{\\Sigma_0})*(1-\\hat{p}) \\\\\ne^{-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_1)}*\\hat{p}&\\ge e^{-(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_0)}*(1-\\hat{p}) \\\\\n-(x_{test}-\\hat{\\mu}_1)^T\\hat{\\Sigma_1}(x_{test}-\\hat{\\mu}_1)+\\log(\\hat{p})&\\ge -(x_{test}-\\hat{\\mu}_0)^T\\hat{\\Sigma_0}(x_{test}-\\hat{\\mu}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] \\[\nx_{test}^T(\\hat{\\Sigma}_1^{-1}-\\hat{\\Sigma}_0^{-1})x_{test}-2(\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}-\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1})x_{test}+(\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\hat{\\mu}_0-\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}\\hat{\\mu}_1) + log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, we can say that the decision function is of the form \\(x^TQx-2b^Tx+c\\ge0\\) where \\(Q=\\hat{\\Sigma}_1^{-1}-\\hat{\\Sigma}_0^{-1}\\), \\(b=\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}-\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\), and \\(c=(\\hat{\\mu}_0^T\\hat{\\Sigma}_0^{-1}\\hat{\\mu}_0-\\hat{\\mu}_1^T\\hat{\\Sigma}_1^{-1}\\hat{\\mu}_1) + log(\\frac{1-\\hat{p}}{\\hat{p}})\\). Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes"
  },
  {
    "objectID": "pages/Wk09.html",
    "href": "pages/Wk09.html",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk09.html#analysis-of-the-update-rule",
    "href": "pages/Wk09.html#analysis-of-the-update-rule",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Analysis of the Update Rule",
    "text": "Analysis of the Update Rule\nFor a training example \\((x, y)\\), where \\(x\\) is the input and \\(y\\) is the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates its weight vector \\(w\\) as follows:\n\nIf the prediction of the perceptron on \\(x\\) is correct (i.e., \\(\\text{sign}(w^Tx_i)==y_i\\)), then no update is performed.\nIf the prediction of the perceptron on \\(x\\) is incorrect (i.e., \\(\\text{sign}(w^Tx_i)\\ne y_i\\)), then the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(w^{(t+1)} = w^t + x_iy_i\\).\n\nThis update rule effectively moves the decision boundary in the direction of the correct classification for the misclassified example. It is guaranteed to converge to a linearly separable solution if the data is linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution."
  },
  {
    "objectID": "pages/Wk09.html#further-assumptions",
    "href": "pages/Wk09.html#further-assumptions",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Further Assumptions",
    "text": "Further Assumptions\nWe make three further assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) is linearly separable with \\(\\gamma\\)-margin if \\(\\exists w^* \\in \\mathbb{R}^d\\) s.t. \\((w^{*T}x_i)y_i\\ge\\gamma\\) \\(\\forall i\\) for some \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let some \\(R&gt;0 \\in \\mathbb{R}\\), \\(\\forall i \\in D\\) \\(||x_i||\\le R\\). In short, let \\(R\\) be the length of the datapoint furthest from the center.\nNormal Length for \\(w^*\\): Let \\(w^*\\) be of unit length."
  },
  {
    "objectID": "pages/Wk09.html#sigmoid-function",
    "href": "pages/Wk09.html#sigmoid-function",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Sigmoid Function",
    "text": "Sigmoid Function\nUntil now, we used the \\(\\text{sign}\\) function to get the class for the output. But can we also provide the probabilities for these outputs?\nLet \\(z=w^Tx\\) and \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? For this, we use the Sigmoid Function. It is given by, \\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is often used in machine learning as an activation function for neural networks. It has a characteristic S-shaped curve, which makes it useful for modeling processes that have a threshold or saturation point, such as logistic growth or binary classification problems.\nWhen the input value is large and positive, the sigmoid function output approaches 1, and when the input value is large and negative, the sigmoid function output approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” comes from the Greek word “sigmoides,” which means “shaped like the letter sigma” (\\(\\Sigma\\)). The letter sigma has a similar shape to the sigmoid function’s characteristic S-shaped curve, which is likely the reason for the function’s name."
  },
  {
    "objectID": "pages/Wk09.html#logistic-regression-1",
    "href": "pages/Wk09.html#logistic-regression-1",
    "title": "Discriminative Models - Perceptron; Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables, which can be either continuous or categorical. The goal of logistic regression is to estimate the probability that the dependent variable is one of the two possible values, given the values of the independent variables.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic(sigmoid) function, which produces an S-shaped curve that ranges between 0 and 1. The logistic function transforms the output of a linear combination of the independent variables into a probability estimate, which can then be used to classify new observations.\nLet \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be the dataset, where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know, \\[\nP(y=1|x) = g(w^Tx_i) = \\frac{1}{1+e^{-w^Tx}}\n\\] Using Maximum Likelihood, we get \\[\\begin{align*}\n\\mathcal{L}(w;\\text{Data}) &= \\prod _{i=1} ^{n} (g(w^Tx_i))^{y_i}(1- g(w^Tx_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(w^Tx_i))+(1-y_i)\\log(1- g(w^Tx_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-w^Tx_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-w^Tx_i) - \\log(1+e^{-w^Tx_i}) \\right ]\n\\end{align*}\\] Therefore, our objective, which is maximizing the log-likelihood function, is given by, \\[\n\\max _{w}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-w^Tx_i) - \\log(1+e^{-w^Tx_i}) \\right ]\n\\] But, there is no closed form solution for this. And hence, we use gradient descent for convergence.\nThe gradient is given by, \\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-x_i) - \\left( \\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \\right ) (-x_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -x_i + x_iy_i + x_i \\left( \\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ x_iy_i - x_i \\left( \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(w;\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ x_i \\left(y_i - \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right ]\n\\end{align*}\\] Using the Gradient Descent update rule, we get, \\[\\begin{align*}\nw_{t+1} &= w_t + \\eta_t\\nabla \\log(\\mathcal{L}(w;\\text{Data})) \\\\\n&= w_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} x_i \\left(y_i - \\frac{1}{1+e^{-w^Tx_i}} \\right ) \\right )\n\\end{align*}\\]\n\nKernel and Regularized Versions\nWe can argue that \\(w^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_ix_i\\), and therefore, can be Kernelized. For further details, refer to this link.\nThe regularized version is given by, \\[\n\\min _{w}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-w^Tx_i}) + w^Tx_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||w||^2\n\\] where \\(\\frac{\\lambda}{2}||w||^2\\) is the regualizer and \\(\\lambda\\) is found using cross-validation."
  },
  {
    "objectID": "pages/Wk10.html",
    "href": "pages/Wk10.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "PDF Link: notes"
  },
  {
    "objectID": "pages/Wk10.html#margin-maximization",
    "href": "pages/Wk10.html#margin-maximization",
    "title": "Support Vector Machines",
    "section": "Margin Maximization",
    "text": "Margin Maximization\nFrom the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\)."
  },
  {
    "objectID": "pages/Wk10.html#hard-margin-svm-algorithm",
    "href": "pages/Wk10.html#hard-margin-svm-algorithm",
    "title": "Support Vector Machines",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThis algorithm only works if the dataset is linearly separable with a \\(\\gamma &gt; 0\\).\n\nCalculate \\(Q=X^TX\\) directly or using a kernel as per the dataset.\nUse the gradient of the dual formula (\\(\\alpha^T1 - \\frac{1}{2}\\alpha^TY^TQY\\alpha\\)), in the gradient descent algorithm to find a satisfactory \\(\\alpha\\). Let the intial \\(\\alpha\\) be a zero vector \\(\\in \\mathbb{R}^n_+\\).\nTo predict:\n\nFor non-kernelized SVM: \\(\\text{label}(x_{test}) = w^Tx_{test} = \\sum _{i=1} ^n \\alpha _i y_i(x_i^Tx_{test})\\)\nFor kernelized SVM: \\(\\text{label}(x_{test}) = w^T\\phi(x_{test}) = \\sum _{i=1} ^n \\alpha _i y_ik(x_i^Tx_{test})\\)"
  },
  {
    "objectID": "pages/lin_reg.html",
    "href": "pages/lin_reg.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a supervised learning algorithm used to predict a continuous output variable based on one or more input features, assuming a linear relationship between the input and output variables. The goal of linear regression is to find the line of best fit that minimizes the sum of squared errors between the predicted and actual output values.\nGiven a dataset \\(\\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^d\\), let \\(\\{y_1, \\ldots, y_n\\}\\) be the labels, where \\(y_i \\in \\mathbb{R}\\).\nThe goal of linear regression is to find the mapping between input and output variables. i.e. \\[\nh: \\mathbb{R}^d \\rightarrow \\mathbb{R}\n\\] The error for the above mapping function is given by, \\[\n\\text{error}(h) = \\sum _{i=1} ^n (h(x_i) - y_i) ^2)\n\\] This error can be as small as zero, and this is achieved when \\(h(x_i)=y_i \\hspace{0.5em} \\forall i\\). But this may not be the desired output as it may represent nothing more than memorizing the data and its outputs.\nThe memorization problem can be mitigated by introducing a structure to the mapping. The simplest structure being of the linear kind, we shall use it as the underlying structure for our data.\nLet \\(\\mathcal{H}_{\\text{linear}}\\) represent the solution space for the mapping in the linear space. \\[\n\\mathcal{H}_{\\text{linear}}=\\Biggr \\lbrace{h_w: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\hspace{0.5em} s.t. \\hspace{0.5em} h_w(x)=w^Tx \\hspace{0.5em} \\forall w \\in \\mathbb{R}^d } \\Biggr \\rbrace\n\\] Therefore, our goal is, \\[\\begin{align*}\n\\min _{h \\in \\mathcal{H}_{\\text{linear}}} \\sum _{i=1} ^n (h(x_i) - y_i) ^2 \\\\\n\\text{Equivalently} \\\\\n\\min _{w \\in \\mathbb{R}^d} \\sum _{i=1} ^n (w^Tx_i - y_i) ^2\n\\end{align*}\\] Optimizing the above is the entire point of the linear regression algorithm."
  },
  {
    "objectID": "pages/lin_reg.html#stochastic-gradient-descent",
    "href": "pages/lin_reg.html#stochastic-gradient-descent",
    "title": "Linear Regression",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm used in machine learning for finding the parameters that minimize the loss function of a model. In contrast to traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters based on a randomly selected subset of the data, known as a batch. This results in faster training times and makes SGD particularly useful for large datasets.\nFor every step \\(t\\), rather than updating \\(w\\) using the entire dataset, we use a small randomly selected (\\(k\\)) data points to update \\(w\\). Therefore, the new gradient is \\(2(\\tilde{X}\\tilde{X}^Tw^t - \\tilde{X}\\tilde{y})\\) where \\(\\tilde{X}\\) and \\(\\tilde{y}\\) is the small sample randomly selected from the dataset. This is manageable because \\(\\tilde{X} \\in \\mathbb{R}^{d \\times k}\\) which is considerably smaller than \\(X\\).\nAfter T rounds, we use, \\[\nw ^T _{SGD} = \\frac{1}{T}  \\sum _{i=1} ^T w^i\n\\] This has a certain guarantee to have optimal convergence partly because of the randomness involved in it.\n\nImplementing Stochastic Gradient Descent in Python\nLet compute the \\(w\\) vector using Stochastic Gradient Descent and print the coefficients and intercept.\nw = np.ones((X.shape[0], 1))\neta = 1e-4\nt = 100000\nn = X.shape[1]\nb = 10\n\nfor i in range(t):\n    # randomly select a batch of samples\n    idx = np.random.choice(n, b, replace=False)\n    X_b = X[:, idx]\n    y_b = y[idx]\n    # compute the gradient for the batch\n    grad = 2*(X_b @ X_b.T @ w) - 2*(X_b @ y_b)\n    # update the weights\n    w = w - eta*grad\n    \nprint ('Coefficients: ', w.reshape((-1,))[:3])\nprint ('Intercept: ', w.reshape((-1,))[-1])\n\n\n\n\n\n\nCoefficients: [10.72000912, 8.3366805, 9.13970723]\nIntercept: 65.2366350549217"
  }
]