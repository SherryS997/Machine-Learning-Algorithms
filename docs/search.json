[
  {
    "objectID": "pages/fp-hfg.html",
    "href": "pages/fp-hfg.html",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "",
    "text": "The realm of speech synthesis has witnessed remarkable advancements in recent years, with the emergence of sophisticated deep learning models capable of generating human-quality speech from text. Among these, the FastPitch HiFi-GAN pipeline has gained considerable traction due to its exceptional efficiency and fidelity. This comprehensive guide will delve into the intricacies of this pipeline, unraveling its architecture, parameters, hyperparameters, and underlying principles, ultimately providing a thorough understanding of its workings.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/fp-hfg.html#architecture",
    "href": "pages/fp-hfg.html#architecture",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "Architecture",
    "text": "Architecture\nThe FastPitch architecture, the cornerstone of our text-to-speech pipeline, is a masterful composition of feed-forward Transformer (FFTr) stacks, meticulously designed to capture the intricate relationships between textual input and the corresponding acoustic features of speech. This section delves into the inner workings of this architecture, dissecting its components and their contributions to the generation of high-quality speech.\n\nInput Representation: A Foundation of Lexical Units\nThe journey of speech synthesis within FastPitch commences with the input text, which undergoes a meticulous transformation into a sequence of discrete lexical units. These units can assume the form of graphemes, representing individual characters; phonemes, embodying distinct units of sound; or even words, depending on the desired level of granularity. This sequence, denoted as \\(x = (x_1, ..., x_n)\\), serves as the foundation upon which the subsequent layers of the architecture will build their understanding of the linguistic content.\n\n\nEmbedding Layer: Mapping Symbols to Meaning\nEach input symbol \\(x_i\\) is then mapped to a high-dimensional vector representation through an embedding layer. This embedding process encodes the semantic and phonetic information associated with each symbol, allowing the model to capture the nuances of language and their implications for speech production. The embedding layer acts as a bridge between the discrete world of text and the continuous domain of acoustic features.\n\n\nFirst FFTr Stack: Extracting Linguistic Context\nThe embedded sequence of symbols is subsequently passed through the first FFTr stack. This stack consists of a series of Transformer encoder layers, each meticulously crafted to extract contextual relationships between the input symbols. Each encoder layer comprises:\n\nSelf-Attention Mechanism: This mechanism allows the model to attend to different parts of the input sequence and learn long-range dependencies between symbols. By considering the interactions between symbols, the self-attention mechanism captures the syntactic and semantic structure of the input text.\nPositional Encoding: As Transformers lack an inherent understanding of sequence order, positional encodings are injected to provide information about the relative positions of symbols within the sequence. This enables the model to distinguish between different word orders and their impact on meaning and pronunciation.\nFeed-Forward Network: This network further refines the representations learned by the self-attention mechanism, extracting additional features relevant for speech generation.\n\nThe output of the first FFTr stack is a contextualized representation of the input text, denoted as \\(h = \\text{FFTr}(x)\\). This representation encapsulates the linguistic information necessary for generating speech that accurately reflects the meaning and structure of the input text.\n\n\nDuration and Pitch Prediction: Modeling Prosodic Features\nThe contextualized representation, \\(h\\), is then utilized to predict two crucial prosodic features of speech: duration and pitch. These features play a pivotal role in conveying the rhythm, intonation, and expressiveness of speech.\n\nDuration Predictor: A dedicated 1-D convolutional neural network (CNN) is employed to predict the duration of each input symbol. This network analyzes the contextualized representation and estimates the length of time each symbol should be pronounced during speech synthesis. The predicted durations, denoted as \\(\\hat{d} = \\text{DurationPredictor}(h)\\), where \\(\\hat{d} ∈ N^n\\), ensure that the generated speech has a natural rhythm and flow.\nPitch Predictor: Another 1-D CNN is employed to predict the average pitch for each input symbol. This network examines the contextualized representation and estimates the fundamental frequency of the voice for each symbol. The predicted pitch values, denoted as \\(\\hat{p} = \\text{PitchPredictor}(h)\\), where \\(\\hat{p} ∈ R^n\\), contribute to the intonation and expressiveness of the synthesized speech.\n\n\n\nPitch Conditioning: Injecting Prosodic Information\nThe predicted pitch values are not merely an output of the FastPitch architecture; they are also used to condition the model, influencing the generated speech’s prosody. This conditioning process involves:\n\nPitch Embedding: The predicted pitch values are projected into a high-dimensional embedding space, allowing the model to capture the subtle nuances of pitch variations.\nAddition: The pitch embedding is then added to the contextualized representation \\(h\\). This step injects pitch information directly into the model, ensuring that the generated speech reflects the desired intonation and expressiveness.\n\n\\[g = h + \\text{PitchEmbedding}(p)\\]\n\n\nUpsampling: Aligning Temporal Resolutions\nThe resulting sum, \\(g\\), containing both linguistic and prosodic information, is upsampled to match the temporal resolution of the output mel-spectrogram frames. This upsampling process ensures that the model has sufficient temporal information to generate speech with the appropriate rhythm and intonation.\n\n\nSecond FFTr Stack: Generating Mel-Spectrograms\nThe upsampled representation, \\(g\\), is then fed into the second FFTr stack. This stack, similar in structure to the first FFTr stack, consists of a series of Transformer decoder layers. Each decoder layer incorporates the self-attention mechanism, positional encoding, and feed-forward network, but with an additional attention mechanism:\n\nEncoder-Decoder Attention: This mechanism allows the decoder to attend to relevant parts of the encoder’s output, enabling the model to align the linguistic and prosodic information with the corresponding acoustic features during mel-spectrogram generation.\n\nThe output of the second FFTr stack is the predicted mel-spectrogram sequence, denoted as \\[\\hat{y} = \\text{FFTr}([g_1, ..., g_1, ..., g_n, ..., g_n])\\] This sequence represents the spectral characteristics of the synthesized speech, capturing the frequencies and their intensities over time.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/fp-hfg.html#training",
    "href": "pages/fp-hfg.html#training",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "Training",
    "text": "Training\nThe training of FastPitch, the initial stage within our text-to-speech (TTS) pipeline, constitutes a meticulously orchestrated optimization process designed to instill within the model the ability to generate mel-spectrograms imbued with both accurate spectral content and expressive prosody. This intricate learning journey leverages a synergy of loss functions, each targeting specific aspects of the model’s performance, ultimately guiding it towards the generation of high-quality speech representations.\n\nLoss Functions: Guiding Lights in the Optimization Landscape\nFastPitch’s training regimen revolves around the minimization of a composite loss function, which encapsulates the model’s performance across multiple dimensions. Let’s dissect the individual loss components and their contributions to the overall training objective:\n\n1. Mean Squared Error (MSE) Loss: Ensuring Spectral Fidelity\nThe MSE loss serves as the cornerstone of FastPitch’s training, ensuring that the generated mel-spectrograms faithfully represent the spectral characteristics of the target speech. It quantifies the discrepancy between the predicted mel-spectrogram, denoted as \\(ŷ\\), and the ground-truth mel-spectrogram, \\(y\\), as follows:\n\\[L_{\\text{MSE}} = ||ŷ - y||^2_2\\]\nBy minimizing this loss, the model learns to capture the intricate spectral nuances of speech, encompassing the distribution of energy across different frequency bands, ultimately contributing to the naturalness and intelligibility of the synthesized speech.\n\n\n2. Pitch Prediction Loss: Sculpting Prosodic Contours\nThe pitch prediction loss plays a pivotal role in shaping the prosodic contours of the generated speech, ensuring that the model accurately predicts the fundamental frequency (\\(F_0\\)) contour for each input symbol. This loss is calculated as the MSE between the predicted pitch contour, \\(p̂\\), and the ground-truth pitch contour, \\(p\\):\n\\[L_{\\text{pitch}} = ||p̂ - p||^2_2\\]\nMinimizing this loss enables the model to learn the intricate relationship between textual input and pitch variations, allowing it to generate speech with appropriate intonation and expressiveness.\n\n\n3. Duration Prediction Loss: Orchestrating Temporal Dynamics\nThe duration prediction loss governs the temporal dynamics of the generated speech, ensuring that the model accurately predicts the duration of each input symbol. This loss is computed as the MSE between the predicted duration sequence, \\(d̂\\), and the ground-truth duration sequence, \\(d\\):\n\\[L_{\\text{duration}} = ||d̂ - d||^2_2\\]\nBy minimizing this loss, the model learns to control the timing of speech events, preventing unnatural pauses or rushed pronunciations, and contributing to the overall fluency and rhythm of the synthesized speech.\n\n\n\nComposite Loss: A Harmonious Convergence\nThe individual loss components discussed above are meticulously combined into a composite loss function that guides the model’s training. This composite loss, denoted as \\(L_{\\text{total}}\\), is typically a weighted sum of the individual losses:\n\\[L_{\\text{total}} = \\lambda_{\\text{MSE}}L_{\\text{MSE}} + \\lambda_{\\text{pitch}}L_{\\text{pitch}} + \\lambda_{\\text{duration}}L_{\\text{duration}}\\]\nwhere \\(\\lambda_{\\text{MSE}}\\), \\(\\lambda_{\\text{pitch}}\\), and \\(\\lambda_{\\text{duration}}\\) represent weighting factors that determine the relative importance of each loss component. These weights are carefully chosen to balance the model’s focus on spectral accuracy, prosodic expressiveness, and temporal dynamics, ultimately leading to the generation of high-quality speech.\n\n\nOptimization Algorithms: Navigating the Loss Landscape\nThe minimization of the composite loss function is typically achieved through the application of gradient-based optimization algorithms, such as Adam or AdamW. These algorithms iteratively update the model’s parameters based on the gradients of the loss function, gradually steering the model towards a configuration that minimizes the overall loss. The choice of optimizer and its associated hyperparameters, such as learning rate and momentum, significantly influence the efficiency and effectiveness of the training process.\n\n\nTraining Data: The Foundation of Knowledge\nThe quality and diversity of the training data are paramount to the success of FastPitch’s training. Ideally, the training corpus should encompass a wide range of speakers, speaking styles, and acoustic conditions to equip the model with the ability to generalize to unseen scenarios and produce versatile speech outputs. Additionally, the training data should be meticulously preprocessed to ensure consistency and accuracy, which may involve steps such as text normalization, alignment of text with speech signals, and extraction of pitch contours.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/fp-hfg.html#hyperparameter-optimization",
    "href": "pages/fp-hfg.html#hyperparameter-optimization",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "Hyperparameter Optimization",
    "text": "Hyperparameter Optimization\nThe performance of FastPitch, as with any deep learning model, is intricately intertwined with the selection of hyperparameters. These hyperparameters act as the sculptor’s tools, shaping the learning process and ultimately influencing the quality of the generated speech. This section delves into the key hyperparameters of FastPitch, exploring their impact on model performance and providing insights into their optimal configuration.\n\nNavigating the Hyperparameter Space\n\nNumber of FFTr Layers: The depth of the Transformer encoder and decoder stacks plays a crucial role in the model’s ability to learn complex representations and long-range dependencies within the input sequence. Increasing the number of layers can enhance the model’s capacity to capture intricate linguistic nuances and phonetic patterns, potentially leading to improved prosody and naturalness in the synthesized speech. However, this depth comes at the cost of increased computational complexity and training time, necessitating a careful balance between model expressiveness and efficiency.\nNumber of Attention Heads: Attention mechanisms lie at the core of Transformer models, enabling them to selectively focus on relevant parts of the input sequence during processing. Each attention head within a Transformer layer learns to attend to different aspects of the input, contributing to a multifaceted understanding of the context. Increasing the number of attention heads can improve the model’s ability to capture diverse linguistic features and relationships, potentially leading to more nuanced and expressive speech synthesis. However, similar to the number of layers, increasing attention heads can impact computational efficiency, requiring careful consideration of the trade-offs.\nDropout Rate: Overfitting is a persistent challenge in deep learning, where the model becomes excessively specialized to the training data and fails to generalize well to unseen examples. Dropout acts as a regularization technique, mitigating overfitting by randomly dropping out a portion of neurons during training. This process prevents the model from relying too heavily on specific neurons or features, encouraging it to learn more robust and generalizable representations. The dropout rate controls the probability of a neuron being dropped out, with higher rates leading to stronger regularization. Selecting an appropriate dropout rate is crucial for achieving a balance between model complexity and generalizability.\nLearning Rate: The learning rate governs the magnitude of updates made to the model’s parameters during optimization. An excessively high learning rate can lead to instability and divergence, while an excessively low learning rate can result in slow convergence and suboptimal performance. Adaptive optimization algorithms, such as Adam, can dynamically adjust the learning rate during training, alleviating the need for manual tuning. However, it is still essential to select an appropriate initial learning rate and potentially employ learning rate scheduling techniques to ensure efficient and stable convergence.\nBatch Size: The batch size determines the number of training examples processed before updating the model’s parameters. Larger batch sizes can improve computational efficiency and accelerate training, particularly when utilizing parallel computing resources. However, excessively large batch sizes can lead to reduced generalization performance and increased memory consumption. Finding an optimal batch size often involves a trade-off between training speed and model generalizability.\nWarmup Steps: During the initial stages of training, it can be beneficial to gradually increase the learning rate from a small value to the target learning rate. This “warmup” period allows the model to stabilize before undergoing larger parameter updates, preventing potential instability and divergence. The number of warmup steps should be carefully chosen to ensure a smooth transition to the target learning rate.\n\n\n\nEmpirical Exploration and Fine-tuning\nDetermining the optimal configuration of hyperparameters often involves an empirical exploration of the hyperparameter space. Techniques such as grid search, random search, and Bayesian optimization can be employed to efficiently search for hyperparameter combinations that yield the best performance. Additionally, monitoring training progress and evaluating the model on a held-out validation set can provide valuable insights into the effectiveness of different hyperparameter choices.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/fp-hfg.html#architecture-1",
    "href": "pages/fp-hfg.html#architecture-1",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "Architecture",
    "text": "Architecture\nHiFi-GAN’s architecture is meticulously designed to capture the intricate details of audio signals and generate high-fidelity waveforms. It achieves this through a synergistic interplay between a generator network and a set of discriminators, each specializing in extracting specific features from the audio data.\n\nGenerator Network: Crafting Audio from Mel-Spectrograms\nThe generator network is a fully convolutional neural network tasked with the intricate process of transforming an input mel-spectrogram into a raw audio waveform. Its architecture is thoughtfully constructed to achieve this upsampling task while preserving the spectral and temporal characteristics of the original audio.\n\nInput Layer: The generator receives as input a mel-spectrogram, a time-frequency representation of the audio signal that captures its spectral envelope.\nUpsampling Layers: A series of transposed convolutional layers progressively increase the temporal resolution of the mel-spectrogram, gradually approaching the target audio sampling rate. These layers effectively learn to reconstruct the fine-grained temporal details from the coarser mel-spectrogram representation.\nMulti-Receptive Field Fusion (MRF) Modules: Following each upsampling layer, the generator incorporates MRF modules, which play a crucial role in capturing patterns of varying lengths within the audio signal. Each MRF module consists of multiple residual blocks with diverse kernel sizes and dilation rates. This allows the network to learn both local and global features, contributing to the generation of natural and realistic audio waveforms.\n\nThe generator’s output is a raw audio waveform that closely resembles the original audio signal in terms of its spectral content and temporal structure.\n\n\nDiscriminator Networks: Guardians of Audio Authenticity\nHiFi-GAN employs a combination of discriminator networks, each designed to scrutinize the generated audio from different perspectives, ensuring its authenticity and fidelity.\n\nMulti-Scale Discriminator (MSD): Capturing Temporal Dependencies\nThe MSD focuses on capturing temporal dependencies within the audio signal across multiple scales. This is achieved by analyzing the audio at different levels of temporal resolution.\n\nSub-discriminators: The MSD consists of multiple sub-discriminators, each operating on a different scale of the input audio. Typically, three scales are employed: the raw audio, a 2x downsampled version, and a 4x downsampled version.\nConvolutional Layers: Each sub-discriminator comprises a stack of strided and grouped convolutional layers, allowing it to extract features at different temporal resolutions while maintaining computational efficiency. Leaky ReLU activation functions introduce non-linearity and enhance the network’s expressiveness.\nNormalization: Spectral normalization is applied to the first sub-discriminator, which operates on the raw audio, to stabilize training and prevent unwanted artifacts. Weight normalization is used for the remaining sub-discriminators to accelerate convergence.\n\nBy analyzing the audio at multiple scales, the MSD can effectively identify inconsistencies and artifacts that might be present in the generated samples, ensuring that the generator learns to produce audio with realistic temporal dynamics.\n\n\nMulti-Period Discriminator (MPD): Unraveling Periodic Structures\nThe MPD focuses on capturing the periodic structures inherent in audio signals, which are essential for generating natural-sounding speech and music.\n\nSub-discriminators: The MPD consists of multiple sub-discriminators, each analyzing a specific periodic component of the input audio. This is achieved by dividing the audio signal into segments of equal length, where the length corresponds to the desired period.\n2D Convolutional Layers: Each sub-discriminator utilizes 2D convolutions with a kernel size of 1 in the width axis to process the periodic samples independently. This design allows the network to extract features specific to each period, enhancing its ability to discern subtle periodic patterns.\nNormalization: Weight normalization is applied to all layers within the MPD to stabilize training and improve convergence speed.\n\nBy analyzing the periodic components of the audio signal, the MPD ensures that the generator learns to produce audio with realistic and natural-sounding periodicity, crucial for replicating the characteristics of human speech and musical instruments.\n\n\n\nSynergistic Collaboration: Adversarial Training\nThe generator and discriminators engage in an adversarial training process, where the generator strives to produce increasingly realistic audio samples, while the discriminators continuously improve their ability to distinguish between real and generated audio. This iterative process ultimately leads to a generator capable of synthesizing high-fidelity audio waveforms that are indistinguishable from real recordings.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/fp-hfg.html#training-1",
    "href": "pages/fp-hfg.html#training-1",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "Training",
    "text": "Training\nThe training of HiFi-GAN embodies an adversarial optimization process, where the generator and discriminator networks engage in a continuous refinement loop, driving each other towards improved performance. This intricate interplay between the two networks lies at the core of HiFi-GAN’s ability to generate high-fidelity audio waveforms. Let’s delve deeper into the specific loss functions and optimization strategies employed during this process.\n\nLoss Functions: Guiding the Adversarial Dance\nHiFi-GAN’s training hinges on the optimization of a combination of loss functions, each playing a distinct role in shaping the model’s capabilities:\n\nAdversarial Loss (L_Adv): This loss function forms the cornerstone of the GAN framework, pitting the generator against the discriminators. The generator aims to minimize this loss by producing samples that are indistinguishable from real audio, effectively “fooling” the discriminators. Conversely, the discriminators strive to maximize this loss by correctly classifying real and generated samples, thereby enhancing their ability to detect the generator’s outputs. Mathematically, the adversarial loss can be expressed as: \\[L_{Adv}(D; G) = E_{x,s}[(D(x) - 1)^2 + (D(G(s)))^2]\\] \\[L_{Adv}(G; D) = E_s[(D(G(s)) - 1)^2]\\]\nwhere:\n\n\\(D\\) represents the discriminator(s).\n\\(G\\) represents the generator.\n\\(x\\) denotes a real audio sample.\n\\(s\\) denotes the input mel-spectrogram.\n\nMel-Spectrogram Loss (L_Mel): To ensure that the generated audio accurately reflects the desired spectral characteristics, HiFi-GAN incorporates a mel-spectrogram loss. This loss measures the L1 distance between the mel-spectrogram of the generated waveform and the target mel-spectrogram. By minimizing this loss, the generator learns to produce audio with the desired frequency content. \\[L_{Mel}(G) = E_{x,s}[||φ(x) - φ(G(s))||_1]\\]\nwhere:\n\n\\(φ\\) denotes the function that transforms a waveform into its corresponding mel-spectrogram.\n\nFeature Matching Loss (L_FM): As an additional measure to guide the generator towards producing realistic audio, HiFi-GAN employs a feature matching loss. This loss calculates the L1 distance between features extracted from real and generated samples by the discriminators. By minimizing this loss, the generator learns to produce samples that exhibit similar characteristics to real audio in the feature space. \\[L_{FM}(G; D) = E_{x,s}[\\sum_{i=1}^T \\frac{1}{N_i} ||D_i(x) - D_i(G(s))||_1]\\]\nwhere:\n\n\\(T\\) denotes the number of layers in the discriminator.\n\\(D_i\\) and \\(N_i\\) represent the features and the number of features in the \\(i\\)th layer of the discriminator, respectively.\n\n\n\n\nOptimization: A Balancing Act\nThe training process involves optimizing the generator and discriminator networks alternately. This iterative procedure can be summarized as follows:\n\nDiscriminator Update: The discriminator is presented with a batch of real audio samples and a batch of generated samples from the generator. The discriminator’s parameters are updated to minimize the adversarial loss and maximize its ability to differentiate between real and generated samples.\nGenerator Update: The generator is presented with a batch of mel-spectrograms and generates corresponding audio waveforms. The generator’s parameters are updated to minimize the combined loss function, which includes the adversarial loss, mel-spectrogram loss, and feature matching loss.\n\nThis back-and-forth optimization process continues for numerous iterations, gradually improving the performance of both the generator and discriminator networks. The generator progressively learns to produce more realistic audio, while the discriminator becomes more adept at detecting subtle discrepancies between real and generated samples.\n\n\nAdditional Considerations: Fine-tuning the Process\nSeveral additional considerations contribute to the effectiveness of HiFi-GAN’s training process:\n\nMulti-discriminator Architecture: The use of both MSD and MPD provides complementary perspectives on the audio data, enhancing the discriminators’ ability to guide the generator towards producing high-fidelity audio.\nWeight Normalization and Spectral Normalization: These techniques stabilize the training process by preventing excessive parameter updates and mitigating the risk of mode collapse.\nOptimizer and Learning Rate Schedule: The choice of optimizer and learning rate schedule can significantly impact the convergence and stability of the training process. HiFi-GAN commonly employs the AdamW optimizer with a carefully designed learning rate decay schedule.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/fp-hfg.html#hyperparameters",
    "href": "pages/fp-hfg.html#hyperparameters",
    "title": "Demystifying FastPitch HiFi-GAN: A Deep Dive into High-Fidelity Text-to-Speech Synthesis",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHiFi-GAN’s ability to generate high-fidelity audio waveforms is contingent upon the judicious selection and optimization of its hyperparameters. These parameters exert a profound influence on the model’s architecture, training dynamics, and ultimately, the quality of the synthesized audio. This section delves into the intricacies of HiFi-GAN hyperparameter optimization, elucidating the role and impact of each parameter.\n\nGenerator Hyperparameters: Shaping the Melodic Canvas\nThe generator, responsible for transforming mel-spectrograms into realistic audio waveforms, is governed by a set of hyperparameters that dictate its structure and functionality:\n\nHidden Dimension (hu): This hyperparameter determines the dimensionality of the generator’s hidden layers, effectively controlling the model’s capacity and expressive power. A higher hidden dimension allows the generator to capture more intricate relationships within the data but may also increase the risk of overfitting and computational complexity.\nUpsampling Factors (ku): These hyperparameters define the upsampling factors employed by the transposed convolutional layers within the generator. The upsampling factors govern the rate at which the temporal resolution of the mel-spectrogram is increased to match that of the raw audio waveform. Careful selection of these factors is crucial to ensure the preservation of spectral details while achieving the desired temporal resolution.\nMRF Kernel Sizes (kr): The Multi-Receptive Field Fusion (MRF) modules within the generator are equipped with multiple residual blocks, each operating with a distinct kernel size. These kernel sizes determine the extent of the local context considered by each residual block during feature extraction. By incorporating residual blocks with varying kernel sizes, the MRF module effectively captures both short-term and long-term dependencies within the data.\nMRF Dilation Rates (Dr): In addition to kernel sizes, the residual blocks within the MRF modules utilize dilation rates to further expand their receptive fields. Dilation involves inserting spaces between the elements of the convolution kernel, enabling the network to capture patterns at different scales without increasing the number of parameters. The selection of dilation rates plays a crucial role in capturing long-range dependencies and modeling the temporal dynamics of audio signals.\n\n\n\nDiscriminator Hyperparameters: The Guardians of Authenticity\nThe discriminators, acting as discerning judges of audio authenticity, also rely on hyperparameters to guide their operation:\n\nMulti-Scale Discriminator (MSD) Configuration: The MSD is composed of multiple sub-discriminators, each operating on a different scale of the input audio. Hyperparameters such as the number of sub-discriminators, their respective filter sizes, and the downsampling factors employed between scales influence the MSD’s ability to capture both local and global features of the audio signal.\nMulti-Period Discriminator (MPD) Periods (p): The MPD consists of multiple sub-discriminators, each analyzing equally spaced samples of the input audio. The spacing between these samples is determined by the periods, which are typically chosen to be prime numbers to minimize overlap and maximize the diversity of captured periodic patterns. The selection of periods plays a critical role in enabling the MPD to effectively identify and evaluate the periodic structures inherent in audio signals.\n\n\n\nTraining Hyperparameters: Orchestrating the Learning Process\nThe training of HiFi-GAN is orchestrated by a set of hyperparameters that govern the learning dynamics and convergence behavior:\n\nOptimizer: The choice of optimizer, such as Adam or AdamW, influences the model’s parameter updates during training. The optimizer’s hyperparameters, including learning rate, momentum, and weight decay, further fine-tune the learning process.\nLearning Rate Schedule: The learning rate schedule dictates how the learning rate evolves over the course of training. A well-designed learning rate schedule can accelerate convergence and prevent overfitting. Common schedules include step decay, exponential decay, and cyclical learning rates.\nBatch Size: The batch size determines the number of training examples processed simultaneously during each iteration. A larger batch size can improve training efficiency but may also require more memory and potentially lead to suboptimal convergence.\nLoss Weights: As HiFi-GAN employs multiple loss functions, such as adversarial loss, mel-spectrogram loss, and feature matching loss, the relative weighting of these losses can impact the model’s training trajectory and the emphasis placed on different aspects of the generated audio.\n\n\n\nHyperparameter Optimization Strategies: Navigating the Parameter Space\nOptimizing HiFi-GAN’s hyperparameters is a meticulous process that often involves a combination of empirical observations, domain knowledge, and automated search techniques. Some common strategies include:\n\nGrid Search: This method involves systematically exploring a predefined range of values for each hyperparameter, evaluating the model’s performance for each combination. While exhaustive, grid search can be computationally expensive and may not effectively explore the entire parameter space.\nRandom Search: This technique randomly samples hyperparameter combinations from a predefined distribution, offering a more efficient exploration of the parameter space compared to grid search.\nBayesian Optimization: This approach leverages a probabilistic model to guide the search for optimal hyperparameters, efficiently navigating the parameter space and converging towards promising regions.\nGradient-Based Optimization: Recent advancements in hyperparameter optimization have introduced gradient-based methods, allowing for the direct optimization of hyperparameters using gradient descent algorithms.\n\nThe optimal hyperparameter configuration for HiFi-GAN is highly dependent on the specific dataset, training setup, and desired audio characteristics. A meticulous exploration of the hyperparameter space is essential to achieve optimal performance and unlock the full potential of HiFi-GAN for high-fidelity audio synthesis.",
    "crumbs": [
      "Others",
      "FastPitch HiFi-GAN Pipeline"
    ]
  },
  {
    "objectID": "pages/weight_init.html",
    "href": "pages/weight_init.html",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "",
    "text": "Training your neural network hinges significantly on the initial values assigned to the weights. An adept choice in initialization methods plays a pivotal role in streamlining the learning process.\nIn the realm of neural networks, weight initialization stands as a critical precursor to successful training. It acts as the bedrock upon which the network learns intricate patterns and correlations within data. Choosing the optimal initialization method can profoundly influence the network’s ability to converge efficiently and effectively.\nThis exploration aims to delve into various initialization techniques, namely random, zeros, and He initialization. Each method introduces distinct characteristics to the neural network’s learning dynamics, impacting its convergence and eventual performance.\nAn adeptly chosen initialization method brings forth several advantages:\n\nAccelerated convergence of gradient descent\nEnhanced likelihood of gradient descent converging towards a lower training and generalization error\n\nLet’s delve into these initialization techniques and witness their unique effects on the learning process. The following is the scatter plot of a datset with two classes:\n\n\n\nThe scatter plot depicts a binary classification scenario featuring two concentric circles. The outer circle corresponds to one class labeled as ‘1,’ while the inner circle represents another class labeled as ‘0.’",
    "crumbs": [
      "Deep Learning",
      "Practical DL",
      "Weight Initializations"
    ]
  },
  {
    "objectID": "pages/weight_init.html#introduction",
    "href": "pages/weight_init.html#introduction",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "",
    "text": "Training your neural network hinges significantly on the initial values assigned to the weights. An adept choice in initialization methods plays a pivotal role in streamlining the learning process.\nIn the realm of neural networks, weight initialization stands as a critical precursor to successful training. It acts as the bedrock upon which the network learns intricate patterns and correlations within data. Choosing the optimal initialization method can profoundly influence the network’s ability to converge efficiently and effectively.\nThis exploration aims to delve into various initialization techniques, namely random, zeros, and He initialization. Each method introduces distinct characteristics to the neural network’s learning dynamics, impacting its convergence and eventual performance.\nAn adeptly chosen initialization method brings forth several advantages:\n\nAccelerated convergence of gradient descent\nEnhanced likelihood of gradient descent converging towards a lower training and generalization error\n\nLet’s delve into these initialization techniques and witness their unique effects on the learning process. The following is the scatter plot of a datset with two classes:\n\n\n\nThe scatter plot depicts a binary classification scenario featuring two concentric circles. The outer circle corresponds to one class labeled as ‘1,’ while the inner circle represents another class labeled as ‘0.’",
    "crumbs": [
      "Deep Learning",
      "Practical DL",
      "Weight Initializations"
    ]
  },
  {
    "objectID": "pages/weight_init.html#zero-initialization",
    "href": "pages/weight_init.html#zero-initialization",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "Zero Initialization",
    "text": "Zero Initialization\nOne fundamental approach in initializing neural network parameters involves setting all weights and biases to zeros. This method, exemplified in the code snippet above, initializes both weight matrices \\((W^{[1]}, W^{[2]}, ..., W^{[L]})\\) and bias vectors \\((b^{[1]}, b^{[2]}, ..., b^{[L]})\\) to zeros.\ndef initialize_parameters_zeros(layers_dims):\n    parameters = {}\n    L = len(layers_dims)\n    \n    for l in range(1, L):\n        parameters[f\"W{l}\"] = np.zeros((layers_dims[l], layers_dims[l-1]))\n        parameters[f\"b{l}\"] = np.zeros((layers_dims[l], 1))\n        \n    return parameters\nWhile this method is simple and computationally efficient, initializing all parameters to zero can lead to symmetry between neurons in a layer. This symmetry perpetuates throughout the network during training, resulting in every neuron in a given layer learning the same features. Consequently, this hampers the network’s ability to learn diverse representations, thereby limiting its capacity to capture complex patterns within the data.\nLet’s look at how the network performs with this initialization method:\n\n\n\nThe plot exhibits a failed classification attempt, portraying uniform predictions of one class due to ineffective weight initialization.\n\n\nLet’s understand what’s happening:\n\nZero Initialization Resulted in Symmetry: Due to initializing all weights and biases to zero, the network encounters symmetry across neurons within each layer.\nReLU Activation Outputs Zero: The ReLU activation function yields zero when the input, calculated as the product of weights and input, is zero. Therefore, \\(a = \\text{ReLU}(z) = \\max(0, z) = 0\\).\nSigmoid Activation for Classification: At the output layer, using the sigmoid activation function, the prediction (\\(y_{\\text{pred}}\\)) becomes \\(0.5\\) due to the output of \\(a\\) being zero.\nIneffective Loss Differentiation: The resulting \\(y_{\\text{pred}} = 0.5\\) for every input provides no distinction in the loss function. For both \\(y = 1\\) and \\(y = 0\\), the loss (\\(\\mathcal{L}\\)) equates to \\(0.6931\\). This lack of differentiation inhibits the adjustment of weights during training.\nStagnation in Learning: With identical loss values for both classes, there’s no gradient to prompt weight adjustments. Consequently, the model remains stuck with the initial zero weights, leading to uniform predictions of ‘0’ for all examples.\nFailure to Break Symmetry Limits Learning: The failure to break symmetry inhibits diverse learning among neurons within each layer. Consequently, the network’s capability equates to that of a linear classifier, severely limiting its capacity to learn complex patterns.\n\nThis initialization failure illustrates how initializing all weights to zero diminishes the network’s learning capacity, rendering it incapable of discerning intricate patterns within the data.",
    "crumbs": [
      "Deep Learning",
      "Practical DL",
      "Weight Initializations"
    ]
  },
  {
    "objectID": "pages/weight_init.html#random-initialization",
    "href": "pages/weight_init.html#random-initialization",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "Random Initialization",
    "text": "Random Initialization\nIntroducing randomness in weight initialization breaks symmetrical behavior within the neural network. By initializing weights randomly, each neuron begins learning distinct functions from its inputs. This specific implementation initializes weights randomly to significantly large values. The resulting function utilizes the code snippet provided below:\ndef initialize_parameters_random(layers_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims)            # integer representing the number of layers\n    \n    for l in range(1, L):\n        parameters[f\"W{l}\"] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10\n        parameters[f\"b{l}\"] = np.zeros((layers_dims[l], 1))\n        \n    return parameters\nThis initialization method sets a seed (in this case, seed 3) to ensure reproducibility and generates weights using a random normal distribution. These weights, scaled by a factor of 10, are initialized to encourage diverse learning among neurons in different layers of the neural network.\nLet’s look at how the network performs with this initialization method:\n\n\n\nThe plot showcases a dataset of two classes, visually divided by colors, although the delineation doesn’t perfectly align with the concentric circles defining the dataset.\n\n\nObservations:\n\nThe model’s initial prediction accuracy for all points is notable. However, this high starting cost is due to the utilization of large random-valued weights. This results in the last activation function (sigmoid) producing outputs close to 0 or 1 for certain examples. When misclassifications occur under these conditions, the incurred loss becomes exceedingly high, potentially reaching infinity when \\(\\log(a^{[3]}) = \\log(0)\\).\nSuboptimal initialization can lead to gradient vanishing or exploding, hindering the optimization process’s speed.\nWhile extended training might yield improved results, the initial phase with excessively large random weight values notably impedes the optimization process.\n\nIn summary:\n\nInitializing weights with notably large random values proves ineffective.\nInitializing with smaller random values is anticipated to yield better outcomes. The critical aspect to address is determining the appropriate magnitude for these random values, a pursuit we explore in the subsequent section.",
    "crumbs": [
      "Deep Learning",
      "Practical DL",
      "Weight Initializations"
    ]
  },
  {
    "objectID": "pages/weight_init.html#he-initialization",
    "href": "pages/weight_init.html#he-initialization",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "He Initialization",
    "text": "He Initialization\nThe He Initialization technique, pioneered by He et al. in 2015, offers an advanced method to initialize neural network parameters. Unlike simple zero initialization or Xavier initialization, He Initialization leverages a specific scaling factor (sqrt(2./layers_dims[l-1])) for the weights \\((W^{[1]}, W^{[2]}, ..., W^{[L]})\\).\ndef initialize_parameters_he(layers_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1\n     \n    for l in range(1, L + 1):\n        parameters[f\"W{l}\"] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2/layers_dims[l-1])\n        parameters[f\"b{l}\"] = np.zeros((layers_dims[l], 1))\n                \n    return parameters\nKey Aspects of He Initialization:\n\nScaling Factor: Employs a scaling factor of sqrt(2./layers_dims[l-1]) for initializing the weights.\nRandom Initialization: Randomizes the weights using a Gaussian distribution with mean zero and variance adjusted by the scaling factor.\n\nImpact:\n\nMitigates Vanishing/Exploding Gradients: The strategic scaling of weights aids in preventing issues like vanishing or exploding gradients during training, facilitating more stable and efficient learning.\nPromotes Non-Linearity: Enables the network to learn more diverse and intricate representations by fostering non-linear activations, thereby enhancing its capability to capture complex patterns within the data.\n\nLet’s look at how the network performs with this initialization method:\n\n\n\nThe plot visually illustrates the model’s precise classification by distinctly coloring the dataset, showcasing perfect separation between classes: one represented by a circular cluster and the remainder of the plot showcasing the alternative class.\n\n\nObservations:\n\nPerfect Separation of Classes:\n\nEffective Classification: The model utilizing He Initialization demonstrates exceptional performance by precisely segregating the two classes (represented by blue and red dots) in the scatter plot.\nAccurate Distribution Capture: Through the learned representations, the model accurately captures the distinctive distribution of the dataset.\n\nEfficient Convergence:\n\nRapid Learning: He Initialization facilitates swift convergence of the model, achieving optimal class separation in a notably small number of iterations.",
    "crumbs": [
      "Deep Learning",
      "Practical DL",
      "Weight Initializations"
    ]
  },
  {
    "objectID": "pages/weight_init.html#conclusion",
    "href": "pages/weight_init.html#conclusion",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "Conclusion",
    "text": "Conclusion\nDifferent initialization methods significantly impact neural network performance:\n\nSymmetry Breakage: Zero initialization fails to break symmetry, hindering the network’s ability to learn distinct representations.\nCaution with Weights: Large random initialization leads to excessively large weights, impacting learning.\nHe Initialization Effectiveness: He initialization emerges as a recommended method, effectively enabling the network to achieve a remarkable 99% accuracy. It aptly suits networks employing ReLU activations, showcasing its suitability for diverse architectures.\n\nKey Takeaways:\n\nInitialization Variance: Varied initializations yield divergent outcomes in network performance.\nSymmetry Disruption: Random initialization mitigates symmetry issues, allowing different units to learn diverse features.\nWeight Magnitude Control: Caution must be exercised to prevent excessively large weights.\nHe Initialization Suitability: Especially effective for networks employing ReLU activations, demonstrating superior performance and suitability for numerous architectures.",
    "crumbs": [
      "Deep Learning",
      "Practical DL",
      "Weight Initializations"
    ]
  },
  {
    "objectID": "pages/nn_basics.html",
    "href": "pages/nn_basics.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "Neural networks, composed of interconnected layers of nodes, serve as powerful computational models inspired by the human brain’s neural structure. Here’s an overview of key components and concepts:\nArchitecture:\n\nA neural network typically comprises an input layer, one or more hidden layers, and an output layer. Each layer contains nodes (neurons) that perform computations.\n\nForward Propagation:\n\nDuring forward propagation, input data \\(X\\) is passed through the network, undergoing computations and activations in each layer:\n\n\\(z^{[l] (i)} =  W^{[l]} a^{[l-1](i)} + b^{[l]}\\): Compute the weighted sum and add bias for each layer \\(l\\).\n\\(a^{[l] (i)} = g(z^{[l] (i)})\\): Apply an activation function \\(g\\) to produce the layer’s output.\n\n\nActivation Functions:\n\nActivation functions introduce non-linearity and are applied at each layer’s output:\n\nSigmoid (\\(\\sigma\\)), Tanh, ReLU, Leaky ReLU, etc., each influencing the network’s ability to learn complex patterns.\n\n\nBackpropagation:\n\nBackpropagation involves computing gradients to update network parameters, facilitating learning:\n\nDerivatives of activation functions \\(\\frac{\\partial a}{\\partial z}\\) aid in error computation and parameter updates.\nGradient Descent updates weights (\\(W\\)) and biases (\\(b\\)) using the learning rate (\\(\\alpha\\)) and computed gradients.\n\n\nImportance of Non-linear Activation:\n\nNon-linear activation functions are crucial. Without them, the network simplifies to a linear model, limiting its ability to learn intricate patterns in data.\n\nTraining and Optimization:\n\nTraining involves iteratively adjusting parameters by minimizing a defined loss function (e.g., cross-entropy, mean squared error) using techniques like gradient descent.",
    "crumbs": [
      "Deep Learning",
      "Neural Networks",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/backprop.html",
    "href": "pages/backprop.html",
    "title": "Backpropagation",
    "section": "",
    "text": "Backpropagation is the process of iteratively computing gradients of the loss function with respect to the weights and biases of the neural network, enabling the adjustment of parameters by propagating errors backward through the network’s layers to enhance learning and optimize performance.\n\nOutput Layer Error Derivative:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial z_{L}^{(i)} } = \\frac{1}{m} (\\hat{y}^{(i)} - y^{(i)}) \\odot g'(z^{[L] (i)})\\)\n\nCalculate the derivative of the cost function with respect to the pre-activation value of the output layer for each training example.\n\\(\\odot\\) denotes element-wise multiplication.\n\\(g'(\\cdot)\\) represents the derivative of the activation function used in the output layer.\n\n\nGradient Calculation for Output Layer Weights and Bias:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial W^{[L]} } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{L}^{(i)} } a^{[L-1] (i) T}\\)\n\nCompute the gradient of the loss function with respect to the weights of the output layer.\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial b^{[L]} } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{L}^{(i)}}}\\)\n\nCompute the gradient of the loss function with respect to the bias of the output layer.\n\n\nBackpropagate the Error to Previous Layers:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial z_{l}^{(i)} } =  W^{[l+1] T} \\frac{\\partial \\mathcal{J} }{ \\partial z_{l+1}^{(i)} } \\odot g'(z^{[l] (i)})\\)\n\nPropagate the error derivative backward through the layers by considering the relationship between consecutive layers.\nInvolves the transpose of weights connecting the subsequent and current layers.\nInvolves the derivative of the activation function used in layer \\(l\\).\n\n\nGradient Calculation for Weights and Bias of Hidden Layers:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{l}^{(i)} }  a^{[l-1] (i) T}\\)\n\nCompute the gradient of the loss function with respect to the weights of layer \\(l\\).\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial b^{[l]} } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{l}^{(i)}}}\\)\n\nCompute the gradient of the loss function with respect to the bias of layer \\(l\\).\n\n\n\nThese steps form the core of backpropagation in a shallow neural network, allowing for the iterative adjustment of weights and biases to minimize the loss function and improve the model’s accuracy in predicting outputs for given inputs.\n\n\n\n\n\n\nCode\n\n\n\n\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n    \n    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n        \n    dZ2 = A2 - Y\n    dW2 = (1/Y.shape[1])*dZ2@A1.T\n    db2 = (1/Y.shape[1])*np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = W2.T@dZ2*(1-A1**2)\n    dW1 = (1/Y.shape[1])*dZ1@X.T\n    db1 = (1/Y.shape[1])*np.sum(dZ1, axis=1, keepdims=True)\n        \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads",
    "crumbs": [
      "Deep Learning",
      "Neural Networks",
      "Backpropagation"
    ]
  },
  {
    "objectID": "pages/notations_nn .html",
    "href": "pages/notations_nn .html",
    "title": "Common Notations Used in Neural Networks",
    "section": "",
    "text": "Superscript and Subscript Indices:\n\n\\([i]\\): Represents the index referring to a specific training example within the dataset.\n\\([l]\\): Denotes the layer indices. In a network with \\(L\\) layers, \\([l]\\) ranges from \\(1\\) to \\(L\\).\n\nVariables:\n\n\\(x^{(i)}\\): Input features for the \\(i\\)-th training example.\n\\(y^{(i)}\\): Actual output or target for the \\(i\\)-th training example.\n\\(\\hat{y}^{(i)}\\), \\(a^{[l](i)}\\): Predicted output or activation of layer \\(l\\) for the \\(i\\)-th training example.\n\\(z^{[l](i)}\\): Pre-activation value of layer \\(l\\) for the \\(i\\)-th training example.\n\nWeights and Biases:\n\n\\(W^{[l]}\\): Weight matrix associated with the connections between layer \\(l - 1\\) and layer \\(l\\).\n\\(b^{[l]}\\): Bias vector added to the weighted sum in layer \\(l\\).\n\nDerivatives and Gradients:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial z^{[l](i)} }\\): Derivative of the cost function with respect to the pre-activation value of layer \\(l\\) for the \\(i\\)-th training example.\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} }\\): Gradient of the loss function with respect to the weights of layer \\(l\\).\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial b^{[l]} }\\): Gradient of the loss function with respect to the bias of layer \\(l\\).\n\nCost Function:\n\n\\(J\\): Overall cost function that measures the discrepancy between predicted and actual outputs across all training examples.\n\\(m\\): Number of training examples in the dataset.\n\nActivation Functions:\n\n\\(\\sigma()\\): Activation function applied element-wise to the weighted sums in each layer.\n\nMathematical Operations:\n\n\\(\\log()\\): Natural logarithm function used in the calculation of the cost function.\n\\(\\odot\\): Element-wise multiplication.\n\n\nThese notations provide a general framework for understanding and representing variables, derivatives, weights, biases, and mathematical operations involved in the computations and optimizations of neural networks across different architectures and configurations.",
    "crumbs": [
      "Deep Learning",
      "Notations"
    ]
  },
  {
    "objectID": "pages/percp.html",
    "href": "pages/percp.html",
    "title": "Perceptron",
    "section": "",
    "text": "The Perceptron Learning Algorithm (PLA) is a supervised learning algorithm widely employed for binary classification tasks. Its primary objective is to determine a decision boundary that effectively separates the two classes in the dataset. This algorithm belongs to the class of discriminative classification methods as it focuses on modeling the boundary between classes instead of characterizing the underlying probability distribution of each class.\nLet \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) represent the dataset, where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe algorithm is founded on the following assumptions:\n\n\\(P(y=1|\\mathbf{x}) = 1\\) if \\(\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\geq0\\), otherwise \\(P(y=1|\\mathbf{x}) = 0\\).\nLinear Separability Assumption: The Linear Separability Assumption is a fundamental assumption made in various machine learning algorithms, including the Perceptron Learning Algorithm. It posits that the classes to be classified can be accurately separated by a linear decision boundary. In other words, there exists a hyperplane in the feature space that can effectively segregate the data points of the two classes.\n\nThe objective function is defined as follows:\n\\[\n\\min _{h \\in \\mathcal{H}} \\sum _{i=1} ^n \\mathbb{1}\\left ( h(\\mathbf{x}_i) \\neq y_i \\right )\n\\]\nEven if \\(\\mathcal{H}\\) accounts only for the Linear Hypotheses, this problem is generally considered NP-Hard.\nUnder the Linear Separability Assumption, assuming the existence of \\(\\mathbf{w} \\in \\mathbb{R}^d\\) such that \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)=y_i\\) holds for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), the PLA solves the convergence problem using an iterative algorithm. The algorithm proceeds as follows:\n\nInitialize \\(\\mathbf{w}^0 = \\mathbf{0} \\in \\mathbb{R}^d\\)\nUntil Convergence:\n\nSelect a \\((\\mathbf{x}_i, y_i)\\) pair from the dataset\nIf \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)\n\nDo nothing\n\nElse\n\nUpdate the weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\)\n\nEnd\n\n\n\n\nFor a given training example \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input and \\(y\\) represents the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates the weight vector \\(\\mathbf{w}\\) according to the following rules:\n\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is correct (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)), no update is performed.\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is incorrect (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)\\neq y_i\\)), the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\).\nIt is important to note that the update occurs solely in response to the current data point. Consequently, data points that were previously classified correctly may not be classified similarly in future iterations.\n\nThis update rule effectively adjusts the decision boundary in the direction of correct classification for the misclassified example. The algorithm is guaranteed to converge to a linearly separable solution if the data is indeed linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution.\n\n\n\nWe introduce three additional assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) is considered linearly separable with a \\(\\gamma\\)-margin if there exists \\(\\mathbf{w}^* \\in \\mathbb{R}^d\\) such that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\geq\\gamma\\) holds for all \\(i\\), where \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let \\(R&gt;0 \\in \\mathbb{R}\\) be a constant such that \\(\\forall i \\in D\\), \\(||\\mathbf{x}_i||\\leq R\\). In other words, \\(R\\) denotes the length of the data point farthest from the center.\nNormal Length for \\(\\mathbf{w}^*\\): Assume that \\(\\mathbf{w}^*\\) has unit length.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Perceptron"
    ]
  },
  {
    "objectID": "pages/percp.html#analysis-of-the-update-rule",
    "href": "pages/percp.html#analysis-of-the-update-rule",
    "title": "Perceptron",
    "section": "",
    "text": "For a given training example \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input and \\(y\\) represents the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates the weight vector \\(\\mathbf{w}\\) according to the following rules:\n\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is correct (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)), no update is performed.\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is incorrect (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)\\neq y_i\\)), the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\).\nIt is important to note that the update occurs solely in response to the current data point. Consequently, data points that were previously classified correctly may not be classified similarly in future iterations.\n\nThis update rule effectively adjusts the decision boundary in the direction of correct classification for the misclassified example. The algorithm is guaranteed to converge to a linearly separable solution if the data is indeed linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Perceptron"
    ]
  },
  {
    "objectID": "pages/percp.html#further-assumptions",
    "href": "pages/percp.html#further-assumptions",
    "title": "Perceptron",
    "section": "",
    "text": "We introduce three additional assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) is considered linearly separable with a \\(\\gamma\\)-margin if there exists \\(\\mathbf{w}^* \\in \\mathbb{R}^d\\) such that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\geq\\gamma\\) holds for all \\(i\\), where \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let \\(R&gt;0 \\in \\mathbb{R}\\) be a constant such that \\(\\forall i \\in D\\), \\(||\\mathbf{x}_i||\\leq R\\). In other words, \\(R\\) denotes the length of the data point farthest from the center.\nNormal Length for \\(\\mathbf{w}^*\\): Assume that \\(\\mathbf{w}^*\\) has unit length.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Perceptron"
    ]
  },
  {
    "objectID": "pages/dec_trees.html",
    "href": "pages/dec_trees.html",
    "title": "Decision tree",
    "section": "",
    "text": "Decision trees are widely used in machine learning for classification and regression tasks. They operate by recursively partitioning the data based on the most informative features until a stopping criterion is met. Decision trees can be visualized as tree-like structures.\nConsider a dataset \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\), where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and let \\(\\{y_1, \\ldots, y_n\\}\\) be the corresponding labels, where \\(y_i \\in \\{0, 1\\}\\). The output of the decision tree algorithm is a constructed decision tree.\nPrediction: Given a test sample \\(\\mathbf{x}_{\\text{test}}\\), we traverse the decision tree to reach a leaf node, and the label assigned to the leaf node is considered as \\(y_{\\text{test}}\\).\nThe following diagram depicts a decision tree:\n\nHere, a question refers to a (feature, value) pair. For example, \\(height \\le 180\\text{cm}\\)?\n\n\nTo evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]\n\n\n\nThe decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "pages/dec_trees.html#goodness-of-a-question",
    "href": "pages/dec_trees.html#goodness-of-a-question",
    "title": "Decision tree",
    "section": "",
    "text": "To evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "pages/dec_trees.html#decision-tree-algorithm",
    "href": "pages/dec_trees.html#decision-tree-algorithm",
    "title": "Decision tree",
    "section": "",
    "text": "The decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "pages/lasso_lin_reg.html",
    "href": "pages/lasso_lin_reg.html",
    "title": "Lasso Regression",
    "section": "",
    "text": "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a linear regression technique that employs a regularization approach to shrink the coefficients of less important features to zero. This method effectively performs feature selection and mitigates overfitting.\nThe objective function of lasso regression is given by:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum^n_{i=1}(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 + \\lambda||\\mathbf{w}||_1^2\n\\]\nLasso regression is similar to ridge regression, with the key difference being the use of \\(||\\mathbf{w}||_1^2\\) instead of \\(||\\mathbf{w}||_2^2\\), representing the squared L1 norm of \\(\\mathbf{w}\\).\n\n\n\nPictoral Representation of what Lasso Regression does.\n\n\nLasso regression does not have a closed-form solution and is often solved using sub-gradients. Further information on sub-gradients can be found here.\nIn conclusion, lasso regression shrinks the coefficients of less important features to exactly zero, enabling feature selection.",
    "crumbs": [
      "Supervised Learning",
      "Regression",
      "LASSO regression"
    ]
  },
  {
    "objectID": "pages/bayes_lin_reg.html",
    "href": "pages/bayes_lin_reg.html",
    "title": "Bayesian view of least squares regression",
    "section": "",
    "text": "Probabilistic View of Linear Regression\nConsider a dataset \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\) with \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and the corresponding labels \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\) with \\(\\mathbf{y}_i \\in \\mathbb{R}\\). The probabilistic view of linear regression assumes that the target variable \\(\\mathbf{y}_i\\) can be modeled as a linear combination of the input features \\(\\mathbf{x}_i\\), with an additional noise term \\(\\epsilon\\) following a zero-mean Gaussian distribution with variance \\(\\sigma^2\\). Mathematically, this can be expressed as:\n\\[\n\\mathbf{y}_i = \\mathbf{w}^T\\mathbf{x}_i + \\epsilon_i\n\\]\nwhere \\(\\mathbf{w} \\in \\mathbb{R}^d\\) represents the weight vector that captures the relationship between the inputs and the target variable.\nTo estimate the weight vector \\(\\mathbf{w}\\) that best fits the data, we can apply the principle of Maximum Likelihood (ML). The ML estimation seeks to find the parameter values that maximize the likelihood of observing the given data.\nAssuming that the noise term \\(\\epsilon_i\\) follows a zero-mean Gaussian distribution with variance \\(\\sigma^2\\), we can express the likelihood function as:\n\\[\\begin{align*}\n\\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y}) &= P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) \\\\\n&= \\prod_{i=1}^n P(\\mathbf{y}_i|\\mathbf{x}_i; \\mathbf{w}) \\\\\n&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2}{2\\sigma^2}\\right)\n\\end{align*}\\]\nTaking the logarithm of the likelihood function, we have:\n\\[\\begin{align*}\n\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y}) &= \\sum_{i=1}^n \\log \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right) - \\frac{(\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2}{2\\sigma^2} \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2\n\\end{align*}\\]\nTo find the maximum likelihood estimate \\(\\mathbf{w}_{\\text{ML}}\\), we want to maximize \\(\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})\\). Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Thus, we seek to minimize:\n\\[\\begin{align*}\n-\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y}) &= \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2\n\\end{align*}\\]\nThis expression is equivalent to the mean squared error (MSE) objective function used in linear regression. Therefore, finding the maximum likelihood estimate \\(\\mathbf{w}_{\\text{ML}}\\) is equivalent to solving the linear regression problem using the squared error loss.\nTo obtain the closed-form solution for \\(\\mathbf{w}_{\\text{ML}}\\), we differentiate the negative log-likelihood with respect to \\(\\mathbf{w}\\) and set the derivative to zero:\n\\[\\begin{align*}\n\\nabla_{\\mathbf{w}} \\left(-\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})\\right) &= \\frac{1}{\\sigma^2}\\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)\\mathbf{x}_i^T = \\mathbf{0}\n\\end{align*}\\]\nThis can be rewritten as:\n\\[\\begin{align*}\n\\frac{1}{\\sigma^2}\\left(\\mathbf{X}\\mathbf{X}^T\\mathbf{w} - \\mathbf{X}\\mathbf{y}\\right) &= \\mathbf{0}\n\\end{align*}\\]\nwhere \\(\\mathbf{X}\\) is the matrix whose rows are the input vectors \\(\\mathbf{x}_i\\) and \\(\\mathbf{y}\\) is the column vector of labels. Rearranging the equation, we have:\n\\[\\begin{align*}\n\\mathbf{X}\\mathbf{X}^T\\mathbf{w} &= \\mathbf{X}\\mathbf{y}\n\\end{align*}\\]\nTo obtain the closed-form solution for \\(\\mathbf{w}_{\\text{ML}}\\), we multiply both sides by the inverse of \\(\\mathbf{X}\\mathbf{X}^T\\), denoted as \\((\\mathbf{X}\\mathbf{X}^T)^{-1}\\):\n\\[\\begin{align*}\n\\mathbf{w}_{\\text{ML}} &= (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{X}\\mathbf{y}\n\\end{align*}\\]\nThus, the closed-form solution for the maximum likelihood estimate \\(\\mathbf{w}_{\\text{ML}}\\) is given by the product of \\((\\mathbf{X}\\mathbf{X}^T)^{-1}\\) and \\(\\mathbf{X}\\mathbf{y}\\).\nThe closed-form solution for \\(\\mathbf{w}_{\\text{ML}}\\) in linear regression demonstrates that it can be obtained by directly applying a matrix inverse operation to the product of the input matrix \\(\\mathbf{X}\\) and the target variable vector \\(\\mathbf{y}\\). This closed-form solution provides an efficient and direct way to estimate the weight vector \\(\\mathbf{w}\\) based on the given data.\n\n\nEvaluation of the Maximum Likelihood Estimator for Linear Regression\nLinear regression is a widely used technique for modeling the relationship between a dependent variable and one or more independent variables. The maximum likelihood estimator (MLE) is commonly employed to estimate the parameters of a linear regression model. Here, we discuss the goodness of the MLE for linear regression, explore cross-validation techniques to minimize mean squared error (MSE), examine Bayesian modeling as an alternative approach, and finally, delve into ridge and lasso regression as methods to mitigate overfitting.\n\n\nGoodness of Maximum Likelihood Estimator for Linear Regression\nConsider a dataset comprising input vectors \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\), where each \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and corresponding labels \\(\\{y_1, \\ldots, y_n\\}\\), with \\(y_i \\in \\mathbb{R}\\). We can express the relationship between the inputs and labels using the linear regression model:\n\\[\ny|\\mathbf{X} = \\mathbf{w}^T\\mathbf{x} + \\epsilon\n\\]\nHere, \\(\\epsilon\\) represents the random noise following a normal distribution \\(\\mathcal{N}(0,\\sigma^2)\\), and \\(\\mathbf{w} \\in \\mathbb{R}^d\\) denotes the regression coefficients. The maximum likelihood parameter estimation for linear regression, denoted as \\(\\hat{\\mathbf{w}}_{\\text{ML}}\\), can be computed as:\n\\[\n\\hat{\\mathbf{w}}_{\\text{ML}} = \\mathbf{w}^* = (\\mathbf{X}\\mathbf{X}^T)^+\\mathbf{X}\\mathbf{y}\n\\]\nTo evaluate the quality of the estimated parameters, we measure the mean squared error (MSE) between the estimated parameters and the true parameters \\(\\mathbf{w}\\). The MSE is given by:\n\\[\n\\mathbb{E}[|| \\hat{\\mathbf{w}}_{\\text{ML}} - \\mathbf{w} ||^2_2]\n\\]\nInterestingly, the MSE can be expressed as:\n\\[\n\\mathbb{E}[|| \\hat{\\mathbf{w}}_{\\text{ML}} - \\mathbf{w} ||^2_2] = \\sigma^2 \\cdot \\text{trace}((\\mathbf{X}\\mathbf{X}^T)^{-1})\n\\]\nThis result provides a quantitative measure of the goodness of the maximum likelihood estimator for linear regression.\n\n\nCross-Validation for Minimizing MSE\nIn order to minimize the MSE, we can utilize cross-validation techniques. Let the eigenvalues of \\(\\mathbf{X}\\mathbf{X}^T\\) be denoted as \\(\\{\\lambda_1, \\ldots, \\lambda_d\\}\\). Consequently, the eigenvalues of \\((\\mathbf{X}\\mathbf{X}^T)^{-1}\\) are given by \\(\\{\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_d}\\}\\).\nThe MSE can be expressed as:\n\\[\n\\mathbb{E}[|| \\hat{\\mathbf{w}}_{\\text{ML}} - \\mathbf{w} ||^2_2] = \\sigma^2 \\sum_{i=1}^d \\frac{1}{\\lambda_i}\n\\]\nTo improve the estimator, we introduce a modified estimator, denoted as \\(\\hat{\\mathbf{w}}_{\\text{new}}\\), defined as:\n\\[\n\\hat{\\mathbf{w}}_{\\text{new}} = (\\mathbf{X}\\mathbf{X}^T + \\lambda \\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}\n\\]\nHere, \\(\\lambda \\in \\mathbb{R}\\) and \\(\\mathbf{I} \\in \\mathbb{R}^{d\\times d}\\) represents the identity matrix. By utilizing this modified estimator, we can calculate:\n\\[\n\\text{trace}((\\mathbf{X}\\mathbf{X}^T + \\lambda \\mathbf{I})^{-1}) = \\sum_{i=1}^d \\frac{1}{\\lambda_i + \\lambda}\n\\]\nAccording to the Existence Theorem, there exists a value of \\(\\lambda\\) such that \\(\\hat{\\mathbf{w}}_{\\text{new}}\\) exhibits a lower mean squared error than \\(\\hat{\\mathbf{w}}_{\\text{ML}}\\). In practice, the value for \\(\\lambda\\) is determined using cross-validation techniques.\nThree commonly used techniques for cross-validation are as follows:\n\nTraining-Validation Split: The training set is randomly divided into a training set and a validation set, typically in an 80:20 ratio. Among various \\(\\lambda\\) values, the one that yields the lowest error is selected.\nK-Fold Cross Validation: The training set is partitioned into K equally-sized parts. The model is trained K times, each time using K-1 parts as the training set and the remaining part as the validation set. The \\(\\lambda\\) value that leads to the lowest average error is chosen.\nLeave-One-Out Cross Validation: The model is trained using all but one sample in the training set, and the left-out sample is used for validation. This process is repeated for each sample in the dataset. The optimal \\(\\lambda\\) is determined based on the average error across all iterations.\n\nBy employing cross-validation techniques, we can enhance the performance of the linear regression model by selecting an appropriate value of \\(\\lambda\\).\n\n\nBayesian Modeling\nAlternatively, we can understand the maximum likelihood estimator \\(\\hat{\\mathbf{w}}_{\\text{ML}}\\) in the context of Bayesian modeling.\nAssume that \\(P(y|\\mathbf{X})\\) follows a normal distribution \\(\\mathcal{N}(\\mathbf{w}^T\\mathbf{x},\\mathbf{I})\\), where \\(I\\) represents the identity matrix for simplicity.\nFor the prior distribution of \\(\\mathbf{w}\\), a suitable choice is the normal distribution \\(\\mathcal{N}(0,\\gamma^2\\mathbf{I})\\), where \\(\\gamma^2\\mathbf{I} \\in\\mathbb{R}^{d\\times d}\\).\nThus, we can write:\n\\[\\begin{align*}\nP(\\mathbf{w}|\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}) &\\propto P(\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}|\\mathbf{w})*P(\\mathbf{w}) \\\\\n&\\propto \\left ( \\prod_{i=1}^n e^{\\frac{-(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2}{2}}  \\right ) * \\left ( \\prod_{i=1}^d  e^{\\frac{-(\\mathbf{w}_i - 0)^2}{2\\gamma^2}} \\right ) \\\\\n&\\propto \\left ( \\prod_{i=1}^n e^{\\frac{-(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2}{2}}  \\right ) * e^{\\frac{-||\\mathbf{w}||^2}{2\\gamma^2}}\n\\end{align*}\\]\nTaking the logarithm, we obtain:\n\\[\n\\log(P(\\mathbf{w}|\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\})) \\propto \\frac{-(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2}{2}-\\frac{||\\mathbf{w}||^2}{2\\gamma^2}\n\\]\nUpon computing the gradient, we find:\n\\[\n\\nabla \\log(P(\\mathbf{w}|\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\})) \\propto  (\\mathbf{X}\\mathbf{X}^T)\\hat{\\mathbf{w}}_{\\text{MAP}} - \\mathbf{X}\\mathbf{y} + \\frac{\\hat{\\mathbf{w}}_{\\text{MAP}}}{\\gamma^2}\n\\]\nConsequently, the maximum a posteriori estimate (MAP) for \\(\\mathbf{w}\\) can be computed as:\n\\[\n\\hat{\\mathbf{w}}_{\\text{MAP}} = (\\mathbf{X}\\mathbf{X}^T + \\frac{1}{\\gamma^2} \\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}\n\\]\nIn practice, the value of \\(\\frac{1}{\\gamma^2}\\) is obtained using cross-validation. Remarkably, this maximum a posteriori estimation for linear regression with a Gaussian prior \\(\\mathcal{N}(0,\\gamma^2\\mathbf{I})\\) for \\(\\mathbf{w}\\) is equivalent to the modified estimator \\(\\hat{\\mathbf{w}}_{\\text{new}}\\) discussed earlier.",
    "crumbs": [
      "Supervised Learning",
      "Regression",
      "Bayesian view of least squares regression"
    ]
  },
  {
    "objectID": "pages/lin_reg.html",
    "href": "pages/lin_reg.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a supervised learning algorithm employed to predict a continuous output variable based on one or more input features, assuming a linear relationship between the input and output variables. The primary objective of linear regression is to determine the line of best fit that minimizes the sum of squared errors between the predicted and actual output values.\nGiven a dataset \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\) where each \\(\\mathbf{x}_i\\) belongs to \\(\\mathbb{R}^d\\), and the corresponding labels \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\) belong to \\(\\mathbb{R}\\), the goal of linear regression is to find a mapping between the input and output variables, represented as follows:\n\\[\nh: \\mathbb{R}^d \\rightarrow \\mathbb{R}\n\\]\nThe error for this mapping function can be quantified as:\n\\[\n\\text{error}(h) = \\sum_{i=1}^n (h(\\mathbf{x}_i) - \\mathbf{y}_i)^2\n\\]\nIdeally, this error should be minimized, which occurs when \\(h(\\mathbf{x}_i) = \\mathbf{y}_i\\) for all \\(i\\). However, achieving this may only result in memorizing the data and its outputs, which is not a desired outcome.\nTo mitigate the memorization problem, introducing a structure to the mapping becomes necessary. The simplest and commonly used structure is linear, which we will adopt as the underlying structure for our data.\nLet \\(\\mathcal{H}_{\\text{linear}}\\) denote the solution space for the mapping in the linear domain:\n\\[\n\\mathcal{H}_{\\text{linear}} = \\left\\lbrace h_w: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\ \\text{s.t.} \\ h_w(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} \\ \\forall \\mathbf{w} \\in \\mathbb{R}^d \\right\\rbrace\n\\]\nThus, our objective is to minimize:\n\\[\\begin{align*}\n\\min_{h \\in \\mathcal{H}_{\\text{linear}}} \\sum_{i=1}^n (h(\\mathbf{x}_i) - \\mathbf{y}_i)^2 \\\\\n\\text{Equivalently,} \\\\\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2\n\\end{align*}\\]\nOptimizing the above objective is the main aim of the linear regression algorithm.",
    "crumbs": [
      "Supervised Learning",
      "Regression",
      "Least squares regression"
    ]
  },
  {
    "objectID": "pages/lin_reg.html#stochastic-gradient-descent",
    "href": "pages/lin_reg.html#stochastic-gradient-descent",
    "title": "Linear Regression",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm widely employed in machine learning to minimize the loss function of a model by determining the optimal parameters. Unlike traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters using a randomly selected subset of the data, known as a batch. This approach leads to faster training times and makes SGD particularly suitable for handling large datasets.\nInstead of updating \\(\\mathbf{w}\\) using the entire dataset at each step \\(t\\), SGD leverages a small randomly selected subset of \\(k\\) data points to update \\(\\mathbf{w}\\). Consequently, the new gradient becomes \\(2(\\tilde{\\mathbf{X}}\\tilde{\\mathbf{X}}^T\\mathbf{w}^t - \\tilde{\\mathbf{X}}\\tilde{\\mathbf{y}})\\), where \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{y}}\\) represent small samples randomly chosen from the dataset. This strategy is feasible since \\(\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{d \\times k}\\), which is considerably smaller compared to \\(\\mathbf{X}\\).\nAfter \\(T\\) rounds of training, the final estimate is obtained as follows:\n\\[\n\\mathbf{w}_{\\text{SGD}}^T = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{w}^i\n\\]\nThe stochastic nature of SGD contributes to optimal convergence to a certain extent.\n\nImplementing Stochastic Gradient Descent in Python\nLet compute the \\(w\\) vector using Stochastic Gradient Descent and print the coefficients and intercept.\nw = np.ones((X.shape[0], 1))\neta = 1e-4\nt = 100000\nn = X.shape[1]\nb = 10\n\nfor i in range(t):\n    # randomly select a batch of samples\n    idx = np.random.choice(n, b, replace=False)\n    X_b = X[:, idx]\n    y_b = y[idx]\n    # compute the gradient for the batch\n    grad = 2*(X_b @ X_b.T @ w) - 2*(X_b @ y_b)\n    # update the weights\n    w = w - eta*grad\n    \nprint ('Coefficients: ', w.reshape((-1,))[:3])\nprint ('Intercept: ', w.reshape((-1,))[-1])\n\n\n\n\n\n\nCoefficients: [10.72000912, 8.3366805, 9.13970723]\nIntercept: 65.2366350549217",
    "crumbs": [
      "Supervised Learning",
      "Regression",
      "Least squares regression"
    ]
  },
  {
    "objectID": "pages/gmm.html",
    "href": "pages/gmm.html",
    "title": "Gaussian mixture model (GMM)",
    "section": "",
    "text": "Gaussian Mixture Models are a type of probabilistic model used to represent complex data distributions by combining multiple Gaussian distributions.\nThe procedure is as follows:\n\nStep 1: Generate a mixture component among \\(\\{1, 2, \\ldots, K\\}\\) where \\(z_i \\in \\{1, 2, \\ldots, K\\}\\). We obtain, \\[\nP(z_i=k) = \\pi_k \\hspace{2em} \\left [ \\sum _{i=1} ^K \\pi_i = 1 \\hspace{1em} 0 \\le \\pi_i \\le 1 \\hspace{1em} \\forall i \\right ]\n\\]\nStep 2: Generate \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{z_i}, \\boldsymbol{\\sigma}^2_{z_i})\\)\n\nHence, there are \\(3K\\) parameters. However, since \\(\\displaystyle \\sum _{i=1} ^K \\pi_i = 1\\), the number of parameters to be estimated becomes \\(3K-1\\) for a GMM with \\(K\\) components.\n\nLikelihood of GMM’s\n\\[\\begin{align*}\n\\mathcal{L}\\left( \\begin{array}{cccc}\n\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K \\\\\n\\boldsymbol{\\sigma}^2_1, \\boldsymbol{\\sigma}^2_2, \\ldots, \\boldsymbol{\\sigma}^2_K\\\\\n\\pi_1, \\pi_2, \\ldots, \\pi_K\n\\end{array}; \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n \\right )\n&= \\prod _{i=1} ^n f_{\\text{mix}} \\left( \\mathbf{x}_i; \\begin{array}{cccc}\n\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K \\\\\n\\boldsymbol{\\sigma}^2_1, \\boldsymbol{\\sigma}^2_2, \\ldots, \\boldsymbol{\\sigma}^2_K\\\\\n\\pi_1, \\pi_2, \\ldots, \\pi_K\n\\end{array} \\right ) \\\\\n&= \\prod _{i=1} ^n \\left [ \\sum _{k=1} ^K \\pi_k * f_{\\text{mix}}(\\mathbf{x}_i; \\boldsymbol{\\mu}_k, \\boldsymbol{\\sigma}_k) \\right ] \\\\\n\\therefore \\log\\mathcal{L}(\\boldsymbol{\\theta}) &= \\sum _{i=1} ^n \\log \\left [ \\sum _{k=1} ^K \\pi_k * \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}_k} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu}_k)^2}{2\\boldsymbol{\\sigma}^2_k}} \\right ] \\\\\n\\end{align*}\\] To solve the above equation, we need to understand convexity.",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Gaussian mixture model (GMM)"
    ]
  },
  {
    "objectID": "pages/mle.html",
    "href": "pages/mle.html",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "",
    "text": "Estimation in machine learning involves inferring unknown parameters or predicting outcomes from observed data. Estimators, often algorithms or models, are used for these tasks and to characterize the data’s underlying distribution.\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) represent a dataset, where each data point \\(\\mathbf{x}_i\\) is in the \\(d\\)-dimensional binary space \\(\\{0,1\\}^d\\). It is assumed that the data points are independent and identically distributed (i.i.d).\nIndependence is denoted as \\(P(\\mathbf{x}_i|\\mathbf{x}_j) = P(\\mathbf{x}_i)\\). Identically distributed means \\(P(\\mathbf{x}_i)=P(\\mathbf{x}_j)=p\\).",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Maximum likelihood estimation (MLE)"
    ]
  },
  {
    "objectID": "pages/mle.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/mle.html#fishers-principle-of-maximum-likelihood",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate parameters of a statistical model by selecting values that maximize the likelihood function. This function quantifies how well the model fits the observed data.",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Maximum likelihood estimation (MLE)"
    ]
  },
  {
    "objectID": "pages/mle.html#likelihood-estimation-for-bernoulli-distributions",
    "href": "pages/mle.html#likelihood-estimation-for-bernoulli-distributions",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Likelihood Estimation for Bernoulli Distributions",
    "text": "Likelihood Estimation for Bernoulli Distributions\nApplying the likelihood function on the aforementioned dataset, we obtain: \\[\\begin{align*}\n\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= P(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;p)\\\\\n&= p(\\mathbf{x}_1;p)p(\\mathbf{x}_2;p)\\ldots p(\\mathbf{x}_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i\n\\end{align*}\\]",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Maximum likelihood estimation (MLE)"
    ]
  },
  {
    "objectID": "pages/mle.html#likelihood-estimation-for-gaussian-distributions",
    "href": "pages/mle.html#likelihood-estimation-for-gaussian-distributions",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Likelihood Estimation for Gaussian Distributions",
    "text": "Likelihood Estimation for Gaussian Distributions\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2)\\). We assume that the data points are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= f_{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n  f_{\\mathbf{x}_i}(\\mathbf{x}_i;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}}  \\right ) - \\frac{(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{By differentiating with respect to $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\boldsymbol{\\mu}}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i \\\\\n\\hat{\\boldsymbol{\\sigma}^2}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n (\\mathbf{x}_i-\\boldsymbol{\\mu})^T(\\mathbf{x}_i-\\boldsymbol{\\mu})\n\\end{align*}\\]",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Maximum likelihood estimation (MLE)"
    ]
  },
  {
    "objectID": "pages/kmeans.html",
    "href": "pages/kmeans.html",
    "title": "K Means",
    "section": "",
    "text": "Clustering represents an essential method in unsupervised machine learning aimed at grouping similar objects into clusters, thereby revealing inherent structures within the data for exploratory analysis or serving as a preprocessing step for subsequent algorithms.\nOur primary objective is to partition a set of \\(n\\) datapoints into \\(k\\) clusters.\nNotation: \\[\n\\mathbf{X}=\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n \\} \\quad \\mathbf{x}_i \\in \\mathbb{R}^d\n\\] \\[\nS = \\{\\mathbf{z} \\quad \\forall \\mathbf{z} \\in \\{1, 2, \\ldots k\\}^n \\}\n\\] \\[\n\\boldsymbol{\\mu} _k = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i=k)}}\n\\]\nWhere:\n\n\\(\\mathbf{x}_i\\) denotes the \\(i^{th}\\) datapoint.\n\\(z_i\\) denotes the cluster indicator of \\(\\mathbf{x}_i\\).\n\\(\\boldsymbol{\\mu}_{z_i}\\) denotes the mean of the cluster with indicator \\(z_i\\).\n\\(S\\) represents the set of all possible cluster assignments. It is important to note that \\(S\\) is finite (\\(k^n\\)).\n\nGoal: \\[\n\\min _{\\mathbf{z} \\in S} \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||}^2\n\\]\nHowever, the manual solution to this optimization problem is classified as an NP-Hard problem, which necessitates considering alternative approaches to approximate its solution due to computational constraints.",
    "crumbs": [
      "Unsupervised Learning",
      "Clustering",
      "K-means clustering"
    ]
  },
  {
    "objectID": "pages/kmeans.html#the-algorithm",
    "href": "pages/kmeans.html#the-algorithm",
    "title": "K Means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm proceeds as follows:\nStep 1: Initialization: Randomly assign datapoints from the dataset as the initial cluster centers.\nStep 2: Reassignment Step: \\[\nz _i ^{t} = \\underset{k}{\\arg \\min} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 3: Compute Means: \\[\n\\boldsymbol{\\mu} _k ^{t+1} = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until the cluster assignments do not change.",
    "crumbs": [
      "Unsupervised Learning",
      "Clustering",
      "K-means clustering"
    ]
  },
  {
    "objectID": "pages/PCA.html",
    "href": "pages/PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Representation learning is a fundamental sub-field of machine learning that is concerned with acquiring meaningful and compact representations of intricate data, facilitating various tasks such as dimensionality reduction, clustering, and classification.\nLet us consider a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\), where each \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\). The objective is to find a representation that minimizes the reconstruction error.\nWe can start by seeking the best linear representation of the dataset, denoted by \\(\\mathbf{w}\\), subject to the constraint \\(||\\mathbf{w}||=1\\).\nThe representation is given by, \\[\\begin{align*}\n    \\frac{(\\mathbf{x}_i^T\\mathbf{w})}{\\mathbf{w}^T\\mathbf{w}}&\\mathbf{w} \\\\\n    \\text{However, }||\\mathbf{w}||&=1\\\\\n    \\therefore \\text{ Projection } &= (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}\n\\end{align*}\\]\nThe reconstruction error is computed as follows, \\[\n\\text{Reconstruction Error}(f(\\mathbf{w})) = \\frac{1}{n} \\sum _{i=1} ^{n} || \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w} || ^ 2\n\\] where \\(\\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}\\) is termed the residue and can be represented as \\(\\mathbf{x}'\\).\nThe primary aim is to minimize the reconstruction error, leading to the following optimization formulation: \\[\\begin{align*}\n    \\min _{\\mathbf{w} \\in ||\\mathbf{w}|| = 1} f(\\mathbf{w}) &= \\frac{1}{n} \\sum _{i=1} ^{n} -(\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n    \\therefore \\max _{\\mathbf{w} \\in ||\\mathbf{w}|| = 1} f(\\mathbf{w}) &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n    &= \\mathbf{w}^T(\\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\\mathbf{x}_i^T)\\mathbf{w} \\\\\n    \\max _{\\mathbf{w} \\in ||\\mathbf{w}|| = 1} f(\\mathbf{w}) &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w}\n\\end{align*}\\] where \\(\\mathbf{C}=\\displaystyle \\frac{1}{n} \\displaystyle \\sum _{i=1} ^{n} \\mathbf{x}_i\\mathbf{x}_i^T\\) represents the Covariance Matrix, and \\(\\mathbf{C} \\in \\mathbb{R}^{d \\times d}\\).\nNotably, the eigenvector \\(\\mathbf{w}\\) corresponding to the largest eigenvalue \\(\\lambda\\) of \\(\\mathbf{C}\\) becomes the sought-after solution for the representation. This \\(\\mathbf{w}\\) is often referred to as the First Principal Component of the dataset.\n\n\nBased on the above concepts, we can outline the following algorithm for representation learning:\nGiven a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset: \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) with \\(||\\mathbf{w}|| = 1\\).\nUpdate the dataset with the representation: \\[\n\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}  \\hspace{1em} \\forall i\n\\]\nRepeat steps 2 and 3 until the residues become zero, resulting in \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nThe question arises: Is this the most effective approach, and how many \\(\\mathbf{w}\\) do we need to achieve optimal compression?",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "pages/PCA.html#potential-algorithm",
    "href": "pages/PCA.html#potential-algorithm",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Based on the above concepts, we can outline the following algorithm for representation learning:\nGiven a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset: \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) with \\(||\\mathbf{w}|| = 1\\).\nUpdate the dataset with the representation: \\[\n\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}  \\hspace{1em} \\forall i\n\\]\nRepeat steps 2 and 3 until the residues become zero, resulting in \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nThe question arises: Is this the most effective approach, and how many \\(\\mathbf{w}\\) do we need to achieve optimal compression?",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "pages/PCA.html#approximate-representation",
    "href": "pages/PCA.html#approximate-representation",
    "title": "Principal Component Analysis",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nThe question arises: If the data can be approximately represented by a lower-dimensional subspace, would it suffice to use only those \\(k\\) projections? Additionally, how much variance should be covered?\nLet us consider a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\). Let \\(\\mathbf{C}\\) represent its covariance matrix, and \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the corresponding eigenvalues, which are non-negative due to the positive semi-definiteness of the covariance matrix. These eigenvalues are arranged in descending order, with \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) as their corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be expressed as follows: \\[\\begin{align*}\n    \\mathbf{C}\\mathbf{w} &= \\lambda \\mathbf{w} \\\\\n    \\mathbf{w}^T\\mathbf{C}\\mathbf{w} &= \\mathbf{w}^T\\lambda \\mathbf{w}\\\\\n    \\therefore \\lambda &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w} \\hspace{2em} \\{\\mathbf{w}^T\\mathbf{w} = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n\\end{align*}\\]\nHence, the mean of the dataset being zero, \\(\\lambda\\) represents the variance captured by the eigenvector \\(\\mathbf{w}\\).\nA commonly accepted heuristic suggests that PCA should capture at least 95% of the variance. If the first \\(k\\) eigenvectors capture the desired variance, it can be stated as: \\[\n\\frac{\\displaystyle \\sum _{j=1} ^{k} \\lambda_j}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\]\nThus, the higher the variance captured, the lower the error incurred.",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "pages/PCA.html#p.c.a.-algorithm",
    "href": "pages/PCA.html#p.c.a.-algorithm",
    "title": "Principal Component Analysis",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nThe Principal Component Analysis algorithm can be summarized as follows for a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), and \\(\\mathbf{C}\\) represents its covariance matrix:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(\\mathbf{C}\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the eigenvalues arranged in descending order, and \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) be their corresponding eigenvectors of unit length.\nStep 2: Calculate \\(k\\), the number of top eigenvalues and eigenvectors required, based on the desired variance to be covered.\nStep 3: Project the data onto the eigenvectors and obtain the desired representation as a linear combination of these projections.\n\n\n\n\nThe dataset depicted in the diagram has two principal components: the green vector represents the first PC, whereas the red vector corresponds to the second PC.\n\n\nIn essence, PCA is a dimensionality reduction technique that identifies feature combinations that are de-correlated (independent of each other).",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "pages/intro.html",
    "href": "pages/intro.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "What is Machine Learning\nMachine learning (ML) is a field of computer science that gives computers the ability to learn without being explicitly programmed. ML algorithms are able to learn from data and make predictions or decisions without being explicitly told how to do so. This makes ML a powerful tool for automating tasks, making predictions, and uncovering patterns in data.\n\n\nWhy Machine Learning?\nThere are many reasons why machine learning is becoming increasingly popular. Some of the key benefits of ML include:\n\nAutomation: ML can be used to automate tasks that would otherwise require human intelligence. This can free up human workers to focus on more creative and strategic tasks.\nAccuracy: ML algorithms can often make more accurate predictions than humans. This is because ML algorithms can learn from large amounts of data and identify patterns that humans would not be able to see.\nScalability: ML algorithms can be scaled to handle large amounts of data. This makes them ideal for applications such as fraud detection, natural language processing, and image recognition.\n\n\n\nWhere is Machine Learning Used?\nMachine learning is used in a wide variety of fields, including:\n\nFinance: ML is used to predict stock prices, identify fraud, and manage risk.\nHealthcare: ML is used to diagnose diseases, develop new treatments, and personalize care.\nRetail: ML is used to personalize recommendations, predict demand, and prevent fraud.\nManufacturing: ML is used to optimize production, improve quality control, and predict maintenance needs.\nLogistics: ML is used to optimize shipping routes, manage inventory, and predict demand.\n\n\n\nTypes of Machine Learning\nThere are many different types of machine learning, each with its own strengths and weaknesses. Some of the most common types of machine learning include:\n\nSupervised learning: In supervised learning, the algorithm is trained on a dataset of labeled data. This means that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data.\nUnsupervised learning: In unsupervised learning, the algorithm is trained on a dataset of unlabeled data. This means that the data only includes inputs and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance.\nReinforcement learning: In reinforcement learning, the algorithm is trained by trial and error. The algorithm is given a goal and a set of actions that it can take. The algorithm then tries different actions and observes the results. Over time, the algorithm learns which actions are most likely to lead to the desired goal.\n\n\n\nThe Future of Machine Learning\nMachine learning is a rapidly growing field with the potential to revolutionize many industries. As ML algorithms become more powerful and sophisticated, they will be able to automate more tasks, make more accurate predictions, and uncover more patterns in data. This will lead to new and innovative applications in a wide variety of fields.",
    "crumbs": [
      "Introduction to MLA"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Machine Learning Algorithms",
    "section": "",
    "text": "Warning\n\n\n\nThe site is not complete and is still under development.\nThis website is a comprehensive resource for machine learning algorithms. Here, you will find explanations of the most popular algorithms, as well as code implementations in Python.\nThe website is divided into two main sections: supervised learning and unsupervised learning. Supervised learning algorithms are used to learn from labeled data, while unsupervised learning algorithms are used to learn from unlabeled data.\nWithin each section, the algorithms are further divided into regression, classification, and clustering. Regression algorithms are used to predict continuous values, while classification algorithms are used to predict discrete values. Clustering algorithms are used to group data points together.\nEach algorithm page includes the following information:\nI hope you find this website to be a valuable resource for learning about machine learning algorithms.\nHere is a more detailed overview of the algorithms covered on this website:",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Welcome to Machine Learning Algorithms",
    "section": "Credits",
    "text": "Credits\nI would like to express my gratitude to Professor Arun Rajkumar for his valuable content and notations, which have greatly influenced my understanding. Additionally, I would like to acknowledge the contribution of IIT Madras, where I had the opportunity to learn from Professor Arun Rajkumar in the Machine Learning Techniques Course.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there, I’m Sherry Thomas, and I’m passionate about making a difference in the field of data science. I have a Bachelor’s degree in Philosophy from Christ University, Bengaluru and I’m currently pursuing a Bachelor’s degree in Data Science and Applications at IIT-Madras.\nI have a diverse academic background that includes philosophy, programming, computer science, and statistics. My philosophy studies have helped me develop critical thinking and problem-solving skills, while my data science coursework has provided me with the technical expertise needed to tackle complex challenges.\nI believe in taking a holistic approach to data analysis and decision-making, considering the ethical and societal implications of technology and data. My goal is to use my knowledge and skills to make a positive impact in the field of data science.\nI’m constantly learning and growing, and I’m committed to expanding my knowledge base and contributing to the ever-evolving world of technology and data.\nVisit my Portfolio: https://sherrys997.github.io/",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "pages/svm.html",
    "href": "pages/svm.html",
    "title": "Support vector machines (SVMs)",
    "section": "",
    "text": "Let dataset \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be linearly separable with \\(\\gamma\\)-margin where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{-1, 1\\}\\).\nLet \\(w* \\in \\mathbb{R}^d\\) be the weight vector s.t. \\((w^{*T}x_i)y_i\\ge\\gamma\\) \\(\\forall i\\).\nLet some \\(R&gt;0 \\in \\mathbb{R}\\), s.t. \\(\\forall i\\) \\(||x_i||\\le R\\).\nTherefore, the number of mistakes made by the algorithm is given by, \\[\n\\text{\\#mistakes} \\le \\frac{R^2}{\\gamma^2}\n\\]\nObservations\nLet \\(w_{perc}\\) be any weight vector which can linearly separate the dataset.\nTherefore, we observe the following:\n\n“Quality” of the solution depends on the margin.\nNumber of mistakes depend on \\(w^*\\)’s margin.\n\\(w_{perc}\\) need not necessarily be \\(w^*\\).\n\nHence, our goal should be to find the solution that maximizes the margin.\n\n\nFrom the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\).",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Support vector machines (SVMs)"
    ]
  },
  {
    "objectID": "pages/svm.html#margin-maximization",
    "href": "pages/svm.html#margin-maximization",
    "title": "Support vector machines (SVMs)",
    "section": "",
    "text": "From the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\).",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Support vector machines (SVMs)"
    ]
  },
  {
    "objectID": "pages/svm.html#hard-margin-svm-algorithm",
    "href": "pages/svm.html#hard-margin-svm-algorithm",
    "title": "Support vector machines (SVMs)",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThis algorithm only works if the dataset is linearly separable with a \\(\\gamma &gt; 0\\).\n\nCalculate \\(Q=X^TX\\) directly or using a kernel as per the dataset.\nUse the gradient of the dual formula (\\(\\alpha^T1 -  \\frac{1}{2}\\alpha^TY^TQY\\alpha\\)), in the gradient descent algorithm to find a satisfactory \\(\\alpha\\). Let the intial \\(\\alpha\\) be a zero vector \\(\\in \\mathbb{R}^n_+\\).\nTo predict:\n\nFor non-kernelized SVM: \\(\\text{label}(x_{test}) = w^Tx_{test} = \\sum _{i=1} ^n \\alpha _i y_i(x_i^Tx_{test})\\)\nFor kernelized SVM: \\(\\text{label}(x_{test}) = w^T\\phi(x_{test}) = \\sum _{i=1} ^n \\alpha _i y_ik(x_i^Tx_{test})\\)",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Support vector machines (SVMs)"
    ]
  },
  {
    "objectID": "pages/svm.html#dual-formulation",
    "href": "pages/svm.html#dual-formulation",
    "title": "Support vector machines (SVMs)",
    "section": "Dual Formulation",
    "text": "Dual Formulation\nMaximizing the Lagrangian function w.r.t. \\(\\alpha\\) and \\(\\beta\\), and minimizing it w.r.t. \\(w\\) and \\(\\epsilon\\), we get,\n\\[\n\\min _{w, \\epsilon}\\left [\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\nThe dual of this is given by,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\mathcal{L}(w, \\epsilon, \\alpha, \\beta) \\right ] \\quad \\ldots[1]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(w\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get, \\[\n\\frac{d\\mathcal{L}}{dw}  = 0\n\\] \\[\n\\frac{d}{dw} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\\\\\n\\] \\[\nw_{\\alpha, \\beta}^* - \\alpha_ix_iy_i = 0\n\\] \\[\n\\therefore w_{\\alpha, \\beta}^* = \\alpha_ix_iy_i \\quad \\ldots [2]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(\\epsilon_i \\forall i\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial\\epsilon_i}  = 0\n\\] \\[\n\\frac{\\partial}{\\partial\\epsilon_i} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\n\\] \\[\nC - \\alpha_i -\\beta_i = 0\n\\] \\[\n\\therefore C = \\alpha_i + \\beta_i \\quad \\ldots [3]\n\\]\nSubstituting the values of \\(w\\) and \\(\\beta\\) from \\([2]\\) and \\([3]\\) in \\([1]\\), we get,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}||\\alpha_ix_iy_i||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-((\\alpha_ix_iy_i)^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n (C-\\alpha_i)(-\\epsilon_i) \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i-\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i - \\sum _{i=1} ^n \\alpha_i\\epsilon_i - C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i\\epsilon_i \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] \\[\n\\therefore \\max _{0 \\le \\alpha \\le C}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] If \\(C=0\\), \\(\\alpha^*=0\\) \\(\\rightarrow\\) \\(\\displaystyle w^*=\\sum _{i=1} ^n \\alpha^*_i x_i y_i = 0\\).\nif \\(C=\\infty\\), the equation will be equal to that of Hard-Margin SVM.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Support vector machines (SVMs)"
    ]
  },
  {
    "objectID": "pages/svm.html#complementary-slackness-in-soft-margin-support-vector-machines",
    "href": "pages/svm.html#complementary-slackness-in-soft-margin-support-vector-machines",
    "title": "Support vector machines (SVMs)",
    "section": "Complementary Slackness in Soft-Margin Support Vector Machines",
    "text": "Complementary Slackness in Soft-Margin Support Vector Machines\nComplementary slackness can be expressed through a set of equations as follows:\n\n\\(\\forall i\\), the following equation holds: \\[\n\\alpha_i(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i) = 0 \\quad \\ldots [1]\n\\]\nAdditionally, the following relation must hold: \\[\n\\beta_i(\\epsilon_i^*) = 0 \\quad \\ldots [2]\n\\]\n\nGiven these equations, various scenarios arise which elucidate the implications of complementary slackness in the context of Soft-Margin SVM.\n\nScenarios of Complementary Slackness\n\nIf \\(\\alpha_i^* = 0\\):\nIn this case, \\(\\beta_i^* = C\\) due to \\(\\alpha_i^* + \\beta_i^* = C\\). From equation [2], we deduce that \\(\\epsilon_i^* = 0\\). Consequently, it follows that the decision function \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i + \\epsilon_i^* \\ge 1\\) simplifies to \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i \\ge 1\\), indicating that the data point is correctly classified and lies outside the margin.\nIf \\(0 &lt; \\alpha_i^* &lt; C\\):\nIn this scenario, both \\(\\alpha_i^*\\) and \\(\\beta_i^*\\) assume positive values within the range \\((0, C)\\). Equation [2] implies \\(\\epsilon_i^* = 0\\), while equation [1] gives \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i = 0\\), leading to \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\). This signifies that the data point resides exactly on the margin boundary.\nIf \\(\\alpha_i^* = C\\):\nWhen \\(\\alpha_i^*\\) takes on the maximum value \\(C\\), the dual variable \\(\\beta_i^*\\) becomes \\(0\\). Equation [2] establishes that \\(\\epsilon_i^*\\ge0\\). Equation [1] implies \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i = 0\\), which further simplifies to \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i = \\epsilon^*_i\\). This results in \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i \\ge 0\\), indicating that the data point is misclassified or lies within the margin.\nIf \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &lt; 1\\):\nGiven that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &lt; 1\\), the inequality \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i + \\epsilon_i^* \\ge 1\\) holds. By manipulating this inequality, we deduce \\(\\epsilon_i^* &gt; 0\\). From equation [2], it follows that \\(\\beta_i^* = 0\\), and consequently, \\(\\alpha_i^* = C\\).\nIf \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\):\nFor data points that satisfy \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\), we once again consider the inequality \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i + \\epsilon_i^* \\ge 1\\), leading to \\(\\epsilon_i^* \\ge 0\\). If \\(\\epsilon_i^* &gt; 0\\), equation [2] indicates \\(\\beta_i^* = 0\\), resulting in \\(\\alpha_i^* = C\\). On the other hand, if \\(\\epsilon_i^* = 0\\), equation [2] implies \\(\\beta_i^* \\ge 0\\), and as a consequence, \\(\\alpha_i^* \\in [0, C]\\).\nIf \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &gt; 1\\):\nWhen \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &gt; 1\\), the inequality \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i &lt; 0\\) holds. From equation [1], we conclude that \\(\\alpha_i^* = 0\\), leading to \\(\\beta^*_i = C\\) as per the equation \\(\\alpha_i^* + \\beta_i^* = C\\). Furthermore, equation [2] reveals that \\(\\epsilon_i^* = 0\\).\n\n\n\nSummary of Scenarios\nIn summary, we have established the following relationships between the dual variable \\(\\alpha_i^*\\) and the decision boundary defined by \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\):\n\n\\(\\alpha_i^* = 0\\): Data point is correctly classified and outside the margin.\n\\(0 &lt; \\alpha_i^* &lt; C\\): Data point lies exactly on the margin boundary.\n\\(\\alpha_i^* = C\\): Data point is misclassified or inside the margin.\n\\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &lt; 1\\): Dual variable \\(\\alpha_i^* = C\\).\n\\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\): Dual variable \\(\\alpha_i^* \\in [0, C]\\) if \\(\\epsilon_i^* = 0\\), else \\(\\alpha_i^* = C\\).\n\\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &gt; 1\\): Dual variable \\(\\alpha_i^* = 0\\).",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Support vector machines (SVMs)"
    ]
  },
  {
    "objectID": "pages/Kernel_PCA.html",
    "href": "pages/Kernel_PCA.html",
    "title": "Kernel PCA",
    "section": "",
    "text": "In the context of a given dataset \\(\\mathbf{X} \\in \\mathbb{R}^{d \\times n}\\), where \\(\\mathbf{C} \\in \\mathbb{R}^{d \\times d}\\) represents the covariance matrix, Principal Component Analysis (PCA) encounters two critical challenges:\n\nTime Complexity: Computing the eigenvalues and eigenvectors of \\(\\mathbf{C}\\) requires an algorithmic complexity of \\(O(d^3)\\). Consequently, as the dimensionality \\(d\\) increases, the computational time becomes prohibitively large.\nNon-Linear Dataset: In situations where the dataset resides in a non-linear subspace, PCA’s attempt to derive linear combinations of Principal Components may yield suboptimal results.\n\nTo address these issues, we propose methods for reducing the time complexity of finding eigenvalues and eigenvectors and for handling non-linear relationships in PCA.",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Kernel PCA"
    ]
  },
  {
    "objectID": "pages/Kernel_PCA.html#transforming-features",
    "href": "pages/Kernel_PCA.html#transforming-features",
    "title": "Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nTo address non-linear relationships, we propose mapping the dataset to higher dimensions as follows: \\[\n\\mathbf{x} \\to \\phi(\\mathbf{x}) \\quad \\mathbb{R}^d \\to \\mathbb{R}^D \\quad \\text{where } [D &gt;&gt; d]\n\\]\nTo compute \\(D\\), let \\(\\mathbf{x}=\\left [\n\\begin{array} {cc}\n    f_1 & f_2\n\\end{array}\n\\right ]\\) represent features of a dataset containing datapoints lying on a second-degree curve in a two-dimensional space.\nTo convert it from quadratic to linear, we map the features to: \\[\n\\phi(\\mathbf{x})=\\left [\n\\begin{array} {cccccc}\n    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2\n\\end{array}\n\\right ]\n\\]\nMapping \\(d\\) features to the polynomial power \\(p\\) results in \\(^{d+p} C_d\\) new features.\nHowever, it is essential to note that finding \\(\\phi(\\mathbf{x})\\) may be computationally demanding.\nTo overcome this issue, we present the solution in the subsequent section.",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Kernel PCA"
    ]
  },
  {
    "objectID": "pages/Kernel_PCA.html#kernel-functions",
    "href": "pages/Kernel_PCA.html#kernel-functions",
    "title": "Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is considered a “valid” Kernel Function if it maps data points to the real numbers.\nProof of a “Valid” Kernel: There are two methods to establish the validity of a kernel:\n\nMethod 1: Explicitly exhibit the mapping to \\(\\phi\\), which may be challenging in certain cases.\nMethod 2: Utilize Mercer’s Theorem, which states that \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a valid kernel if and only if:\n\n\\(k\\) is symmetric, i.e., \\(k(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x}',\\mathbf{x})\\)\nFor any dataset \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\), the matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), where \\(\\mathbf{K}_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)\\), is Positive Semi-Definite.\n\n\nTwo popular kernel functions are:\n\nPolynomial Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x} + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = \\exp\\left(\\displaystyle-\\frac{\\|\\mathbf{x}-\\mathbf{x}'\\|^2}{2\\sigma^2}\\right)\\)",
    "crumbs": [
      "Unsupervised Learning",
      "Representation learning",
      "Kernel PCA"
    ]
  },
  {
    "objectID": "pages/kkmeans.html",
    "href": "pages/kkmeans.html",
    "title": "Kernel K-means",
    "section": "",
    "text": "Smart Initialization - K-means++\nThe concept of K-means++ involves selecting centroids that are maximally distant from each other.\n\nStep 1: Randomly select \\(\\boldsymbol{\\mu} _1 ^0\\) from the dataset.\nStep 2: For \\(l \\in \\{2, 3, \\ldots, k\\}\\), choose \\(\\boldsymbol{\\mu} _l ^0\\) probabilistically proportional to the score(\\(S\\)), where \\(S\\) is defined as follows: \\[\n  S(\\mathbf{x}_i) = \\min _{\\{j=1, 2, \\ldots, l-1\\}} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{j} ^0 ||}^2 \\quad \\forall \\mathbf{x}_i \\in \\mathbf{X}\n\\] The probabilistic aspect of the algorithm provides an expected guarantee of optimal convergence in K-means. The guarantee is given by: \\[\n  \\mathbb{E} \\left[ \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||}^2 \\right ]\n  \\le O(\\log k) \\left [ \\min _{\\{z_1, z_2, \\ldots, z_n\\}} \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||}^2 \\right ]\n\\] where \\(O(\\log k)\\) is a constant of order \\(\\log k\\).\nStep 3: Once the centroids are determined, we proceed with our usual Lloyd’s Algorithm.\n\n\n\nChoice of K\nA prerequisite for K-means is determining the number of clusters, denoted as \\(k\\). However, what if the value of \\(k\\) is unknown?\nIf we were to choose \\(k\\) to be equal to \\(n\\): \\[\nF(z_1, z_2, \\dots, z_n) = \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||} ^2 = 0\n\\] However, as having as many clusters as datapoints is undesirable, we aim to minimize \\(k\\) while penalizing large values of \\(k\\). \\[\n\\underset{k}{\\arg \\min} \\left [ \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||} ^2 + \\text{Penalty}(k) \\right ]\n\\] Two common criteria for making the above argument are:\n\n\\(\\text{\\textcolor{blue}{Akaike Information Criterion}}\\): \\(\\left [ 2K - 2\\ln(\\hat{\\mathcal{L}}(\\theta ^*)) \\right ]\\)\n\\(\\text{\\textcolor{blue}{Bayesian Information Criterion}}\\): \\(\\left [ K\\ln(n) - 2\\ln(\\hat{\\mathcal{L}}(\\theta ^*)) \\right ]\\)\n\nHowever, detailed elaboration of these criteria is not discussed here.",
    "crumbs": [
      "Unsupervised Learning",
      "Clustering",
      "Kernel K-means clustering"
    ]
  },
  {
    "objectID": "pages/bayes_est.html",
    "href": "pages/bayes_est.html",
    "title": "Bayesian estimation",
    "section": "",
    "text": "Bayesian estimation is a statistical method that updates parameter estimates by incorporating prior knowledge or beliefs along with observed data to calculate the posterior probability distribution of the parameters.\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i\\) follows a distribution with parameters \\(\\boldsymbol{\\theta}\\). We assume that the data points are independent and identically distributed, and we also consider \\(\\boldsymbol{\\theta}\\) as a random variable with its own probability distribution.\nOur objective is to update the parameters using the available data.\ni.e. \\[\nP(\\boldsymbol{\\theta})\\Rightarrow P(\\boldsymbol{\\theta}|\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})\n\\] where, employing Bayes’ Law, we find \\[\nP(\\boldsymbol{\\theta}|\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})=\\left ( \\frac{P(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}|\\boldsymbol{\\theta})}{P(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})} \\right )*P(\\boldsymbol{\\theta})\n\\]",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Bayesian estimation"
    ]
  },
  {
    "objectID": "pages/bayes_est.html#bayesian-estimation-for-a-bernoulli-distribution",
    "href": "pages/bayes_est.html#bayesian-estimation-for-a-bernoulli-distribution",
    "title": "Bayesian estimation",
    "section": "Bayesian Estimation for a Bernoulli Distribution",
    "text": "Bayesian Estimation for a Bernoulli Distribution\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\) with parameter \\(\\theta\\). What distribution can be suitable for \\(P(\\theta)\\)?\nA commonly used distribution for priors is the Beta Distribution. \\[\nf(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{z} \\hspace{2em} \\forall p \\in [0,1] \\\\\n\\] \\[\n\\text{where $z$ is a normalizing factor}\n\\]\nHence, utilizing the Beta Distribution as the Prior, we obtain, \\[\\begin{align*}\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\}) &\\propto P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})*P(\\theta) \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto \\left [ \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ]*\\left [ p^{\\alpha-1}(1-p)^{\\beta-1} \\right ] \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto p^{\\sum _{i=1} ^n x_i + \\alpha - 1}(1-p)^{\\sum _{i=1} ^n(1-x_i) + \\beta - 1}\n\\end{align*}\\] i.e. we obtain, \\[\n\\text{BETA PRIOR }(\\alpha, \\beta) \\xrightarrow[Bernoulli]{\\{x_1, x_2, \\ldots, x_n\\}} \\text{BETA POSTERIOR }(\\alpha + n_h, \\beta + n_t)\n\\] \\[\n\\therefore \\hat{p_{\\text{ML}}} = \\mathbb{E}[\\text{Posterior}]=\\mathbb{E}[\\text{Beta}(\\alpha +n_h, \\beta + n_t)]= \\frac{\\alpha + n_h}{\\alpha + n_h + \\beta + n_t}\n\\]",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Bayesian estimation"
    ]
  },
  {
    "objectID": "pages/em.html",
    "href": "pages/em.html",
    "title": "Expectation-maximization (EM) algorithm",
    "section": "",
    "text": "Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality.",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Expectation-maximization (EM) algorithm"
    ]
  },
  {
    "objectID": "pages/em.html#convexity-and-jensens-inequality",
    "href": "pages/em.html#convexity-and-jensens-inequality",
    "title": "Expectation-maximization (EM) algorithm",
    "section": "",
    "text": "Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality.",
    "crumbs": [
      "Unsupervised Learning",
      "Estimation",
      "Expectation-maximization (EM) algorithm"
    ]
  },
  {
    "objectID": "pages/klin_reg.html",
    "href": "pages/klin_reg.html",
    "title": "Kernel Least squares regression",
    "section": "",
    "text": "What if the data points reside in a non-linear subspace? Similar to dealing with non-linear data clustering, kernel functions are employed in this scenario as well.\nLet \\(\\mathbf{w}^* = \\mathbf{X}\\boldsymbol{\\alpha}^*\\), where \\(\\boldsymbol{\\alpha}^* \\in \\mathbb{R}^n\\). \\[\\begin{align*}\n\\mathbf{X}\\boldsymbol{\\alpha}^* &= \\mathbf{w}^* \\\\\n\\therefore \\mathbf{X}\\boldsymbol{\\alpha}^* &= (\\mathbf{X}\\mathbf{X}^T)^+\\mathbf{X}\\mathbf{y} \\\\\n(\\mathbf{X}\\mathbf{X}^T)\\mathbf{X}\\boldsymbol{\\alpha}^* &= (\\mathbf{X}\\mathbf{X}^T)(\\mathbf{X}\\mathbf{X}^T)^+\\mathbf{X}\\mathbf{y} \\\\\n(\\mathbf{X}\\mathbf{X}^T)\\mathbf{X}\\boldsymbol{\\alpha}^* &= \\mathbf{X}\\mathbf{y} \\\\\n\\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)\\mathbf{X}\\boldsymbol{\\alpha}^* &= \\mathbf{X}^T\\mathbf{X}\\mathbf{y} \\\\\n(\\mathbf{X}^T\\mathbf{X})^2\\boldsymbol{\\alpha}^* &= \\mathbf{X}^T\\mathbf{X}\\mathbf{y} \\\\\n\\mathbf{K}^2\\boldsymbol{\\alpha}^* &= \\mathbf{K}\\mathbf{y} \\\\\n\\therefore \\boldsymbol{\\alpha}^* &= \\mathbf{K}^{-1}\\mathbf{y}\n\\end{align*}\\]\nHere, \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), and it can be obtained using a kernel function such as the Polynomial Kernel or RBF Kernel.\nTo predict using \\(\\boldsymbol{\\alpha}\\) and the kernel function, let \\(\\mathbf{X}_{\\text{test}} \\in \\mathbb{R}^{d \\times m}\\) represent the test dataset. The prediction is made as follows:\n\\[\\begin{align*}\n\\mathbf{w}^*\\phi(\\mathbf{X}_{\\text{test}}) &= \\sum_{i=1}^n \\alpha_i^* k(\\mathbf{x}_i, \\mathbf{x}_{\\text{test}_i})\n\\end{align*}\\]\nHere, \\(\\alpha_i^*\\) denotes the importance of the \\(i\\)-th data point in relation to \\(\\mathbf{w}^*\\), and \\(k(\\mathbf{x}_i, \\mathbf{x}_{\\text{test}_i})\\) signifies the similarity between \\(\\mathbf{x}_{\\text{test}_i}\\) and \\(\\mathbf{x}_i\\).",
    "crumbs": [
      "Supervised Learning",
      "Regression",
      "Kernel Least squares regression"
    ]
  },
  {
    "objectID": "pages/rid_lin_reg.html",
    "href": "pages/rid_lin_reg.html",
    "title": "Ridge regression",
    "section": "",
    "text": "Ridge regression is a linear regression technique that addresses multicollinearity and overfitting by adding a penalty term to the ordinary least squares method.\nThe objective function of ridge regression is given by:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum^n_{i=1}(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 + \\lambda||\\mathbf{w}||_2^2\n\\]\nHere, \\(\\lambda||\\mathbf{w}||_2^2\\) serves as the regularization term, and \\(||\\mathbf{w}||_2^2\\) represents the squared L2 norm of \\(\\mathbf{w}\\). Let us denote this equation as \\(f(\\mathbf{w})\\).\nAlternatively, we can express ridge regression as:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum^n_{i=1}(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 \\hspace{1em}\\text{s.t.}\\hspace{1em}||\\mathbf{w}||_2^2\\leq\\theta\n\\]\nHere, the value of \\(\\theta\\) depends on \\(\\lambda\\). We can conclude that for any choice of \\(\\lambda&gt;0\\), there exists a corresponding \\(\\theta\\) such that optimal solutions to our objective function exist.\nThe loss function for linear regression with the maximum likelihood estimator \\(\\mathbf{w}_{\\text{ML}}\\) is defined as:\n\\[\nf(\\mathbf{w}_{\\text{ML}}) = \\sum^n_{i=1}(\\mathbf{w}_{\\text{ML}}^T\\mathbf{x}_i-y_i)^2\n\\]\nConsider the set of all \\(\\mathbf{w}\\) such that \\(f(\\mathbf{w}_{\\text{ML}}) = f(\\mathbf{w}) + c\\), where \\(c&gt;0\\). This set can be represented as:\n\\[\nS_c = \\left \\{\\mathbf{w}: f(\\mathbf{w}_{\\text{ML}}) = f(\\mathbf{w}) + c \\right \\}\n\\]\nIn other words, every \\(\\mathbf{w} \\in S_c\\) satisfies the equation:\n\\[\n||\\mathbf{X}^T\\mathbf{w}-\\mathbf{y}||^2 = ||\\mathbf{X}^T\\mathbf{w}_{\\text{ML}}-\\mathbf{y}||^2 + c\n\\]\nSimplifying the equation yields:\n\\[\n(\\mathbf{w}-\\mathbf{w}_{\\text{ML}})^T(\\mathbf{X}\\mathbf{X}^T)(\\mathbf{w}-\\mathbf{w}_{\\text{ML}}) = c'\n\\]\nThe value of \\(c'\\) depends on \\(c\\), \\(\\mathbf{X}\\mathbf{X}^T\\), and \\(\\mathbf{w}_{\\text{ML}}\\), but it does not depend on \\(\\mathbf{w}\\).\n\n\n\nPictoral Representation of what Ridge Regression does.\n\n\nIn summary, ridge regression regularizes the feature values, pushing them towards zero, but not necessarily to zero.",
    "crumbs": [
      "Supervised Learning",
      "Regression",
      "Ridge regression"
    ]
  },
  {
    "objectID": "pages/knn.html",
    "href": "pages/knn.html",
    "title": "K-nearest neighbors (KNN)",
    "section": "",
    "text": "Binary classification is a fundamental task in machine learning, commonly employed in various domains such as computer vision, natural language processing, and bioinformatics. Its objective is to assign objects into one of two categories based on their features.\nConsider a dataset \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\), where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and let \\(\\{y_1, \\ldots, y_n\\}\\) be the corresponding labels, where \\(y_i \\in \\{0, 1\\}\\). The goal is to find a function \\(h: \\mathbb{R}^d \\rightarrow \\{0, 1\\}\\) that accurately predicts the labels.\nTo assess the performance of the classification function, a loss measure is employed. The loss function is defined as follows:\n\\[\n\\text{loss}(h) = \\frac{1}{n} \\sum ^n _{i=1}\\mathbb{1}\\left ( h(\\mathbf{x}_i) \\ne y_i \\right )\n\\]\nLet \\(\\mathcal{H}_{\\text{linear}}\\) denote the solution space for the linear mapping:\n\\[\n\\mathcal{H}_{\\text{linear}}=\\left\\{\\mathbf{h}_w: \\mathbb{R}^d \\rightarrow \\{1, 0\\} \\hspace{0.5em} \\text{s.t.} \\hspace{0.5em} \\mathbf{h}_w(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^T\\mathbf{x}) \\hspace{0.5em} \\forall \\mathbf{w} \\in \\mathbb{R}^d \\right\\}\n\\]\nHence, the objective function can be expressed as:\n\\[\n\\min_{h \\in \\mathcal{H}_{\\text{linear}}} \\sum_{i=1}^n \\mathbb{1}\\left ( h(\\mathbf{x}_i) \\ne y_i \\right )\n\\]\nHowever, it is important to note that this objective function presents an NP-Hard problem, making it challenging to find optimal and sufficient parameters. Therefore, improved implementations are required to address this complexity and achieve satisfactory results.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "K-nearest neighbors (KNN)"
    ]
  },
  {
    "objectID": "pages/knn.html#issues-with-k-nn",
    "href": "pages/knn.html#issues-with-k-nn",
    "title": "K-nearest neighbors (KNN)",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nThe K-NN algorithm suffers from several limitations:\n\nThe choice of distance function can yield different results. The Euclidean distance, commonly used, might not always be the best fit for all scenarios.\nComputationally, the algorithm can be demanding. When making predictions for a single test data point, the distances between that data point and all training points must be calculated and sorted. Consequently, the algorithm has a complexity of \\(O(n \\log(n))\\), where \\(n\\) represents the size of the dataset.\nThe algorithm does not learn a model but instead relies on the training dataset for making predictions.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "K-nearest neighbors (KNN)"
    ]
  },
  {
    "objectID": "pages/naive_bayes.html",
    "href": "pages/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "A Generative Model Based Algorithm is an approach that seeks to model the probability distribution of the input data and generate new samples based on this distribution. The key idea involves learning the joint probability distribution of the features and labels in the training data and utilizing this learned model to make predictions for previously unseen data.\nLet us consider a dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), where \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features using the labels obtained in Step 1 through the conditional probability \\(P(\\mathbf{x}_i|y_i)\\).\n\nThe parameters in the generative model are defined as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(2^d-1\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(2^d-1\\)\n\nConsequently, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + (2^d-1) + (2^d-1) \\\\\n    & = 1 + 2(2^d-1) \\\\\n    & = 2^{d+1}-1\n\\end{align*}\\]\nIssues:\n\nToo many parameters, which may lead to overfitting.\nThe model may not be practically viable due to the assumption made in the generative process.\n\n\n\nAn alternative generative model starts with the class conditional independence assumption, which is a common assumption in various machine learning algorithms. This assumption states that the features of an object are conditionally independent given its class label.\nLet us again consider the dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), with \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm under this alternative model are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features for \\(\\mathbf{x}\\) given \\(y\\) using the following conditional probability: \\[\nP(\\mathbf{x} = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^{y_i}_i)^{f_i}(1-p^{y_i}_i)^{1-f_i}\n\\]\n\nThe parameters in this alternative model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(d\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(d\\)\n\nThus, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + d + d \\\\\n    & = 2d + 1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Generative models",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "pages/naive_bayes.html#alternate-generative-model",
    "href": "pages/naive_bayes.html#alternate-generative-model",
    "title": "Naive Bayes",
    "section": "",
    "text": "An alternative generative model starts with the class conditional independence assumption, which is a common assumption in various machine learning algorithms. This assumption states that the features of an object are conditionally independent given its class label.\nLet us again consider the dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), with \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm under this alternative model are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features for \\(\\mathbf{x}\\) given \\(y\\) using the following conditional probability: \\[\nP(\\mathbf{x} = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^{y_i}_i)^{f_i}(1-p^{y_i}_i)^{1-f_i}\n\\]\n\nThe parameters in this alternative model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(d\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(d\\)\n\nThus, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + d + d \\\\\n    & = 2d + 1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Generative models",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "pages/naive_bayes.html#prediction-using-the-parameters",
    "href": "pages/naive_bayes.html#prediction-using-the-parameters",
    "title": "Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(\\mathbf{x}^{test}\\in\\{0,1\\}^d\\), the prediction for \\(\\hat{y}^{test}\\) is done using the following criterion:\n\\[\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) \\ge P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\n\\]\nIf the above inequality holds, then \\(\\hat{y}^{test}=1\\), otherwise \\(\\hat{y}^{test}=0\\).\nUsing Bayes’ rule, we can express \\(P(\\hat{y}^{test}=1|\\mathbf{x}^{test})\\) and \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\\) as follows:\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(\\mathbf{x}^{test})} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(\\mathbf{x}^{test})}\n\\end{align*}\\]\nHowever, since we are only interested in the comparison of these probabilities, we can avoid calculating \\(P(\\mathbf{x}^{test})\\).\nBy solving for \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we find:\n\\[\\begin{align*}\n&=P(\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1) \\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\]\nSimilarly, we can obtain \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, we predict \\(\\hat{y}^{test}=1\\) if:\n\\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\]\nOtherwise, we predict \\(\\hat{y}^{test}=0\\).\nThe Naive Bayes algorithm employs two main techniques:\n\nThe Class Conditional Independence Assumption.\nUtilizing Bayes’ Rule.\n\nAs a result, this algorithm is commonly referred to as Naive Bayes.\nIn summary, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It estimates the conditional probabilities of features given the class and uses these probabilities to make predictions for new data. Despite its naive assumption, Naive Bayes has demonstrated good performance across various applications, particularly when dealing with high-dimensional data and limited training examples.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Generative models",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "pages/naive_bayes.html#pitfalls-of-naive-bayes",
    "href": "pages/naive_bayes.html#pitfalls-of-naive-bayes",
    "title": "Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nOne prominent issue with Naive Bayes is that if a feature is not observed in the training set but present in the testing set, the prediction probabilities for both classes become zero.\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\]\nIf any feature \\(f_i\\) was absent in the training set, it results in \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), leading to \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})=P(\\hat{y}^{test}=1|\\mathbf{x}^{test})=0\\).\nA popular remedy for this issue is to introduce two “pseudo” data points with labels 1 and 0, respectively, into the dataset, where all their features are set to 1. This technique is also known as Laplace smoothing.\nIn brief, Laplace smoothing is a technique employed to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By incorporating this smoothing term, the model becomes more robust and better suited for handling unseen data.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Generative models",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "pages/naive_bayes.html#prediction-using-bayes-rule",
    "href": "pages/naive_bayes.html#prediction-using-bayes-rule",
    "title": "Naive Bayes",
    "section": "Prediction using Bayes’ Rule",
    "text": "Prediction using Bayes’ Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|\\mathbf{x}_{test})\\propto P(\\mathbf{x}_{test}|y_{test})*P(y_{test})\n\\]\nWhere \\(P(\\mathbf{x}_{test}|y_{test})\\equiv f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_{y_{test}}, \\hat{\\boldsymbol{\\Sigma}})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nTo predict \\(y_{test}=1\\), we compare the probabilities:\n\\[\\begin{align*}\nf(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{1} ,\\hat{\\boldsymbol{\\Sigma} }_{1} )\\hat{p} & \\geq f(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{0} ,\\hat{\\boldsymbol{\\Sigma} }_{0} )(1-\\hat{p} )\\\\\ne^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )}\\hat{p} & \\geq e^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )} (1-\\hat{p} )\\\\\n-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )+\\log (\\hat{p} ) & \\geq -(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )+\\log (1-\\hat{p} )\n\\end{align*}\\]\nThis inequality can be expressed as a linear decision function:\n\\[\n\\left( (\\hat{\\boldsymbol{\\mu}}_1-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}^{-1} \\right)\\mathbf{x}_{test} + \\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_0 - \\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_1 + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\]\nThus, the decision function of Gaussian Naive Bayes is linear.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Generative models",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "pages/naive_bayes.html#decision-boundaries-for-different-covariances",
    "href": "pages/naive_bayes.html#decision-boundaries-for-different-covariances",
    "title": "Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As previously discussed, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is both linear and the perpendicular bisector of the line drawn from \\(\\hat{\\boldsymbol{\\mu}}_1\\) to \\(\\hat{\\boldsymbol{\\mu}}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\boldsymbol{\\Sigma}}_1\\) and \\(\\hat{\\boldsymbol{\\Sigma}}_0\\) be the covariance matrices for classes 1 and 0, respectively. They are given by, \\[\\begin{align*}\n\\hat{\\boldsymbol{\\Sigma}}_1 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=1)} \\\\\n\\hat{\\boldsymbol{\\Sigma}}_0 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=0)}\n\\end{align*}\\] To predict \\(y_{test}=1\\), we compare the probabilities: \\[\\begin{align*}\nf(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}}_1)*\\hat{p}&\\ge f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\Sigma}}_0)*(1-\\hat{p}) \\\\\ne^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)}*\\hat{p}&\\ge e^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)}*(1-\\hat{p}) \\\\\n-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)+\\log(\\hat{p})&\\ge -(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_0(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] This inequality leads to a quadratic decision function: \\[\n\\mathbf{x}_{test}^T(\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}-2(\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}+(\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1}\\hat{\\boldsymbol{\\mu}}_0-\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}\\hat{\\boldsymbol{\\mu}}_1) + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Generative models",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "pages/log_reg.html",
    "href": "pages/log_reg.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Until now, we have utilized the \\(\\text{sign}\\) function to determine the class for the output. However, what if we also wish to obtain the probabilities associated with these outputs?\nLet \\(z=\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\), where \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? To address this, we introduce the Sigmoid Function, defined as follows:\n\\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is commonly employed in machine learning as an activation function for neural networks. It exhibits an S-shaped curve, making it well-suited for modeling processes with a threshold or saturation point, such as logistic growth or binary classification problems.\nFor large positive input values, the sigmoid function approaches 1, while for large negative input values, it approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” is derived from the Greek word “sigmoides,” meaning “shaped like the letter sigma” (\\(\\Sigma\\)). The sigmoid function’s characteristic S-shaped curve resembles the shape of the letter sigma, which likely influenced the function’s name.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "pages/log_reg.html#sigmoid-function",
    "href": "pages/log_reg.html#sigmoid-function",
    "title": "Logistic regression",
    "section": "",
    "text": "Until now, we have utilized the \\(\\text{sign}\\) function to determine the class for the output. However, what if we also wish to obtain the probabilities associated with these outputs?\nLet \\(z=\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\), where \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? To address this, we introduce the Sigmoid Function, defined as follows:\n\\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is commonly employed in machine learning as an activation function for neural networks. It exhibits an S-shaped curve, making it well-suited for modeling processes with a threshold or saturation point, such as logistic growth or binary classification problems.\nFor large positive input values, the sigmoid function approaches 1, while for large negative input values, it approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” is derived from the Greek word “sigmoides,” meaning “shaped like the letter sigma” (\\(\\Sigma\\)). The sigmoid function’s characteristic S-shaped curve resembles the shape of the letter sigma, which likely influenced the function’s name.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "pages/log_reg.html#logistic-regression",
    "href": "pages/log_reg.html#logistic-regression",
    "title": "Logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables. The independent variables can be either continuous or categorical. The main objective of logistic regression is to estimate the probability that the dependent variable belongs to one of the two possible values, given the independent variable values.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic (sigmoid) function. This function generates an S-shaped curve ranging between 0 and 1. By transforming the output of a linear combination of the independent variables using the logistic function, logistic regression provides a probability estimate that can be used for classifying new observations.\nLet \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) denote the dataset, where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know that:\n\\[\nP(y=1|\\mathbf{x}) = g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) = \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}}}\n\\]\nUsing the maximum likelihood approach, we can derive the following expression:\n\\[\\begin{align*}\n\\mathcal{L}(\\mathbf{w};\\text{Data}) &= \\prod _{i=1} ^{n} (g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{y_i}(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))+(1-y_i)\\log(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\end{align*}\\]\nTherefore, our objective, which involves maximizing the log-likelihood function, can be formulated as follows:\n\\[\n\\max _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\]\nHowever, a closed-form solution for this problem does not exist. Therefore, we resort to using gradient descent for convergence.\nThe gradient of the log-likelihood function is computed as follows:\n\\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{x}_i) - \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) (-\\mathbf{x}_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -\\mathbf{x}_i + \\mathbf{x}_iy_i + \\mathbf{x}_i \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_iy_i - \\mathbf{x}_i \\left( \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ]\n\\end{align*}\\]\nUtilizing the gradient descent update rule, we obtain:\n\\[\\begin{align*}\n\\mathbf{w}_{t+1} &= \\mathbf{w}_t + \\eta_t\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) \\\\\n&= \\mathbf{w}_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right )\n\\end{align*}\\]",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "pages/log_reg.html#kernel-and-regularized-versions",
    "href": "pages/log_reg.html#kernel-and-regularized-versions",
    "title": "Logistic regression",
    "section": "Kernel and Regularized Versions",
    "text": "Kernel and Regularized Versions\nIt is possible to argue that \\(\\mathbf{w}^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_i\\mathbf{x}_i\\), thereby allowing for kernelization. For additional information, please refer to this link.\nThe regularized version of logistic regression can be expressed as follows:\n\\[\n\\min _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) + \\mathbf{w}^\\mathbf{T}\\mathbf{x}_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||\\mathbf{w}||^2\n\\]\nHere, \\(\\frac{\\lambda}{2}||\\mathbf{w}||^2\\) serves as the regularizer, and \\(\\lambda\\) is determined through cross-validation.",
    "crumbs": [
      "Supervised Learning",
      "Classification",
      "Discriminative models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "pages/ffn.html",
    "href": "pages/ffn.html",
    "title": "Forward Propagation",
    "section": "",
    "text": "The forward pass in a neural network involves processing input data through the network’s layers to generate predictions or outputs by sequentially applying weights, biases, and activation functions.\nMathematical Representation:\nFor one training example \\(x^{(i)}\\):\n\nHidden Layer Computations:\n\nWeighted Sum Calculation:\n\n\\(z^{[l] (i)} =  W^{[l]} a^{[l-1](i)} + b^{[l]}\\)\n\nCompute the weighted sum of inputs (\\(a^{[l-1](i)}\\)) by the weights (\\(W^{[l]}\\)) and add the bias (\\(b^{[l]}\\)) for layer \\(l\\).\n\n\nActivation Calculation:\n\n\\(a^{[l] (i)} = g(z^{[l] (i)})\\)\n\nApply an activation function \\(g\\) (such as sigmoid, tanh, ReLU, etc.) to the computed weighted sum to get the activation of layer \\(l\\).\n\n\n\nOutput Layer Computations:\n\nWeighted Sum Calculation:\n\n\\(z^{[L] (i)} = W^{[L]} a^{[L-1] (i)} + b^{[L]}\\)\n\nCompute the weighted sum of inputs (\\(a^{[L-1] (i)}\\)) by the weights (\\(W^{[L]}\\)) and add the bias (\\(b^{[L]}\\)) for the output layer.\n\n\nActivation Calculation:\n\n\\(\\hat{y}^{(i)} = a^{[L] (i)} = \\sigma(z^{ [L] (i)})\\)\n\nApply an appropriate activation function (e.g., softmax for multiclass classification, sigmoid for binary classification) to the computed weighted sum to obtain the predicted output (\\(\\hat{y}^{(i)}\\)).\n\n\n\nCost Function:\n\n\\(J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\left(y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right)\\right)\\)\n\nCompute the appropriate cost function (e.g., cross-entropy, mean squared error) to evaluate the difference between the predicted output (\\(a^{[L] (i)}\\)) and the actual output (\\(y^{(i)}\\)) for all training examples (\\(m\\)).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nimport numpy as np\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n    \n    Z1 = W1@X + b1\n    A1 = np.tanh(Z1)\n    Z2 = W2@A1 + b2\n    A2 = 1/(1+np.exp(-Z2))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache",
    "crumbs": [
      "Deep Learning",
      "Neural Networks",
      "Forward Propagation"
    ]
  },
  {
    "objectID": "pages/gradient.html",
    "href": "pages/gradient.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Optimizing Model Parameters: Gradient Descent is used to update the parameters (weights and biases) of a neural network in the direction that minimizes the loss function.\nGeneral Gradient Descent Rule: The general update rule for a parameter \\(\\theta\\) using Gradient Descent is: \\[\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\] where \\(\\alpha\\) is the learning rate and \\(\\frac{\\partial J }{ \\partial \\theta }\\) denotes the partial derivative of the cost function \\(J\\) with respect to the parameter \\(\\theta\\).\nParameter Update for Weights and Biases:\n\nOutput Layer:\n\nUpdate the weights and biases of the output layer using the gradients calculated during backpropagation:\n\n\\(W^{[L]} = W^{[L]} - \\alpha \\frac{\\partial J }{ \\partial W^{[L]} }\\)\n\\(b^{[L]} = b^{[L]} - \\alpha \\frac{\\partial J }{ \\partial b^{[L]} }\\)\n\n\nHidden Layers:\n\nUpdate the weights and biases of the hidden layers using the gradients computed during backpropagation:\n\nFor \\(l\\)th hidden layer:\n\n\\(W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial J }{ \\partial W^{[l]} }\\)\n\\(b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial J }{ \\partial b^{[l]} }\\)\n\n\n\n\nImpact of Learning Rate (\\(\\alpha\\)):\n\nThe learning rate \\(\\alpha\\) determines the step size during parameter updates.\nA larger \\(\\alpha\\) might lead to faster convergence but can overshoot the minimum.\nA smaller \\(\\alpha\\) might converge slowly but can be more precise.\n\nIteration and Convergence:\n\nIterate through multiple epochs (iterations) of forward propagation, backpropagation, and parameter updates until the model’s performance converges or the predefined number of epochs is reached.\nMonitor the decrease in the cost function to assess convergence.\n\nImportance of Gradient Descent:\n\nGradient Descent is crucial in adjusting model parameters to minimize the loss function, thereby enhancing the accuracy of predictions in a neural network.\nIt enables the network to learn from the data by iteratively updating weights and biases to improve the overall performance of the model.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nimport numpy as np\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    logprobs = np.multiply(Y, np.log(A2)) + np.multiply((1-Y), np.log(1-A2))\n    cost = - np.sum(logprobs)/Y.shape[1]\n        \n    cost = float(np.squeeze(cost)) \n\n    return cost\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    W1, b1, W2, b2 = copy.deepcopy(parameters[\"W1\"]), copy.deepcopy(parameters[\"b1\"]), copy.deepcopy(parameters[\"W2\"]), copy.deepcopy(parameters[\"b2\"])\n    \n    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n    \n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2\n        \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n        \n    parameters = initialize_parameters(n_x, n_h, n_y)\n        \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads)\n                \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters",
    "crumbs": [
      "Deep Learning",
      "Neural Networks",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "pages/activations.html",
    "href": "pages/activations.html",
    "title": "Weight Initializations",
    "section": "",
    "text": "Activation functions play a crucial role in introducing non-linearity to neural networks, enabling them to model complex relationships and learn intricate patterns from the data. Here are common activation functions used in neural networks:\n\n\n\n\n\n\n\n\n\nActivation\nFormula\nGraph\nDescription\n\n\n\n\nSigmoid\n\\(a = \\frac{1}{1 + e^{-z}}\\)\n\nThe sigmoid function produces outputs between 0 and 1, resembling an S-shaped curve. It’s suitable for binary classification tasks where the output needs to be in the range [0, 1].\n\n\nTanh\n\\(a = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\)\n\nTanh, or hyperbolic tangent, produces outputs between -1 and 1. It is similar to the sigmoid function but with outputs centered around zero, making it suitable for hidden layers.\n\n\nReLU (Rectified Linear Unit)\n\\(a = \\max(0, z)\\)\n\nReLU is a simple and widely used activation function that outputs zero for negative inputs and linearly increases for positive inputs, efficiently mitigating the vanishing gradient problem.\n\n\nLeaky ReLU\n\\(a = \\max(0.01z, z)\\)\n\nLeaky ReLU is an improved version of ReLU, allowing a small gradient for negative inputs to address the “dying ReLU” problem encountered with the standard ReLU function.\n\n\n\n\nImportance of Non-linear Activation Functions:\nNeural networks with non-linear activation functions can approximate complex functions and relationships between inputs and outputs. Linear activation functions result in neural networks that are essentially just linear transformations, limiting their ability to capture intricate patterns present in real-world data.\nA detailed explanation on Stack Overflow provides insights into the necessity of non-linear activation functions in enabling neural networks to learn complex mappings.\n\n\nDerivatives of Activation Functions:\nThe derivatives of activation functions are essential for backpropagation during the training of neural networks. Here are the derivatives of common activation functions:\n\n\n\nActivation\nFormula\nDerivative\n\n\n\n\nSigmoid\n\\(a = \\frac{1}{1 + e^{-z}}\\)\n\\(a(1 - a)\\)\n\n\nTanh\n\\(a = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\)\n\\(1 - a^2\\)\n\n\nReLU\n\\(a = \\max(0, z)\\)\n0 if \\(z &lt; 0\\); 1 if \\(z \\geq 0\\)\n\n\nLeaky ReLU\n\\(a = \\max(0.01z, z)\\)\n0.01 if \\(z &lt; 0\\); 1 if \\(z \\geq 0\\)\n\n\n\nUnderstanding these derivatives is crucial in computing gradients during backpropagation, facilitating the adjustment of weights and biases for effective training of neural networks.\nThese diverse activation functions empower neural networks to handle various types of data and learn complex relationships, contributing to their adaptability and efficiency in modeling intricate patterns within datasets.",
    "crumbs": [
      "Deep Learning",
      "Neural Networks",
      "Activations"
    ]
  },
  {
    "objectID": "pages/mutli-arm.html",
    "href": "pages/mutli-arm.html",
    "title": "Multi-Armed Bandit (MAB)",
    "section": "",
    "text": "The Multi-Arm Bandit (MAB) problem is a fundamental framework in reinforcement learning and decision theory, addressing the critical trade-off between exploration and exploitation. It provides insights into decision-making in uncertain environments, where an agent must strategically choose between multiple options (arms) with unknown reward distributions to maximize cumulative rewards over time.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-Armed Bandit"
    ]
  },
  {
    "objectID": "pages/mutli-arm.html#mathematical-representation",
    "href": "pages/mutli-arm.html#mathematical-representation",
    "title": "Multi-Armed Bandit (MAB)",
    "section": "Mathematical Representation",
    "text": "Mathematical Representation\nLet’s delve into the mathematical symbols that define the \\(\\epsilon\\)-Greedy MAB algorithm:\n\n\\(k\\): Number of arms in the bandit.\n\\(Q(a):\\) Estimated value (average reward) of arm \\(a\\).\n\\(N(a):\\) Number of times arm \\(a\\) has been selected.\n\\(\\epsilon:\\) Exploration-exploitation trade-off parameter, representing the probability of choosing a random action.\n\nThe algorithm initializes estimates and counters for each arm:\n\\[\n\\begin{align*}\nQ(a) &\\leftarrow 0 \\quad \\text{(Initialize estimated values)} \\\\\nN(a) &\\leftarrow 0 \\quad \\text{(Initialize visit counts)}\n\\end{align*}\n\\]\nThe main loop of the algorithm is executed indefinitely:\n\nAction Selection:\n\n\\(A \\leftarrow\n\\begin{cases}\n\\text{argmax}_a Q(a) & \\text{with probability } 1 - \\epsilon \\text{ (breaking ties randomly)} \\\\\n\\text{a random action} & \\text{with probability } \\epsilon\n\\end{cases}\\)\n\nHere, the agent selects an action based on estimated values with high probability but introduces randomness (exploration) with probability \\(\\epsilon\\).\nReward Observation:\n\n\\(R \\leftarrow \\text{bandit}(A)\\)\n\nThe agent receives a reward from the bandit by pulling the chosen arm.\nUpdate Estimates:\n\n\\(N(A) \\leftarrow N(A) + 1\\)\n\\(Q(A) \\leftarrow Q(A) + \\alpha [R - Q(A)]\\) where \\(\\alpha = \\frac{1}{N(A) + 1}\\).\n\nThe estimates and visit counts for the chosen arm are updated based on the received reward.\n\n\nExpanding on the Algorithm\n\nExploration-Exploitation Trade-off: The parameter \\(\\epsilon\\) controls the balance between exploration and exploitation. A higher \\(\\epsilon\\) encourages more exploration, while a lower value promotes exploitation.\nIncremental Update Rule: The update rule \\(Q(A) \\leftarrow Q(A) + \\alpha [R - Q(A)]\\), where \\(\\alpha = \\frac{1}{N(A) + 1}\\), ensures adaptive learning by gradually replacing past estimates with new observations.\nRandom Tie-Breaking: Ties in selecting the action with the maximum estimated value are broken randomly, preventing bias towards a specific action in case of ties.\nConvergence and Learning: Over time, as the algorithm continues to explore and exploit, the estimated values \\(Q(a)\\) converge to the true values of the arms. This results in improved decision-making as the agent learns from experience.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-Armed Bandit"
    ]
  },
  {
    "objectID": "pages/mutli-arm.html#bandit-simulation",
    "href": "pages/mutli-arm.html#bandit-simulation",
    "title": "Multi-Armed Bandit (MAB)",
    "section": "Bandit Simulation",
    "text": "Bandit Simulation\nLet’s run a simulation of a 10-armed bandit using the described \\(\\epsilon\\)-Greedy algorithm:\n\nimport numpy as np\n\nclass MultiArmBandit:\n    def __init__(self, k, epsilon):\n        self.k = k  # Number of arms\n        self.epsilon = epsilon  # Exploration-exploitation trade-off parameter\n        self.q_values = np.zeros(k)  # Estimated values for each arm\n        self.action_counts = np.zeros(k)  # Number of times each arm has been selected\n\n    def select_action(self):\n        if np.random.rand() &lt; self.epsilon:\n            # Exploration: Choose a random action\n            action = np.random.choice(self.k)\n        else:\n            # Exploitation: Choose the action with the highest estimated value\n            action = np.argmax(self.q_values)\n\n        return action\n\n    def update_values(self, action, reward):\n        # Incremental update rule with alpha = 1 / (N(A) + 1)\n        alpha = 1 / (self.action_counts[action] + 1)\n        self.action_counts[action] += 1\n        self.q_values[action] += alpha * (reward - self.q_values[action])\n\n\n# Simulation parameters\nnum_arms = 10\nnum_steps = 10000\ntrue_reward_means = np.random.normal(loc=0, scale=1, size=num_arms)\nepsilon_values = [0, 0.01, 0.1, 0.5, 0.9, 1.0]\n\n# Run simulations for each epsilon value\nreward_history = {epsilon: [] for epsilon in epsilon_values}\n\nfor epsilon in epsilon_values:\n    bandit = MultiArmBandit(num_arms, epsilon)\n    step_rewards = []\n\n    for step in range(num_steps):\n        chosen_action = bandit.select_action()\n        true_reward = np.random.normal(loc=true_reward_means[chosen_action], scale=1)\n        bandit.update_values(chosen_action, true_reward)\n        step_rewards.append(true_reward)\n\n    reward_history[epsilon] = step_rewards\n\n\n# Plotting\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nfor epsilon, rewards in reward_history.items():\n    plt.plot(np.arange(1, num_steps + 1), np.cumsum(rewards) / np.arange(1, num_steps + 1), label=f'Epsilon = {epsilon}')\n\nplt.title('Reward vs Step for Different Epsilon Values')\nplt.xlabel('Step')\nplt.ylabel('Average Reward')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe generated plot illustrates the average reward evolution over time, showcasing the impact of various exploration-exploitation trade-off values (\\(\\epsilon\\)). Each line corresponds to a specific \\(\\epsilon\\) value, offering insights into the algorithm’s behavior under varying degrees of exploration and exploitation.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-Armed Bandit"
    ]
  },
  {
    "objectID": "pages/mutli-arm.html#softmax-action-selection-equation",
    "href": "pages/mutli-arm.html#softmax-action-selection-equation",
    "title": "Multi-Armed Bandit (MAB)",
    "section": "Softmax Action Selection Equation",
    "text": "Softmax Action Selection Equation\nThe softmax action selection equation is given by:\n\\[\n\\pi(a_i) = \\frac{Q(a_i)/\\tau}{\\sum_{i=1}^k Q(a_i)/\\tau}\n\\]\nwhere:\n\n\\(\\pi(a_i)\\) is the probability of selecting action \\(a_i\\).\n\\(Q(a_i)\\) is the estimated value of action \\(a_i\\).\n\\(\\tau\\) is the softmax temperature parameter.\n\\(k\\) is the number of arms.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-Armed Bandit"
    ]
  },
  {
    "objectID": "pages/mutli-arm.html#implementation-in-multi-arm-bandit",
    "href": "pages/mutli-arm.html#implementation-in-multi-arm-bandit",
    "title": "Multi-Armed Bandit (MAB)",
    "section": "Implementation in Multi-Arm Bandit",
    "text": "Implementation in Multi-Arm Bandit\nLet’s modify the MultiArmBandit class to incorporate softmax action selection:\n\nclass MultiArmBandit:\n    def __init__(self, k, tau):\n        self.k = k  # Number of arms\n        self.tau = tau  # Softmax temperature parameter\n        self.q_values = np.zeros(k)  # Estimated values for each arm\n        self.action_counts = np.zeros(k)  # Number of times each arm has been selected\n\n    def select_action(self):\n        # Softmax action selection\n        softmax_probs = np.exp(self.q_values / self.tau) / np.sum(np.exp(self.q_values / self.tau))\n        action = np.random.choice(self.k, p=softmax_probs)\n        return action\n\n    def update_values(self, action, reward):\n        # Incremental update rule with alpha = 1 / (N(A) + 1)\n        alpha = 1 / (self.action_counts[action] + 1)\n        self.action_counts[action] += 1\n        self.q_values[action] += alpha * (reward - self.q_values[action])\n\nIn this modification, the select_action method calculates softmax probabilities based on the estimated values and the softmax temperature. The action is then chosen probabilistically according to these softmax probabilities.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-Armed Bandit"
    ]
  },
  {
    "objectID": "pages/mutli-arm.html#simulation-with-softmax-action-selection",
    "href": "pages/mutli-arm.html#simulation-with-softmax-action-selection",
    "title": "Multi-Armed Bandit (MAB)",
    "section": "Simulation with Softmax Action Selection",
    "text": "Simulation with Softmax Action Selection\nNow, let’s run a simulation using the softmax-based MultiArmBandit class. Experiment with different values of the softmax temperature (\\(\\tau\\)) to observe its impact on the algorithm’s learning strategy and the trade-off between exploration and exploitation.\n\n# Simulation parameters\nnum_arms = 10\nnum_steps = 10000\ntrue_reward_means = np.random.normal(loc=0, scale=1, size=num_arms)\nsoftmax_tau_values = [0.1, 1.0, 10.0]\n\n# Run simulations for each softmax temperature value\nsoftmax_reward_history = {tau: [] for tau in softmax_tau_values}\n\nfor tau in softmax_tau_values:\n    softmax_bandit = MultiArmBandit(num_arms, tau)\n    softmax_step_rewards = []\n\n    for step in range(num_steps):\n        chosen_action = softmax_bandit.select_action()\n        true_reward = np.random.normal(loc=true_reward_means[chosen_action], scale=1)\n        softmax_bandit.update_values(chosen_action, true_reward)\n        softmax_step_rewards.append(true_reward)\n\n    softmax_reward_history[tau] = softmax_step_rewards\n\n\n# Plotting for Softmax Action Selection\nplt.figure(figsize=(12, 8))\nfor tau, rewards in softmax_reward_history.items():\n    plt.plot(np.arange(1, num_steps + 1), np.cumsum(rewards) / np.arange(1, num_steps + 1), label=f'Tau = {tau}')\n\nplt.title('Reward vs Step for Different Softmax Tau Values')\nplt.xlabel('Step')\nplt.ylabel('Average Reward')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe generated plot for softmax action selection illustrates the average reward over time for different softmax temperature values (\\(\\tau\\)). This provides insights into how the algorithm adapts its learning strategy under varying levels of exploration and exploitation.",
    "crumbs": [
      "Reinforcement Learning",
      "Multi-Armed Bandit"
    ]
  }
]