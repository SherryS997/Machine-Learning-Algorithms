[
  {
    "objectID": "pages/weight_init.html",
    "href": "pages/weight_init.html",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "",
    "text": "Training your neural network hinges significantly on the initial values assigned to the weights. An adept choice in initialization methods plays a pivotal role in streamlining the learning process.\nIn the realm of neural networks, weight initialization stands as a critical precursor to successful training. It acts as the bedrock upon which the network learns intricate patterns and correlations within data. Choosing the optimal initialization method can profoundly influence the network’s ability to converge efficiently and effectively.\nThis exploration aims to delve into various initialization techniques, namely random, zeros, and He initialization. Each method introduces distinct characteristics to the neural network’s learning dynamics, impacting its convergence and eventual performance.\nAn adeptly chosen initialization method brings forth several advantages:\n\nAccelerated convergence of gradient descent\nEnhanced likelihood of gradient descent converging towards a lower training and generalization error\n\nLet’s delve into these initialization techniques and witness their unique effects on the learning process. The following is the scatter plot of a datset with two classes:\n\n\n\nThe scatter plot depicts a binary classification scenario featuring two concentric circles. The outer circle corresponds to one class labeled as ‘1,’ while the inner circle represents another class labeled as ‘0.’"
  },
  {
    "objectID": "pages/weight_init.html#introduction",
    "href": "pages/weight_init.html#introduction",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "",
    "text": "Training your neural network hinges significantly on the initial values assigned to the weights. An adept choice in initialization methods plays a pivotal role in streamlining the learning process.\nIn the realm of neural networks, weight initialization stands as a critical precursor to successful training. It acts as the bedrock upon which the network learns intricate patterns and correlations within data. Choosing the optimal initialization method can profoundly influence the network’s ability to converge efficiently and effectively.\nThis exploration aims to delve into various initialization techniques, namely random, zeros, and He initialization. Each method introduces distinct characteristics to the neural network’s learning dynamics, impacting its convergence and eventual performance.\nAn adeptly chosen initialization method brings forth several advantages:\n\nAccelerated convergence of gradient descent\nEnhanced likelihood of gradient descent converging towards a lower training and generalization error\n\nLet’s delve into these initialization techniques and witness their unique effects on the learning process. The following is the scatter plot of a datset with two classes:\n\n\n\nThe scatter plot depicts a binary classification scenario featuring two concentric circles. The outer circle corresponds to one class labeled as ‘1,’ while the inner circle represents another class labeled as ‘0.’"
  },
  {
    "objectID": "pages/weight_init.html#zero-initialization",
    "href": "pages/weight_init.html#zero-initialization",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "Zero Initialization",
    "text": "Zero Initialization\nOne fundamental approach in initializing neural network parameters involves setting all weights and biases to zeros. This method, exemplified in the code snippet above, initializes both weight matrices \\((W^{[1]}, W^{[2]}, ..., W^{[L]})\\) and bias vectors \\((b^{[1]}, b^{[2]}, ..., b^{[L]})\\) to zeros.\ndef initialize_parameters_zeros(layers_dims):\n    parameters = {}\n    L = len(layers_dims)\n    \n    for l in range(1, L):\n        parameters[f\"W{l}\"] = np.zeros((layers_dims[l], layers_dims[l-1]))\n        parameters[f\"b{l}\"] = np.zeros((layers_dims[l], 1))\n        \n    return parameters\nWhile this method is simple and computationally efficient, initializing all parameters to zero can lead to symmetry between neurons in a layer. This symmetry perpetuates throughout the network during training, resulting in every neuron in a given layer learning the same features. Consequently, this hampers the network’s ability to learn diverse representations, thereby limiting its capacity to capture complex patterns within the data.\nLet’s look at how the network performs with this initialization method:\n\n\n\nThe plot exhibits a failed classification attempt, portraying uniform predictions of one class due to ineffective weight initialization.\n\n\nLet’s understand what’s happening:\n\nZero Initialization Resulted in Symmetry: Due to initializing all weights and biases to zero, the network encounters symmetry across neurons within each layer.\nReLU Activation Outputs Zero: The ReLU activation function yields zero when the input, calculated as the product of weights and input, is zero. Therefore, \\(a = \\text{ReLU}(z) = \\max(0, z) = 0\\).\nSigmoid Activation for Classification: At the output layer, using the sigmoid activation function, the prediction (\\(y_{\\text{pred}}\\)) becomes \\(0.5\\) due to the output of \\(a\\) being zero.\nIneffective Loss Differentiation: The resulting \\(y_{\\text{pred}} = 0.5\\) for every input provides no distinction in the loss function. For both \\(y = 1\\) and \\(y = 0\\), the loss (\\(\\mathcal{L}\\)) equates to \\(0.6931\\). This lack of differentiation inhibits the adjustment of weights during training.\nStagnation in Learning: With identical loss values for both classes, there’s no gradient to prompt weight adjustments. Consequently, the model remains stuck with the initial zero weights, leading to uniform predictions of ‘0’ for all examples.\nFailure to Break Symmetry Limits Learning: The failure to break symmetry inhibits diverse learning among neurons within each layer. Consequently, the network’s capability equates to that of a linear classifier, severely limiting its capacity to learn complex patterns.\n\nThis initialization failure illustrates how initializing all weights to zero diminishes the network’s learning capacity, rendering it incapable of discerning intricate patterns within the data."
  },
  {
    "objectID": "pages/weight_init.html#random-initialization",
    "href": "pages/weight_init.html#random-initialization",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "Random Initialization",
    "text": "Random Initialization\nIntroducing randomness in weight initialization breaks symmetrical behavior within the neural network. By initializing weights randomly, each neuron begins learning distinct functions from its inputs. This specific implementation initializes weights randomly to significantly large values. The resulting function utilizes the code snippet provided below:\ndef initialize_parameters_random(layers_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims)            # integer representing the number of layers\n    \n    for l in range(1, L):\n        parameters[f\"W{l}\"] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10\n        parameters[f\"b{l}\"] = np.zeros((layers_dims[l], 1))\n        \n    return parameters\nThis initialization method sets a seed (in this case, seed 3) to ensure reproducibility and generates weights using a random normal distribution. These weights, scaled by a factor of 10, are initialized to encourage diverse learning among neurons in different layers of the neural network.\nLet’s look at how the network performs with this initialization method:\n\n\n\nThe plot showcases a dataset of two classes, visually divided by colors, although the delineation doesn’t perfectly align with the concentric circles defining the dataset.\n\n\nObservations:\n\nThe model’s initial prediction accuracy for all points is notable. However, this high starting cost is due to the utilization of large random-valued weights. This results in the last activation function (sigmoid) producing outputs close to 0 or 1 for certain examples. When misclassifications occur under these conditions, the incurred loss becomes exceedingly high, potentially reaching infinity when \\(\\log(a^{[3]}) = \\log(0)\\).\nSuboptimal initialization can lead to gradient vanishing or exploding, hindering the optimization process’s speed.\nWhile extended training might yield improved results, the initial phase with excessively large random weight values notably impedes the optimization process.\n\nIn summary:\n\nInitializing weights with notably large random values proves ineffective.\nInitializing with smaller random values is anticipated to yield better outcomes. The critical aspect to address is determining the appropriate magnitude for these random values, a pursuit we explore in the subsequent section."
  },
  {
    "objectID": "pages/weight_init.html#he-initialization",
    "href": "pages/weight_init.html#he-initialization",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "He Initialization",
    "text": "He Initialization\nThe He Initialization technique, pioneered by He et al. in 2015, offers an advanced method to initialize neural network parameters. Unlike simple zero initialization or Xavier initialization, He Initialization leverages a specific scaling factor (sqrt(2./layers_dims[l-1])) for the weights \\((W^{[1]}, W^{[2]}, ..., W^{[L]})\\).\ndef initialize_parameters_he(layers_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1\n     \n    for l in range(1, L + 1):\n        parameters[f\"W{l}\"] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2/layers_dims[l-1])\n        parameters[f\"b{l}\"] = np.zeros((layers_dims[l], 1))\n                \n    return parameters\nKey Aspects of He Initialization:\n\nScaling Factor: Employs a scaling factor of sqrt(2./layers_dims[l-1]) for initializing the weights.\nRandom Initialization: Randomizes the weights using a Gaussian distribution with mean zero and variance adjusted by the scaling factor.\n\nImpact:\n\nMitigates Vanishing/Exploding Gradients: The strategic scaling of weights aids in preventing issues like vanishing or exploding gradients during training, facilitating more stable and efficient learning.\nPromotes Non-Linearity: Enables the network to learn more diverse and intricate representations by fostering non-linear activations, thereby enhancing its capability to capture complex patterns within the data.\n\nLet’s look at how the network performs with this initialization method:\n\n\n\nThe plot visually illustrates the model’s precise classification by distinctly coloring the dataset, showcasing perfect separation between classes: one represented by a circular cluster and the remainder of the plot showcasing the alternative class.\n\n\nObservations:\n\nPerfect Separation of Classes:\n\nEffective Classification: The model utilizing He Initialization demonstrates exceptional performance by precisely segregating the two classes (represented by blue and red dots) in the scatter plot.\nAccurate Distribution Capture: Through the learned representations, the model accurately captures the distinctive distribution of the dataset.\n\nEfficient Convergence:\n\nRapid Learning: He Initialization facilitates swift convergence of the model, achieving optimal class separation in a notably small number of iterations."
  },
  {
    "objectID": "pages/weight_init.html#conclusion",
    "href": "pages/weight_init.html#conclusion",
    "title": "Importance of Proper Initialization of Model Weights",
    "section": "Conclusion",
    "text": "Conclusion\nDifferent initialization methods significantly impact neural network performance:\n\nSymmetry Breakage: Zero initialization fails to break symmetry, hindering the network’s ability to learn distinct representations.\nCaution with Weights: Large random initialization leads to excessively large weights, impacting learning.\nHe Initialization Effectiveness: He initialization emerges as a recommended method, effectively enabling the network to achieve a remarkable 99% accuracy. It aptly suits networks employing ReLU activations, showcasing its suitability for diverse architectures.\n\nKey Takeaways:\n\nInitialization Variance: Varied initializations yield divergent outcomes in network performance.\nSymmetry Disruption: Random initialization mitigates symmetry issues, allowing different units to learn diverse features.\nWeight Magnitude Control: Caution must be exercised to prevent excessively large weights.\nHe Initialization Suitability: Especially effective for networks employing ReLU activations, demonstrating superior performance and suitability for numerous architectures."
  },
  {
    "objectID": "pages/nn_basics.html",
    "href": "pages/nn_basics.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "Neural networks, composed of interconnected layers of nodes, serve as powerful computational models inspired by the human brain’s neural structure. Here’s an overview of key components and concepts:\nArchitecture:\n\nA neural network typically comprises an input layer, one or more hidden layers, and an output layer. Each layer contains nodes (neurons) that perform computations.\n\nForward Propagation:\n\nDuring forward propagation, input data \\(X\\) is passed through the network, undergoing computations and activations in each layer:\n\n\\(z^{[l] (i)} = W^{[l]} a^{[l-1](i)} + b^{[l]}\\): Compute the weighted sum and add bias for each layer \\(l\\).\n\\(a^{[l] (i)} = g(z^{[l] (i)})\\): Apply an activation function \\(g\\) to produce the layer’s output.\n\n\nActivation Functions:\n\nActivation functions introduce non-linearity and are applied at each layer’s output:\n\nSigmoid (\\(\\sigma\\)), Tanh, ReLU, Leaky ReLU, etc., each influencing the network’s ability to learn complex patterns.\n\n\nBackpropagation:\n\nBackpropagation involves computing gradients to update network parameters, facilitating learning:\n\nDerivatives of activation functions \\(\\frac{\\partial a}{\\partial z}\\) aid in error computation and parameter updates.\nGradient Descent updates weights (\\(W\\)) and biases (\\(b\\)) using the learning rate (\\(\\alpha\\)) and computed gradients.\n\n\nImportance of Non-linear Activation:\n\nNon-linear activation functions are crucial. Without them, the network simplifies to a linear model, limiting its ability to learn intricate patterns in data.\n\nTraining and Optimization:\n\nTraining involves iteratively adjusting parameters by minimizing a defined loss function (e.g., cross-entropy, mean squared error) using techniques like gradient descent."
  },
  {
    "objectID": "pages/backprop.html",
    "href": "pages/backprop.html",
    "title": "Backpropagation",
    "section": "",
    "text": "Backpropagation is the process of iteratively computing gradients of the loss function with respect to the weights and biases of the neural network, enabling the adjustment of parameters by propagating errors backward through the network’s layers to enhance learning and optimize performance.\n\nOutput Layer Error Derivative:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial z_{L}^{(i)} } = \\frac{1}{m} (\\hat{y}^{(i)} - y^{(i)}) \\odot g'(z^{[L] (i)})\\)\n\nCalculate the derivative of the cost function with respect to the pre-activation value of the output layer for each training example.\n\\(\\odot\\) denotes element-wise multiplication.\n\\(g'(\\cdot)\\) represents the derivative of the activation function used in the output layer.\n\n\nGradient Calculation for Output Layer Weights and Bias:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial W^{[L]} } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{L}^{(i)} } a^{[L-1] (i) T}\\)\n\nCompute the gradient of the loss function with respect to the weights of the output layer.\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial b^{[L]} } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{L}^{(i)}}}\\)\n\nCompute the gradient of the loss function with respect to the bias of the output layer.\n\n\nBackpropagate the Error to Previous Layers:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial z_{l}^{(i)} } = W^{[l+1] T} \\frac{\\partial \\mathcal{J} }{ \\partial z_{l+1}^{(i)} } \\odot g'(z^{[l] (i)})\\)\n\nPropagate the error derivative backward through the layers by considering the relationship between consecutive layers.\nInvolves the transpose of weights connecting the subsequent and current layers.\nInvolves the derivative of the activation function used in layer \\(l\\).\n\n\nGradient Calculation for Weights and Bias of Hidden Layers:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{l}^{(i)} } a^{[l-1] (i) T}\\)\n\nCompute the gradient of the loss function with respect to the weights of layer \\(l\\).\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial b^{[l]} } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{l}^{(i)}}}\\)\n\nCompute the gradient of the loss function with respect to the bias of layer \\(l\\).\n\n\n\nThese steps form the core of backpropagation in a shallow neural network, allowing for the iterative adjustment of weights and biases to minimize the loss function and improve the model’s accuracy in predicting outputs for given inputs.\n\n\n\n\n\n\nCode\n\n\n\n\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n    \n    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n        \n    dZ2 = A2 - Y\n    dW2 = (1/Y.shape[1])*dZ2@A1.T\n    db2 = (1/Y.shape[1])*np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = W2.T@dZ2*(1-A1**2)\n    dW1 = (1/Y.shape[1])*dZ1@X.T\n    db1 = (1/Y.shape[1])*np.sum(dZ1, axis=1, keepdims=True)\n        \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads"
  },
  {
    "objectID": "pages/notations_nn .html",
    "href": "pages/notations_nn .html",
    "title": "Common Notations Used in Neural Networks",
    "section": "",
    "text": "Superscript and Subscript Indices:\n\n\\([i]\\): Represents the index referring to a specific training example within the dataset.\n\\([l]\\): Denotes the layer indices. In a network with \\(L\\) layers, \\([l]\\) ranges from \\(1\\) to \\(L\\).\n\nVariables:\n\n\\(x^{(i)}\\): Input features for the \\(i\\)-th training example.\n\\(y^{(i)}\\): Actual output or target for the \\(i\\)-th training example.\n\\(\\hat{y}^{(i)}\\), \\(a^{[l](i)}\\): Predicted output or activation of layer \\(l\\) for the \\(i\\)-th training example.\n\\(z^{[l](i)}\\): Pre-activation value of layer \\(l\\) for the \\(i\\)-th training example.\n\nWeights and Biases:\n\n\\(W^{[l]}\\): Weight matrix associated with the connections between layer \\(l - 1\\) and layer \\(l\\).\n\\(b^{[l]}\\): Bias vector added to the weighted sum in layer \\(l\\).\n\nDerivatives and Gradients:\n\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial z^{[l](i)} }\\): Derivative of the cost function with respect to the pre-activation value of layer \\(l\\) for the \\(i\\)-th training example.\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} }\\): Gradient of the loss function with respect to the weights of layer \\(l\\).\n\\(\\frac{\\partial \\mathcal{J} }{ \\partial b^{[l]} }\\): Gradient of the loss function with respect to the bias of layer \\(l\\).\n\nCost Function:\n\n\\(J\\): Overall cost function that measures the discrepancy between predicted and actual outputs across all training examples.\n\\(m\\): Number of training examples in the dataset.\n\nActivation Functions:\n\n\\(\\sigma()\\): Activation function applied element-wise to the weighted sums in each layer.\n\nMathematical Operations:\n\n\\(\\log()\\): Natural logarithm function used in the calculation of the cost function.\n\\(\\odot\\): Element-wise multiplication.\n\n\nThese notations provide a general framework for understanding and representing variables, derivatives, weights, biases, and mathematical operations involved in the computations and optimizations of neural networks across different architectures and configurations."
  },
  {
    "objectID": "pages/percp.html",
    "href": "pages/percp.html",
    "title": "Perceptron",
    "section": "",
    "text": "The Perceptron Learning Algorithm (PLA) is a supervised learning algorithm widely employed for binary classification tasks. Its primary objective is to determine a decision boundary that effectively separates the two classes in the dataset. This algorithm belongs to the class of discriminative classification methods as it focuses on modeling the boundary between classes instead of characterizing the underlying probability distribution of each class.\nLet \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) represent the dataset, where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe algorithm is founded on the following assumptions:\n\n\\(P(y=1|\\mathbf{x}) = 1\\) if \\(\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\geq0\\), otherwise \\(P(y=1|\\mathbf{x}) = 0\\).\nLinear Separability Assumption: The Linear Separability Assumption is a fundamental assumption made in various machine learning algorithms, including the Perceptron Learning Algorithm. It posits that the classes to be classified can be accurately separated by a linear decision boundary. In other words, there exists a hyperplane in the feature space that can effectively segregate the data points of the two classes.\n\nThe objective function is defined as follows:\n\\[\n\\min _{h \\in \\mathcal{H}} \\sum _{i=1} ^n \\mathbb{1}\\left ( h(\\mathbf{x}_i) \\neq y_i \\right )\n\\]\nEven if \\(\\mathcal{H}\\) accounts only for the Linear Hypotheses, this problem is generally considered NP-Hard.\nUnder the Linear Separability Assumption, assuming the existence of \\(\\mathbf{w} \\in \\mathbb{R}^d\\) such that \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)=y_i\\) holds for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), the PLA solves the convergence problem using an iterative algorithm. The algorithm proceeds as follows:\n\nInitialize \\(\\mathbf{w}^0 = \\mathbf{0} \\in \\mathbb{R}^d\\)\nUntil Convergence:\n\nSelect a \\((\\mathbf{x}_i, y_i)\\) pair from the dataset\nIf \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)\n\nDo nothing\n\nElse\n\nUpdate the weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\)\n\nEnd\n\n\n\n\nFor a given training example \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input and \\(y\\) represents the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates the weight vector \\(\\mathbf{w}\\) according to the following rules:\n\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is correct (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)), no update is performed.\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is incorrect (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)\\neq y_i\\)), the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\).\nIt is important to note that the update occurs solely in response to the current data point. Consequently, data points that were previously classified correctly may not be classified similarly in future iterations.\n\nThis update rule effectively adjusts the decision boundary in the direction of correct classification for the misclassified example. The algorithm is guaranteed to converge to a linearly separable solution if the data is indeed linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution.\n\n\n\nWe introduce three additional assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) is considered linearly separable with a \\(\\gamma\\)-margin if there exists \\(\\mathbf{w}^* \\in \\mathbb{R}^d\\) such that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\geq\\gamma\\) holds for all \\(i\\), where \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let \\(R&gt;0 \\in \\mathbb{R}\\) be a constant such that \\(\\forall i \\in D\\), \\(||\\mathbf{x}_i||\\leq R\\). In other words, \\(R\\) denotes the length of the data point farthest from the center.\nNormal Length for \\(\\mathbf{w}^*\\): Assume that \\(\\mathbf{w}^*\\) has unit length."
  },
  {
    "objectID": "pages/percp.html#analysis-of-the-update-rule",
    "href": "pages/percp.html#analysis-of-the-update-rule",
    "title": "Perceptron",
    "section": "",
    "text": "For a given training example \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input and \\(y\\) represents the correct output (either \\(1\\) or \\(-1\\)), the perceptron algorithm updates the weight vector \\(\\mathbf{w}\\) according to the following rules:\n\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is correct (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)==y_i\\)), no update is performed.\nIf the perceptron’s prediction on \\(\\mathbf{x}\\) is incorrect (i.e., \\(\\text{sign}(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)\\neq y_i\\)), the weights are updated by adding the product of the input vector and the correct output to the current weight vector: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^t + \\mathbf{x}_iy_i\\).\nIt is important to note that the update occurs solely in response to the current data point. Consequently, data points that were previously classified correctly may not be classified similarly in future iterations.\n\nThis update rule effectively adjusts the decision boundary in the direction of correct classification for the misclassified example. The algorithm is guaranteed to converge to a linearly separable solution if the data is indeed linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution."
  },
  {
    "objectID": "pages/percp.html#further-assumptions",
    "href": "pages/percp.html#further-assumptions",
    "title": "Perceptron",
    "section": "",
    "text": "We introduce three additional assumptions:\n\nLinear Separability with \\(\\gamma\\)-Margin: A dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) is considered linearly separable with a \\(\\gamma\\)-margin if there exists \\(\\mathbf{w}^* \\in \\mathbb{R}^d\\) such that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\geq\\gamma\\) holds for all \\(i\\), where \\(\\gamma&gt;0\\).\n\n\n\n\nLinear Separability with \\(\\gamma\\)-Margin\n\n\n\nRadius Assumption: Let \\(R&gt;0 \\in \\mathbb{R}\\) be a constant such that \\(\\forall i \\in D\\), \\(||\\mathbf{x}_i||\\leq R\\). In other words, \\(R\\) denotes the length of the data point farthest from the center.\nNormal Length for \\(\\mathbf{w}^*\\): Assume that \\(\\mathbf{w}^*\\) has unit length."
  },
  {
    "objectID": "pages/dec_trees.html",
    "href": "pages/dec_trees.html",
    "title": "Decision tree",
    "section": "",
    "text": "Decision trees are widely used in machine learning for classification and regression tasks. They operate by recursively partitioning the data based on the most informative features until a stopping criterion is met. Decision trees can be visualized as tree-like structures.\nConsider a dataset \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\), where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and let \\(\\{y_1, \\ldots, y_n\\}\\) be the corresponding labels, where \\(y_i \\in \\{0, 1\\}\\). The output of the decision tree algorithm is a constructed decision tree.\nPrediction: Given a test sample \\(\\mathbf{x}_{\\text{test}}\\), we traverse the decision tree to reach a leaf node, and the label assigned to the leaf node is considered as \\(y_{\\text{test}}\\).\nThe following diagram depicts a decision tree:\n\nHere, a question refers to a (feature, value) pair. For example, \\(height \\le 180\\text{cm}\\)?\n\n\nTo evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]\n\n\n\nThe decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:"
  },
  {
    "objectID": "pages/dec_trees.html#goodness-of-a-question",
    "href": "pages/dec_trees.html#goodness-of-a-question",
    "title": "Decision tree",
    "section": "",
    "text": "To evaluate the quality of a question, we need a measure of “impurity” for a set of labels \\(\\{y_1, \\ldots, y_n\\}\\). Various measures can be employed, but we will use the Entropy function.\nThe Entropy function is defined as:\n\\[\n\\text{Entropy}(\\{y_1, \\ldots, y_n\\}) = \\text{Entropy}(p) = -\\left( p\\log(p)+(1-p)\\log(1-p) \\right )\n\\]\nHere, \\(\\log(0)\\) is conventionally treated as \\(0\\).\nPictorial representation of the Entropy function:\n\nInformation Gain is then utilized to measure the quality of a split in the decision tree algorithm.\nInformation gain is a commonly used criterion in decision tree algorithms that quantifies the reduction in entropy or impurity of a dataset after splitting based on a given feature. High information gain signifies features that effectively differentiate between the different classes of data and lead to accurate predictions.\nInformation gain is calculated as:\n\\[\n\\text{Information Gain}(\\text{feature}, \\text{value}) = \\text{Entropy}(D) - \\left[\\gamma \\cdot \\text{Entropy}(D_{\\text{yes}}) + (1-\\gamma) \\cdot \\text{Entropy}(D_{\\text{no}}) \\right]\n\\]\nwhere \\(\\gamma\\) is defined as:\n\\[\n\\gamma = \\frac{|D_{\\text{yes}}|}{|D|}\n\\]"
  },
  {
    "objectID": "pages/dec_trees.html#decision-tree-algorithm",
    "href": "pages/dec_trees.html#decision-tree-algorithm",
    "title": "Decision tree",
    "section": "",
    "text": "The decision tree algorithm follows these steps:\n\nDiscretize each feature within the range [min, max].\nSelect the question that provides the highest information gain.\nRepeat the procedure for subsets \\(D_{\\text{yes}}\\) and \\(D_{\\text{no}}\\).\nStop growing the tree when a node becomes sufficiently “pure” according to a predefined criterion.\n\nDifferent measures, such as the Gini Index, can also be employed to evaluate the quality of a question.\nPictorial depiction of the decision boundary and its decision tree:"
  },
  {
    "objectID": "pages/lasso_lin_reg.html",
    "href": "pages/lasso_lin_reg.html",
    "title": "Lasso Regression",
    "section": "",
    "text": "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a linear regression technique that employs a regularization approach to shrink the coefficients of less important features to zero. This method effectively performs feature selection and mitigates overfitting.\nThe objective function of lasso regression is given by:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum^n_{i=1}(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 + \\lambda||\\mathbf{w}||_1^2\n\\]\nLasso regression is similar to ridge regression, with the key difference being the use of \\(||\\mathbf{w}||_1^2\\) instead of \\(||\\mathbf{w}||_2^2\\), representing the squared L1 norm of \\(\\mathbf{w}\\).\n\n\n\nPictoral Representation of what Lasso Regression does.\n\n\nLasso regression does not have a closed-form solution and is often solved using sub-gradients. Further information on sub-gradients can be found here.\nIn conclusion, lasso regression shrinks the coefficients of less important features to exactly zero, enabling feature selection."
  },
  {
    "objectID": "pages/bayes_lin_reg.html",
    "href": "pages/bayes_lin_reg.html",
    "title": "Bayesian view of least squares regression",
    "section": "",
    "text": "Probabilistic View of Linear Regression\nConsider a dataset \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\) with \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and the corresponding labels \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\) with \\(\\mathbf{y}_i \\in \\mathbb{R}\\). The probabilistic view of linear regression assumes that the target variable \\(\\mathbf{y}_i\\) can be modeled as a linear combination of the input features \\(\\mathbf{x}_i\\), with an additional noise term \\(\\epsilon\\) following a zero-mean Gaussian distribution with variance \\(\\sigma^2\\). Mathematically, this can be expressed as:\n\\[\n\\mathbf{y}_i = \\mathbf{w}^T\\mathbf{x}_i + \\epsilon_i\n\\]\nwhere \\(\\mathbf{w} \\in \\mathbb{R}^d\\) represents the weight vector that captures the relationship between the inputs and the target variable.\nTo estimate the weight vector \\(\\mathbf{w}\\) that best fits the data, we can apply the principle of Maximum Likelihood (ML). The ML estimation seeks to find the parameter values that maximize the likelihood of observing the given data.\nAssuming that the noise term \\(\\epsilon_i\\) follows a zero-mean Gaussian distribution with variance \\(\\sigma^2\\), we can express the likelihood function as:\n\\[\\begin{align*}\n\\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y}) &= P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) \\\\\n&= \\prod_{i=1}^n P(\\mathbf{y}_i|\\mathbf{x}_i; \\mathbf{w}) \\\\\n&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2}{2\\sigma^2}\\right)\n\\end{align*}\\]\nTaking the logarithm of the likelihood function, we have:\n\\[\\begin{align*}\n\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y}) &= \\sum_{i=1}^n \\log \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right) - \\frac{(\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2}{2\\sigma^2} \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2\n\\end{align*}\\]\nTo find the maximum likelihood estimate \\(\\mathbf{w}_{\\text{ML}}\\), we want to maximize \\(\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})\\). Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Thus, we seek to minimize:\n\\[\\begin{align*}\n-\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y}) &= \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2\n\\end{align*}\\]\nThis expression is equivalent to the mean squared error (MSE) objective function used in linear regression. Therefore, finding the maximum likelihood estimate \\(\\mathbf{w}_{\\text{ML}}\\) is equivalent to solving the linear regression problem using the squared error loss.\nTo obtain the closed-form solution for \\(\\mathbf{w}_{\\text{ML}}\\), we differentiate the negative log-likelihood with respect to \\(\\mathbf{w}\\) and set the derivative to zero:\n\\[\\begin{align*}\n\\nabla_{\\mathbf{w}} \\left(-\\log \\mathcal{L}(\\mathbf{w}; \\mathbf{X}, \\mathbf{y})\\right) &= \\frac{1}{\\sigma^2}\\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)\\mathbf{x}_i^T = \\mathbf{0}\n\\end{align*}\\]\nThis can be rewritten as:\n\\[\\begin{align*}\n\\frac{1}{\\sigma^2}\\left(\\mathbf{X}\\mathbf{X}^T\\mathbf{w} - \\mathbf{X}\\mathbf{y}\\right) &= \\mathbf{0}\n\\end{align*}\\]\nwhere \\(\\mathbf{X}\\) is the matrix whose rows are the input vectors \\(\\mathbf{x}_i\\) and \\(\\mathbf{y}\\) is the column vector of labels. Rearranging the equation, we have:\n\\[\\begin{align*}\n\\mathbf{X}\\mathbf{X}^T\\mathbf{w} &= \\mathbf{X}\\mathbf{y}\n\\end{align*}\\]\nTo obtain the closed-form solution for \\(\\mathbf{w}_{\\text{ML}}\\), we multiply both sides by the inverse of \\(\\mathbf{X}\\mathbf{X}^T\\), denoted as \\((\\mathbf{X}\\mathbf{X}^T)^{-1}\\):\n\\[\\begin{align*}\n\\mathbf{w}_{\\text{ML}} &= (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{X}\\mathbf{y}\n\\end{align*}\\]\nThus, the closed-form solution for the maximum likelihood estimate \\(\\mathbf{w}_{\\text{ML}}\\) is given by the product of \\((\\mathbf{X}\\mathbf{X}^T)^{-1}\\) and \\(\\mathbf{X}\\mathbf{y}\\).\nThe closed-form solution for \\(\\mathbf{w}_{\\text{ML}}\\) in linear regression demonstrates that it can be obtained by directly applying a matrix inverse operation to the product of the input matrix \\(\\mathbf{X}\\) and the target variable vector \\(\\mathbf{y}\\). This closed-form solution provides an efficient and direct way to estimate the weight vector \\(\\mathbf{w}\\) based on the given data.\n\n\nEvaluation of the Maximum Likelihood Estimator for Linear Regression\nLinear regression is a widely used technique for modeling the relationship between a dependent variable and one or more independent variables. The maximum likelihood estimator (MLE) is commonly employed to estimate the parameters of a linear regression model. Here, we discuss the goodness of the MLE for linear regression, explore cross-validation techniques to minimize mean squared error (MSE), examine Bayesian modeling as an alternative approach, and finally, delve into ridge and lasso regression as methods to mitigate overfitting.\n\n\nGoodness of Maximum Likelihood Estimator for Linear Regression\nConsider a dataset comprising input vectors \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\), where each \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and corresponding labels \\(\\{y_1, \\ldots, y_n\\}\\), with \\(y_i \\in \\mathbb{R}\\). We can express the relationship between the inputs and labels using the linear regression model:\n\\[\ny|\\mathbf{X} = \\mathbf{w}^T\\mathbf{x} + \\epsilon\n\\]\nHere, \\(\\epsilon\\) represents the random noise following a normal distribution \\(\\mathcal{N}(0,\\sigma^2)\\), and \\(\\mathbf{w} \\in \\mathbb{R}^d\\) denotes the regression coefficients. The maximum likelihood parameter estimation for linear regression, denoted as \\(\\hat{\\mathbf{w}}_{\\text{ML}}\\), can be computed as:\n\\[\n\\hat{\\mathbf{w}}_{\\text{ML}} = \\mathbf{w}^* = (\\mathbf{X}\\mathbf{X}^T)^+\\mathbf{X}\\mathbf{y}\n\\]\nTo evaluate the quality of the estimated parameters, we measure the mean squared error (MSE) between the estimated parameters and the true parameters \\(\\mathbf{w}\\). The MSE is given by:\n\\[\n\\mathbb{E}[|| \\hat{\\mathbf{w}}_{\\text{ML}} - \\mathbf{w} ||^2_2]\n\\]\nInterestingly, the MSE can be expressed as:\n\\[\n\\mathbb{E}[|| \\hat{\\mathbf{w}}_{\\text{ML}} - \\mathbf{w} ||^2_2] = \\sigma^2 \\cdot \\text{trace}((\\mathbf{X}\\mathbf{X}^T)^{-1})\n\\]\nThis result provides a quantitative measure of the goodness of the maximum likelihood estimator for linear regression.\n\n\nCross-Validation for Minimizing MSE\nIn order to minimize the MSE, we can utilize cross-validation techniques. Let the eigenvalues of \\(\\mathbf{X}\\mathbf{X}^T\\) be denoted as \\(\\{\\lambda_1, \\ldots, \\lambda_d\\}\\). Consequently, the eigenvalues of \\((\\mathbf{X}\\mathbf{X}^T)^{-1}\\) are given by \\(\\{\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_d}\\}\\).\nThe MSE can be expressed as:\n\\[\n\\mathbb{E}[|| \\hat{\\mathbf{w}}_{\\text{ML}} - \\mathbf{w} ||^2_2] = \\sigma^2 \\sum_{i=1}^d \\frac{1}{\\lambda_i}\n\\]\nTo improve the estimator, we introduce a modified estimator, denoted as \\(\\hat{\\mathbf{w}}_{\\text{new}}\\), defined as:\n\\[\n\\hat{\\mathbf{w}}_{\\text{new}} = (\\mathbf{X}\\mathbf{X}^T + \\lambda \\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}\n\\]\nHere, \\(\\lambda \\in \\mathbb{R}\\) and \\(\\mathbf{I} \\in \\mathbb{R}^{d\\times d}\\) represents the identity matrix. By utilizing this modified estimator, we can calculate:\n\\[\n\\text{trace}((\\mathbf{X}\\mathbf{X}^T + \\lambda \\mathbf{I})^{-1}) = \\sum_{i=1}^d \\frac{1}{\\lambda_i + \\lambda}\n\\]\nAccording to the Existence Theorem, there exists a value of \\(\\lambda\\) such that \\(\\hat{\\mathbf{w}}_{\\text{new}}\\) exhibits a lower mean squared error than \\(\\hat{\\mathbf{w}}_{\\text{ML}}\\). In practice, the value for \\(\\lambda\\) is determined using cross-validation techniques.\nThree commonly used techniques for cross-validation are as follows:\n\nTraining-Validation Split: The training set is randomly divided into a training set and a validation set, typically in an 80:20 ratio. Among various \\(\\lambda\\) values, the one that yields the lowest error is selected.\nK-Fold Cross Validation: The training set is partitioned into K equally-sized parts. The model is trained K times, each time using K-1 parts as the training set and the remaining part as the validation set. The \\(\\lambda\\) value that leads to the lowest average error is chosen.\nLeave-One-Out Cross Validation: The model is trained using all but one sample in the training set, and the left-out sample is used for validation. This process is repeated for each sample in the dataset. The optimal \\(\\lambda\\) is determined based on the average error across all iterations.\n\nBy employing cross-validation techniques, we can enhance the performance of the linear regression model by selecting an appropriate value of \\(\\lambda\\).\n\n\nBayesian Modeling\nAlternatively, we can understand the maximum likelihood estimator \\(\\hat{\\mathbf{w}}_{\\text{ML}}\\) in the context of Bayesian modeling.\nAssume that \\(P(y|\\mathbf{X})\\) follows a normal distribution \\(\\mathcal{N}(\\mathbf{w}^T\\mathbf{x},\\mathbf{I})\\), where \\(I\\) represents the identity matrix for simplicity.\nFor the prior distribution of \\(\\mathbf{w}\\), a suitable choice is the normal distribution \\(\\mathcal{N}(0,\\gamma^2\\mathbf{I})\\), where \\(\\gamma^2\\mathbf{I} \\in\\mathbb{R}^{d\\times d}\\).\nThus, we can write:\n\\[\\begin{align*}\nP(\\mathbf{w}|\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}) &\\propto P(\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}|\\mathbf{w})*P(\\mathbf{w}) \\\\\n&\\propto \\left ( \\prod_{i=1}^n e^{\\frac{-(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2}{2}}  \\right ) * \\left ( \\prod_{i=1}^d  e^{\\frac{-(\\mathbf{w}_i - 0)^2}{2\\gamma^2}} \\right ) \\\\\n&\\propto \\left ( \\prod_{i=1}^n e^{\\frac{-(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2}{2}}  \\right ) * e^{\\frac{-||\\mathbf{w}||^2}{2\\gamma^2}}\n\\end{align*}\\]\nTaking the logarithm, we obtain:\n\\[\n\\log(P(\\mathbf{w}|\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\})) \\propto \\frac{-(y_i - \\mathbf{w}^T\\mathbf{x}_i)^2}{2}-\\frac{||\\mathbf{w}||^2}{2\\gamma^2}\n\\]\nUpon computing the gradient, we find:\n\\[\n\\nabla \\log(P(\\mathbf{w}|\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\})) \\propto  (\\mathbf{X}\\mathbf{X}^T)\\hat{\\mathbf{w}}_{\\text{MAP}} - \\mathbf{X}\\mathbf{y} + \\frac{\\hat{\\mathbf{w}}_{\\text{MAP}}}{\\gamma^2}\n\\]\nConsequently, the maximum a posteriori estimate (MAP) for \\(\\mathbf{w}\\) can be computed as:\n\\[\n\\hat{\\mathbf{w}}_{\\text{MAP}} = (\\mathbf{X}\\mathbf{X}^T + \\frac{1}{\\gamma^2} \\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}\n\\]\nIn practice, the value of \\(\\frac{1}{\\gamma^2}\\) is obtained using cross-validation. Remarkably, this maximum a posteriori estimation for linear regression with a Gaussian prior \\(\\mathcal{N}(0,\\gamma^2\\mathbf{I})\\) for \\(\\mathbf{w}\\) is equivalent to the modified estimator \\(\\hat{\\mathbf{w}}_{\\text{new}}\\) discussed earlier."
  },
  {
    "objectID": "pages/lin_reg.html",
    "href": "pages/lin_reg.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a supervised learning algorithm employed to predict a continuous output variable based on one or more input features, assuming a linear relationship between the input and output variables. The primary objective of linear regression is to determine the line of best fit that minimizes the sum of squared errors between the predicted and actual output values.\nGiven a dataset \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\) where each \\(\\mathbf{x}_i\\) belongs to \\(\\mathbb{R}^d\\), and the corresponding labels \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\) belong to \\(\\mathbb{R}\\), the goal of linear regression is to find a mapping between the input and output variables, represented as follows:\n\\[\nh: \\mathbb{R}^d \\rightarrow \\mathbb{R}\n\\]\nThe error for this mapping function can be quantified as:\n\\[\n\\text{error}(h) = \\sum_{i=1}^n (h(\\mathbf{x}_i) - \\mathbf{y}_i)^2\n\\]\nIdeally, this error should be minimized, which occurs when \\(h(\\mathbf{x}_i) = \\mathbf{y}_i\\) for all \\(i\\). However, achieving this may only result in memorizing the data and its outputs, which is not a desired outcome.\nTo mitigate the memorization problem, introducing a structure to the mapping becomes necessary. The simplest and commonly used structure is linear, which we will adopt as the underlying structure for our data.\nLet \\(\\mathcal{H}_{\\text{linear}}\\) denote the solution space for the mapping in the linear domain:\n\\[\n\\mathcal{H}_{\\text{linear}} = \\left\\lbrace h_w: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\ \\text{s.t.} \\ h_w(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} \\ \\forall \\mathbf{w} \\in \\mathbb{R}^d \\right\\rbrace\n\\]\nThus, our objective is to minimize:\n\\[\\begin{align*}\n\\min_{h \\in \\mathcal{H}_{\\text{linear}}} \\sum_{i=1}^n (h(\\mathbf{x}_i) - \\mathbf{y}_i)^2 \\\\\n\\text{Equivalently,} \\\\\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum_{i=1}^n (\\mathbf{w}^T\\mathbf{x}_i - \\mathbf{y}_i)^2\n\\end{align*}\\]\nOptimizing the above objective is the main aim of the linear regression algorithm."
  },
  {
    "objectID": "pages/lin_reg.html#stochastic-gradient-descent",
    "href": "pages/lin_reg.html#stochastic-gradient-descent",
    "title": "Linear Regression",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nStochastic gradient descent (SGD) is an optimization algorithm widely employed in machine learning to minimize the loss function of a model by determining the optimal parameters. Unlike traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters using a randomly selected subset of the data, known as a batch. This approach leads to faster training times and makes SGD particularly suitable for handling large datasets.\nInstead of updating \\(\\mathbf{w}\\) using the entire dataset at each step \\(t\\), SGD leverages a small randomly selected subset of \\(k\\) data points to update \\(\\mathbf{w}\\). Consequently, the new gradient becomes \\(2(\\tilde{\\mathbf{X}}\\tilde{\\mathbf{X}}^T\\mathbf{w}^t - \\tilde{\\mathbf{X}}\\tilde{\\mathbf{y}})\\), where \\(\\tilde{\\mathbf{X}}\\) and \\(\\tilde{\\mathbf{y}}\\) represent small samples randomly chosen from the dataset. This strategy is feasible since \\(\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{d \\times k}\\), which is considerably smaller compared to \\(\\mathbf{X}\\).\nAfter \\(T\\) rounds of training, the final estimate is obtained as follows:\n\\[\n\\mathbf{w}_{\\text{SGD}}^T = \\frac{1}{T} \\sum_{i=1}^T \\mathbf{w}^i\n\\]\nThe stochastic nature of SGD contributes to optimal convergence to a certain extent.\n\nImplementing Stochastic Gradient Descent in Python\nLet compute the \\(w\\) vector using Stochastic Gradient Descent and print the coefficients and intercept.\nw = np.ones((X.shape[0], 1))\neta = 1e-4\nt = 100000\nn = X.shape[1]\nb = 10\n\nfor i in range(t):\n    # randomly select a batch of samples\n    idx = np.random.choice(n, b, replace=False)\n    X_b = X[:, idx]\n    y_b = y[idx]\n    # compute the gradient for the batch\n    grad = 2*(X_b @ X_b.T @ w) - 2*(X_b @ y_b)\n    # update the weights\n    w = w - eta*grad\n    \nprint ('Coefficients: ', w.reshape((-1,))[:3])\nprint ('Intercept: ', w.reshape((-1,))[-1])\n\n\n\n\n\n\nCoefficients: [10.72000912, 8.3366805, 9.13970723]\nIntercept: 65.2366350549217"
  },
  {
    "objectID": "pages/gmm.html",
    "href": "pages/gmm.html",
    "title": "Gaussian mixture model (GMM)",
    "section": "",
    "text": "Gaussian Mixture Models are a type of probabilistic model used to represent complex data distributions by combining multiple Gaussian distributions.\nThe procedure is as follows:\n\nStep 1: Generate a mixture component among \\(\\{1, 2, \\ldots, K\\}\\) where \\(z_i \\in \\{1, 2, \\ldots, K\\}\\). We obtain, \\[\nP(z_i=k) = \\pi_k \\hspace{2em} \\left [ \\sum _{i=1} ^K \\pi_i = 1 \\hspace{1em} 0 \\le \\pi_i \\le 1 \\hspace{1em} \\forall i \\right ]\n\\]\nStep 2: Generate \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{z_i}, \\boldsymbol{\\sigma}^2_{z_i})\\)\n\nHence, there are \\(3K\\) parameters. However, since \\(\\displaystyle \\sum _{i=1} ^K \\pi_i = 1\\), the number of parameters to be estimated becomes \\(3K-1\\) for a GMM with \\(K\\) components.\n\nLikelihood of GMM’s\n\\[\\begin{align*}\n\\mathcal{L}\\left( \\begin{array}{cccc}\n\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K \\\\\n\\boldsymbol{\\sigma}^2_1, \\boldsymbol{\\sigma}^2_2, \\ldots, \\boldsymbol{\\sigma}^2_K\\\\\n\\pi_1, \\pi_2, \\ldots, \\pi_K\n\\end{array}; \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n \\right )\n&= \\prod _{i=1} ^n f_{\\text{mix}} \\left( \\mathbf{x}_i; \\begin{array}{cccc}\n\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K \\\\\n\\boldsymbol{\\sigma}^2_1, \\boldsymbol{\\sigma}^2_2, \\ldots, \\boldsymbol{\\sigma}^2_K\\\\\n\\pi_1, \\pi_2, \\ldots, \\pi_K\n\\end{array} \\right ) \\\\\n&= \\prod _{i=1} ^n \\left [ \\sum _{k=1} ^K \\pi_k * f_{\\text{mix}}(\\mathbf{x}_i; \\boldsymbol{\\mu}_k, \\boldsymbol{\\sigma}_k) \\right ] \\\\\n\\therefore \\log\\mathcal{L}(\\boldsymbol{\\theta}) &= \\sum _{i=1} ^n \\log \\left [ \\sum _{k=1} ^K \\pi_k * \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}_k} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu}_k)^2}{2\\boldsymbol{\\sigma}^2_k}} \\right ] \\\\\n\\end{align*}\\] To solve the above equation, we need to understand convexity."
  },
  {
    "objectID": "pages/mle.html",
    "href": "pages/mle.html",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "",
    "text": "Estimation in machine learning involves inferring unknown parameters or predicting outcomes from observed data. Estimators, often algorithms or models, are used for these tasks and to characterize the data’s underlying distribution.\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) represent a dataset, where each data point \\(\\mathbf{x}_i\\) is in the \\(d\\)-dimensional binary space \\(\\{0,1\\}^d\\). It is assumed that the data points are independent and identically distributed (i.i.d).\nIndependence is denoted as \\(P(\\mathbf{x}_i|\\mathbf{x}_j) = P(\\mathbf{x}_i)\\). Identically distributed means \\(P(\\mathbf{x}_i)=P(\\mathbf{x}_j)=p\\)."
  },
  {
    "objectID": "pages/mle.html#fishers-principle-of-maximum-likelihood",
    "href": "pages/mle.html#fishers-principle-of-maximum-likelihood",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Fisher’s Principle of Maximum Likelihood",
    "text": "Fisher’s Principle of Maximum Likelihood\nFisher’s principle of maximum likelihood is a statistical method used to estimate parameters of a statistical model by selecting values that maximize the likelihood function. This function quantifies how well the model fits the observed data."
  },
  {
    "objectID": "pages/mle.html#likelihood-estimation-for-bernoulli-distributions",
    "href": "pages/mle.html#likelihood-estimation-for-bernoulli-distributions",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Likelihood Estimation for Bernoulli Distributions",
    "text": "Likelihood Estimation for Bernoulli Distributions\nApplying the likelihood function on the aforementioned dataset, we obtain: \\[\\begin{align*}\n\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= P(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;p)\\\\\n&= p(\\mathbf{x}_1;p)p(\\mathbf{x}_2;p)\\ldots p(\\mathbf{x}_n;p) \\\\\n&=\\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}}\n\\end{align*}\\] \\[\\begin{align*}\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &=\\underset{p} {\\arg \\max}\\log \\left ( \\prod _{i=1} ^n {p^{\\mathbf{x}_i}(1-p)^{1-\\mathbf{x}_i}} \\right ) \\\\\n\\text{Differentiating wrt $p$, we get}\\\\\n\\therefore \\hat{p}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/mle.html#likelihood-estimation-for-gaussian-distributions",
    "href": "pages/mle.html#likelihood-estimation-for-gaussian-distributions",
    "title": "Maximum likelihood estimation (MLE)",
    "section": "Likelihood Estimation for Gaussian Distributions",
    "text": "Likelihood Estimation for Gaussian Distributions\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2)\\). We assume that the data points are independent and identically distributed.\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}) &= f_{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n  f_{\\mathbf{x}_i}(\\mathbf{x}_i;\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\\\\n&=\\prod _{i=1} ^n \\left [ \\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}} e^{\\frac{-(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2}} \\right ] \\\\\n\\therefore \\log(\\mathcal{L}(p;\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})) &= \\sum _{i=1} ^n \\left[ \\log \\left (\\frac{1}{\\sqrt{2\\pi}\\boldsymbol{\\sigma}}  \\right ) - \\frac{(\\mathbf{x}_i-\\boldsymbol{\\mu})^2}{2\\boldsymbol{\\sigma}^2} \\right] \\\\\n\\end{align*}\\] \\[\n\\text{By differentiating with respect to $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, we get}\n\\] \\[\\begin{align*}\n\\hat{\\boldsymbol{\\mu}}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n \\mathbf{x}_i \\\\\n\\hat{\\boldsymbol{\\sigma}^2}_{\\text{ML}} &= \\frac{1}{n}\\sum _{i=1} ^n (\\mathbf{x}_i-\\boldsymbol{\\mu})^T(\\mathbf{x}_i-\\boldsymbol{\\mu})\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/kmeans.html",
    "href": "pages/kmeans.html",
    "title": "K Means",
    "section": "",
    "text": "Clustering represents an essential method in unsupervised machine learning aimed at grouping similar objects into clusters, thereby revealing inherent structures within the data for exploratory analysis or serving as a preprocessing step for subsequent algorithms.\nOur primary objective is to partition a set of \\(n\\) datapoints into \\(k\\) clusters.\nNotation: \\[\n\\mathbf{X}=\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n \\} \\quad \\mathbf{x}_i \\in \\mathbb{R}^d\n\\] \\[\nS = \\{\\mathbf{z} \\quad \\forall \\mathbf{z} \\in \\{1, 2, \\ldots k\\}^n \\}\n\\] \\[\n\\boldsymbol{\\mu} _k = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i=k)}}\n\\]\nWhere:\n\n\\(\\mathbf{x}_i\\) denotes the \\(i^{th}\\) datapoint.\n\\(z_i\\) denotes the cluster indicator of \\(\\mathbf{x}_i\\).\n\\(\\boldsymbol{\\mu}_{z_i}\\) denotes the mean of the cluster with indicator \\(z_i\\).\n\\(S\\) represents the set of all possible cluster assignments. It is important to note that \\(S\\) is finite (\\(k^n\\)).\n\nGoal: \\[\n\\min _{\\mathbf{z} \\in S} \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||}^2\n\\]\nHowever, the manual solution to this optimization problem is classified as an NP-Hard problem, which necessitates considering alternative approaches to approximate its solution due to computational constraints."
  },
  {
    "objectID": "pages/kmeans.html#the-algorithm",
    "href": "pages/kmeans.html#the-algorithm",
    "title": "K Means",
    "section": "The Algorithm",
    "text": "The Algorithm\nThe algorithm proceeds as follows:\nStep 1: Initialization: Randomly assign datapoints from the dataset as the initial cluster centers.\nStep 2: Reassignment Step: \\[\nz _i ^{t} = \\underset{k}{\\arg \\min} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{k} ^t ||}_2 ^2 \\hspace{2em} \\forall i\n\\]\nStep 3: Compute Means: \\[\n\\boldsymbol{\\mu} _k ^{t+1} = \\frac{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbf{x}_i \\cdot \\mathbb{1}(z_i^t=k)}}{\\displaystyle \\sum _{i = 1} ^{n} {\\mathbb{1}(z_i^t=k)}} \\hspace{2em} \\forall k\n\\]\nStep 4: Loop until Convergence: Repeat steps 2 and 3 until the cluster assignments do not change."
  },
  {
    "objectID": "pages/PCA.html",
    "href": "pages/PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Representation learning is a fundamental sub-field of machine learning that is concerned with acquiring meaningful and compact representations of intricate data, facilitating various tasks such as dimensionality reduction, clustering, and classification.\nLet us consider a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\), where each \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\). The objective is to find a representation that minimizes the reconstruction error.\nWe can start by seeking the best linear representation of the dataset, denoted by \\(\\mathbf{w}\\), subject to the constraint \\(||\\mathbf{w}||=1\\).\nThe representation is given by, \\[\\begin{align*}\n    \\frac{(\\mathbf{x}_i^T\\mathbf{w})}{\\mathbf{w}^T\\mathbf{w}}&\\mathbf{w} \\\\\n    \\text{However, }||\\mathbf{w}||&=1\\\\\n    \\therefore \\text{ Projection } &= (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}\n\\end{align*}\\]\nThe reconstruction error is computed as follows, \\[\n\\text{Reconstruction Error}(f(\\mathbf{w})) = \\frac{1}{n} \\sum _{i=1} ^{n} || \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w} || ^ 2\n\\] where \\(\\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}\\) is termed the residue and can be represented as \\(\\mathbf{x}'\\).\nThe primary aim is to minimize the reconstruction error, leading to the following optimization formulation: \\[\\begin{align*}\n    \\min _{\\mathbf{w} \\in ||\\mathbf{w}|| = 1} f(\\mathbf{w}) &= \\frac{1}{n} \\sum _{i=1} ^{n} -(\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n    \\therefore \\max _{\\mathbf{w} \\in ||\\mathbf{w}|| = 1} f(\\mathbf{w}) &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n    &= \\mathbf{w}^T(\\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\\mathbf{x}_i^T)\\mathbf{w} \\\\\n    \\max _{\\mathbf{w} \\in ||\\mathbf{w}|| = 1} f(\\mathbf{w}) &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w}\n\\end{align*}\\] where \\(\\mathbf{C}=\\displaystyle \\frac{1}{n} \\displaystyle \\sum _{i=1} ^{n} \\mathbf{x}_i\\mathbf{x}_i^T\\) represents the Covariance Matrix, and \\(\\mathbf{C} \\in \\mathbb{R}^{d \\times d}\\).\nNotably, the eigenvector \\(\\mathbf{w}\\) corresponding to the largest eigenvalue \\(\\lambda\\) of \\(\\mathbf{C}\\) becomes the sought-after solution for the representation. This \\(\\mathbf{w}\\) is often referred to as the First Principal Component of the dataset.\n\n\nBased on the above concepts, we can outline the following algorithm for representation learning:\nGiven a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset: \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) with \\(||\\mathbf{w}|| = 1\\).\nUpdate the dataset with the representation: \\[\n\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}  \\hspace{1em} \\forall i\n\\]\nRepeat steps 2 and 3 until the residues become zero, resulting in \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nThe question arises: Is this the most effective approach, and how many \\(\\mathbf{w}\\) do we need to achieve optimal compression?"
  },
  {
    "objectID": "pages/PCA.html#potential-algorithm",
    "href": "pages/PCA.html#potential-algorithm",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Based on the above concepts, we can outline the following algorithm for representation learning:\nGiven a dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\),\n\nCenter the dataset: \\[\n\\mathbf{\\mu} = \\frac{1}{n} \\sum _{i=1} ^{n} \\mathbf{x}_i\n\\] \\[\n\\mathbf{x}_i = \\mathbf{x}_i - \\mathbf{\\mu}  \\hspace{2em} \\forall i\n\\]\nFind the best representation \\(\\mathbf{w} \\in \\mathbb{R}^d\\) with \\(||\\mathbf{w}|| = 1\\).\nUpdate the dataset with the representation: \\[\n\\mathbf{x}_i = \\mathbf{x}_i - (\\mathbf{x}_i^T\\mathbf{w})\\mathbf{w}  \\hspace{1em} \\forall i\n\\]\nRepeat steps 2 and 3 until the residues become zero, resulting in \\(\\mathbf{w}_2, \\mathbf{w}_3, \\ldots, \\mathbf{w}_d\\).\n\nThe question arises: Is this the most effective approach, and how many \\(\\mathbf{w}\\) do we need to achieve optimal compression?"
  },
  {
    "objectID": "pages/PCA.html#approximate-representation",
    "href": "pages/PCA.html#approximate-representation",
    "title": "Principal Component Analysis",
    "section": "Approximate Representation",
    "text": "Approximate Representation\nThe question arises: If the data can be approximately represented by a lower-dimensional subspace, would it suffice to use only those \\(k\\) projections? Additionally, how much variance should be covered?\nLet us consider a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\). Let \\(\\mathbf{C}\\) represent its covariance matrix, and \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the corresponding eigenvalues, which are non-negative due to the positive semi-definiteness of the covariance matrix. These eigenvalues are arranged in descending order, with \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) as their corresponding eigenvectors of unit length.\nThe eigen equation for the covariance matrix can be expressed as follows: \\[\\begin{align*}\n    \\mathbf{C}\\mathbf{w} &= \\lambda \\mathbf{w} \\\\\n    \\mathbf{w}^T\\mathbf{C}\\mathbf{w} &= \\mathbf{w}^T\\lambda \\mathbf{w}\\\\\n    \\therefore \\lambda &= \\mathbf{w}^T\\mathbf{C}\\mathbf{w} \\hspace{2em} \\{\\mathbf{w}^T\\mathbf{w} = 1\\} \\\\\n    \\lambda &= \\frac{1}{n} \\sum _{i=1} ^{n} (\\mathbf{x}_i^T\\mathbf{w})^2 \\\\\n\\end{align*}\\]\nHence, the mean of the dataset being zero, \\(\\lambda\\) represents the variance captured by the eigenvector \\(\\mathbf{w}\\).\nA commonly accepted heuristic suggests that PCA should capture at least 95% of the variance. If the first \\(k\\) eigenvectors capture the desired variance, it can be stated as: \\[\n\\frac{\\displaystyle \\sum _{j=1} ^{k} \\lambda_j}{\\displaystyle \\sum _{i=1} ^{d} \\lambda_i} \\ge 0.95\n\\]\nThus, the higher the variance captured, the lower the error incurred."
  },
  {
    "objectID": "pages/PCA.html#p.c.a.-algorithm",
    "href": "pages/PCA.html#p.c.a.-algorithm",
    "title": "Principal Component Analysis",
    "section": "P.C.A. Algorithm",
    "text": "P.C.A. Algorithm\nThe Principal Component Analysis algorithm can be summarized as follows for a centered dataset \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^{d}\\), and \\(\\mathbf{C}\\) represents its covariance matrix:\n\nStep 1: Find the eigenvalues and eigenvectors of \\(\\mathbf{C}\\). Let \\(\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_d \\}\\) be the eigenvalues arranged in descending order, and \\(\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_d \\}\\) be their corresponding eigenvectors of unit length.\nStep 2: Calculate \\(k\\), the number of top eigenvalues and eigenvectors required, based on the desired variance to be covered.\nStep 3: Project the data onto the eigenvectors and obtain the desired representation as a linear combination of these projections.\n\n\n\n\nThe dataset depicted in the diagram has two principal components: the green vector represents the first PC, whereas the red vector corresponds to the second PC.\n\n\nIn essence, PCA is a dimensionality reduction technique that identifies feature combinations that are de-correlated (independent of each other)."
  },
  {
    "objectID": "pages/intro.html",
    "href": "pages/intro.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "What is Machine Learning\nMachine learning (ML) is a field of computer science that gives computers the ability to learn without being explicitly programmed. ML algorithms are able to learn from data and make predictions or decisions without being explicitly told how to do so. This makes ML a powerful tool for automating tasks, making predictions, and uncovering patterns in data.\n\n\nWhy Machine Learning?\nThere are many reasons why machine learning is becoming increasingly popular. Some of the key benefits of ML include:\n\nAutomation: ML can be used to automate tasks that would otherwise require human intelligence. This can free up human workers to focus on more creative and strategic tasks.\nAccuracy: ML algorithms can often make more accurate predictions than humans. This is because ML algorithms can learn from large amounts of data and identify patterns that humans would not be able to see.\nScalability: ML algorithms can be scaled to handle large amounts of data. This makes them ideal for applications such as fraud detection, natural language processing, and image recognition.\n\n\n\nWhere is Machine Learning Used?\nMachine learning is used in a wide variety of fields, including:\n\nFinance: ML is used to predict stock prices, identify fraud, and manage risk.\nHealthcare: ML is used to diagnose diseases, develop new treatments, and personalize care.\nRetail: ML is used to personalize recommendations, predict demand, and prevent fraud.\nManufacturing: ML is used to optimize production, improve quality control, and predict maintenance needs.\nLogistics: ML is used to optimize shipping routes, manage inventory, and predict demand.\n\n\n\nTypes of Machine Learning\nThere are many different types of machine learning, each with its own strengths and weaknesses. Some of the most common types of machine learning include:\n\nSupervised learning: In supervised learning, the algorithm is trained on a dataset of labeled data. This means that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data.\nUnsupervised learning: In unsupervised learning, the algorithm is trained on a dataset of unlabeled data. This means that the data only includes inputs and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance.\nReinforcement learning: In reinforcement learning, the algorithm is trained by trial and error. The algorithm is given a goal and a set of actions that it can take. The algorithm then tries different actions and observes the results. Over time, the algorithm learns which actions are most likely to lead to the desired goal.\n\n\n\nThe Future of Machine Learning\nMachine learning is a rapidly growing field with the potential to revolutionize many industries. As ML algorithms become more powerful and sophisticated, they will be able to automate more tasks, make more accurate predictions, and uncover more patterns in data. This will lead to new and innovative applications in a wide variety of fields."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Machine Learning Algorithms",
    "section": "",
    "text": "Warning\n\n\n\nThe site is not complete and is still under development.\nThis website is a comprehensive resource for machine learning algorithms. Here, you will find explanations of the most popular algorithms, as well as code implementations in Python.\nThe website is divided into two main sections: supervised learning and unsupervised learning. Supervised learning algorithms are used to learn from labeled data, while unsupervised learning algorithms are used to learn from unlabeled data.\nWithin each section, the algorithms are further divided into regression, classification, and clustering. Regression algorithms are used to predict continuous values, while classification algorithms are used to predict discrete values. Clustering algorithms are used to group data points together.\nEach algorithm page includes the following information:\nI hope you find this website to be a valuable resource for learning about machine learning algorithms.\nHere is a more detailed overview of the algorithms covered on this website:"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Welcome to Machine Learning Algorithms",
    "section": "Credits",
    "text": "Credits\nI would like to express my gratitude to Professor Arun Rajkumar for his valuable content and notations, which have greatly influenced my understanding. Additionally, I would like to acknowledge the contribution of IIT Madras, where I had the opportunity to learn from Professor Arun Rajkumar in the Machine Learning Techniques Course."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there, I’m Sherry Thomas, and I’m passionate about making a difference in the field of data science. I have a Bachelor’s degree in Philosophy from Christ University, Bengaluru and I’m currently pursuing a Bachelor’s degree in Data Science and Applications at IIT-Madras.\nI have a diverse academic background that includes philosophy, programming, computer science, and statistics. My philosophy studies have helped me develop critical thinking and problem-solving skills, while my data science coursework has provided me with the technical expertise needed to tackle complex challenges.\nI believe in taking a holistic approach to data analysis and decision-making, considering the ethical and societal implications of technology and data. My goal is to use my knowledge and skills to make a positive impact in the field of data science.\nI’m constantly learning and growing, and I’m committed to expanding my knowledge base and contributing to the ever-evolving world of technology and data."
  },
  {
    "objectID": "pages/svm.html",
    "href": "pages/svm.html",
    "title": "Support vector machines (SVMs)",
    "section": "",
    "text": "Let dataset \\(D=\\{(x_1, y_1), \\ldots, (x_n,y_n)\\}\\) be linearly separable with \\(\\gamma\\)-margin where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{-1, 1\\}\\).\nLet \\(w* \\in \\mathbb{R}^d\\) be the weight vector s.t. \\((w^{*T}x_i)y_i\\ge\\gamma\\) \\(\\forall i\\).\nLet some \\(R&gt;0 \\in \\mathbb{R}\\), s.t. \\(\\forall i\\) \\(||x_i||\\le R\\).\nTherefore, the number of mistakes made by the algorithm is given by, \\[\n\\text{\\#mistakes} \\le \\frac{R^2}{\\gamma^2}\n\\]\nObservations\nLet \\(w_{perc}\\) be any weight vector which can linearly separate the dataset.\nTherefore, we observe the following:\n\n“Quality” of the solution depends on the margin.\nNumber of mistakes depend on \\(w^*\\)’s margin.\n\\(w_{perc}\\) need not necessarily be \\(w^*\\).\n\nHence, our goal should be to find the solution that maximizes the margin.\n\n\nFrom the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\)."
  },
  {
    "objectID": "pages/svm.html#margin-maximization",
    "href": "pages/svm.html#margin-maximization",
    "title": "Support vector machines (SVMs)",
    "section": "",
    "text": "From the previous analysis, it is clear that a single dataset could have multiple linear classifiers with varying margins. The following diagram illustrates this phenomenon,\n\n\n\nMultiple Classifiers\n\n\nTherefore, for getting the best classifier, our goal can be written as, \\[\n\\max_{w,\\gamma} \\gamma\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge \\gamma \\hspace{1em} \\forall i \\\\\n||w||^2 &= 1\\\n\\end{align*}\\]\\end{align*} The boundary of the margin is given by, \\[\\begin{align*}\n\\{x:(w^Tx_i)y_i &= \\gamma\\}\\\\\n\\{x:(\\frac{w}{\\gamma}^Tx_i)y_i &= 1\\}\\\\\n\\end{align*}\\] From the above equation, we can see that \\(\\gamma\\) depends on the width of \\(w\\). Therefore, we reformulate our goal as, \\[\n\\max_{w} \\text{width}(w)\n\\] \\[\\begin{align*}\ns.t. (w^Tx_i)y_i &\\ge 1 \\hspace{1em} \\forall i \\\\\n\\end{align*}\\] Let the width be the distance between the two parallel margins, and let \\(x\\) and \\(z\\) be two points who are on the two lines exactly opposite to each other s.t. \\(w^Tx=-1\\) and \\(w^Tz=1\\) or vice versa.\nLet \\(x_1\\) and \\(x_2\\) be two points which lie on opposite side of the decision boundary as well as on the margins.\n\n\n\nMargin Width\n\n\nTherefore, the width is given by, \\[\\begin{align*}\n||x_1^Tw - x_2^Tw||_2^2 &= 2 \\\\\n||x_1-x_2||_2^2||w||^2_2 &= 2\\\\\n\\therefore ||x_1 - x_2||^2_2 &= \\frac{2}{||w||^2_2}\n\\end{align*}\\]\nTherefore, our objective function can be written as, \\[\n\\max_{w}  \\frac{2}{||w||^2_2} \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Equivalently, \\[\n\\min_{w}  \\frac{1}{2}||w||^2_2 \\hspace{1em} s.t. (w^Tx_i)y_i \\ge 1 \\hspace{1em} \\forall i\n\\] Therefore ﬁnding the separating hyperplane with maximum margin is equivalent to ﬁnding the one with the smallest possible normal vector \\(w\\)."
  },
  {
    "objectID": "pages/svm.html#hard-margin-svm-algorithm",
    "href": "pages/svm.html#hard-margin-svm-algorithm",
    "title": "Support vector machines (SVMs)",
    "section": "Hard-Margin SVM Algorithm",
    "text": "Hard-Margin SVM Algorithm\nThis algorithm only works if the dataset is linearly separable with a \\(\\gamma &gt; 0\\).\n\nCalculate \\(Q=X^TX\\) directly or using a kernel as per the dataset.\nUse the gradient of the dual formula (\\(\\alpha^T1 - \\frac{1}{2}\\alpha^TY^TQY\\alpha\\)), in the gradient descent algorithm to find a satisfactory \\(\\alpha\\). Let the intial \\(\\alpha\\) be a zero vector \\(\\in \\mathbb{R}^n_+\\).\nTo predict:\n\nFor non-kernelized SVM: \\(\\text{label}(x_{test}) = w^Tx_{test} = \\sum _{i=1} ^n \\alpha _i y_i(x_i^Tx_{test})\\)\nFor kernelized SVM: \\(\\text{label}(x_{test}) = w^T\\phi(x_{test}) = \\sum _{i=1} ^n \\alpha _i y_ik(x_i^Tx_{test})\\)"
  },
  {
    "objectID": "pages/svm.html#dual-formulation",
    "href": "pages/svm.html#dual-formulation",
    "title": "Support vector machines (SVMs)",
    "section": "Dual Formulation",
    "text": "Dual Formulation\nMaximizing the Lagrangian function w.r.t. \\(\\alpha\\) and \\(\\beta\\), and minimizing it w.r.t. \\(w\\) and \\(\\epsilon\\), we get,\n\\[\n\\min _{w, \\epsilon}\\left [\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\nThe dual of this is given by,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) \\right ]\n\\]\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0}\\left [\\min _{w, \\epsilon}\\mathcal{L}(w, \\epsilon, \\alpha, \\beta) \\right ] \\quad \\ldots[1]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(w\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get, \\[\n\\frac{d\\mathcal{L}}{dw}  = 0\n\\] \\[\n\\frac{d}{dw} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\\\\\n\\] \\[\nw_{\\alpha, \\beta}^* - \\alpha_ix_iy_i = 0\n\\] \\[\n\\therefore w_{\\alpha, \\beta}^* = \\alpha_ix_iy_i \\quad \\ldots [2]\n\\]\nDifferentiating the above function\\([1]\\) w.r.t. \\(\\epsilon_i \\forall i\\) while fixing \\(\\alpha\\) and \\(\\beta\\), we get,\n\\[\n\\frac{\\partial\\mathcal{L}}{\\partial\\epsilon_i}  = 0\n\\] \\[\n\\frac{\\partial}{\\partial\\epsilon_i} \\frac{1}{2}||w||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-(w^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n \\beta(-\\epsilon_i) = 0\n\\] \\[\nC - \\alpha_i -\\beta_i = 0\n\\] \\[\n\\therefore C = \\alpha_i + \\beta_i \\quad \\ldots [3]\n\\]\nSubstituting the values of \\(w\\) and \\(\\beta\\) from \\([2]\\) and \\([3]\\) in \\([1]\\), we get,\n\\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}||\\alpha_ix_iy_i||^2_2 + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i(1-((\\alpha_ix_iy_i)^Tx_i)y_i - \\epsilon_i) + \\sum _{i=1} ^n (C-\\alpha_i)(-\\epsilon_i) \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i + C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i-\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i - \\sum _{i=1} ^n \\alpha_i\\epsilon_i - C\\sum _{i=1} ^n\\epsilon_i + \\sum _{i=1} ^n \\alpha_i\\epsilon_i \\right ]\n\\] \\[\n\\max _{\\alpha \\ge 0; \\beta \\ge 0; C = \\alpha_i + \\beta_i}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] \\[\n\\therefore \\max _{0 \\le \\alpha \\le C}\\left [\\sum _{i=1} ^n \\alpha_i - \\frac{1}{2}\\alpha_i^Tx_i^Ty_i^Ty_ix_i\\alpha_i\\right ]\n\\] If \\(C=0\\), \\(\\alpha^*=0\\) \\(\\rightarrow\\) \\(\\displaystyle w^*=\\sum _{i=1} ^n \\alpha^*_i x_i y_i = 0\\).\nif \\(C=\\infty\\), the equation will be equal to that of Hard-Margin SVM."
  },
  {
    "objectID": "pages/svm.html#complementary-slackness-in-soft-margin-support-vector-machines",
    "href": "pages/svm.html#complementary-slackness-in-soft-margin-support-vector-machines",
    "title": "Support vector machines (SVMs)",
    "section": "Complementary Slackness in Soft-Margin Support Vector Machines",
    "text": "Complementary Slackness in Soft-Margin Support Vector Machines\nComplementary slackness can be expressed through a set of equations as follows:\n\n\\(\\forall i\\), the following equation holds: \\[\n\\alpha_i(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i) = 0 \\quad \\ldots [1]\n\\]\nAdditionally, the following relation must hold: \\[\n\\beta_i(\\epsilon_i^*) = 0 \\quad \\ldots [2]\n\\]\n\nGiven these equations, various scenarios arise which elucidate the implications of complementary slackness in the context of Soft-Margin SVM.\n\nScenarios of Complementary Slackness\n\nIf \\(\\alpha_i^* = 0\\):\nIn this case, \\(\\beta_i^* = C\\) due to \\(\\alpha_i^* + \\beta_i^* = C\\). From equation [2], we deduce that \\(\\epsilon_i^* = 0\\). Consequently, it follows that the decision function \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i + \\epsilon_i^* \\ge 1\\) simplifies to \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i \\ge 1\\), indicating that the data point is correctly classified and lies outside the margin.\nIf \\(0 &lt; \\alpha_i^* &lt; C\\):\nIn this scenario, both \\(\\alpha_i^*\\) and \\(\\beta_i^*\\) assume positive values within the range \\((0, C)\\). Equation [2] implies \\(\\epsilon_i^* = 0\\), while equation [1] gives \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i = 0\\), leading to \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\). This signifies that the data point resides exactly on the margin boundary.\nIf \\(\\alpha_i^* = C\\):\nWhen \\(\\alpha_i^*\\) takes on the maximum value \\(C\\), the dual variable \\(\\beta_i^*\\) becomes \\(0\\). Equation [2] establishes that \\(\\epsilon_i^*\\ge0\\). Equation [1] implies \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i = 0\\), which further simplifies to \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i = \\epsilon^*_i\\). This results in \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i \\ge 0\\), indicating that the data point is misclassified or lies within the margin.\nIf \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &lt; 1\\):\nGiven that \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &lt; 1\\), the inequality \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i + \\epsilon_i^* \\ge 1\\) holds. By manipulating this inequality, we deduce \\(\\epsilon_i^* &gt; 0\\). From equation [2], it follows that \\(\\beta_i^* = 0\\), and consequently, \\(\\alpha_i^* = C\\).\nIf \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\):\nFor data points that satisfy \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\), we once again consider the inequality \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i + \\epsilon_i^* \\ge 1\\), leading to \\(\\epsilon_i^* \\ge 0\\). If \\(\\epsilon_i^* &gt; 0\\), equation [2] indicates \\(\\beta_i^* = 0\\), resulting in \\(\\alpha_i^* = C\\). On the other hand, if \\(\\epsilon_i^* = 0\\), equation [2] implies \\(\\beta_i^* \\ge 0\\), and as a consequence, \\(\\alpha_i^* \\in [0, C]\\).\nIf \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &gt; 1\\):\nWhen \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &gt; 1\\), the inequality \\(1 - \\mathbf{w}^{*T}\\mathbf{x}_i y_i - \\epsilon^*_i &lt; 0\\) holds. From equation [1], we conclude that \\(\\alpha_i^* = 0\\), leading to \\(\\beta^*_i = C\\) as per the equation \\(\\alpha_i^* + \\beta_i^* = C\\). Furthermore, equation [2] reveals that \\(\\epsilon_i^* = 0\\).\n\n\n\nSummary of Scenarios\nIn summary, we have established the following relationships between the dual variable \\(\\alpha_i^*\\) and the decision boundary defined by \\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i\\):\n\n\\(\\alpha_i^* = 0\\): Data point is correctly classified and outside the margin.\n\\(0 &lt; \\alpha_i^* &lt; C\\): Data point lies exactly on the margin boundary.\n\\(\\alpha_i^* = C\\): Data point is misclassified or inside the margin.\n\\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &lt; 1\\): Dual variable \\(\\alpha_i^* = C\\).\n\\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i = 1\\): Dual variable \\(\\alpha_i^* \\in [0, C]\\) if \\(\\epsilon_i^* = 0\\), else \\(\\alpha_i^* = C\\).\n\\((\\mathbf{w}^{*T}\\mathbf{x}_i)y_i &gt; 1\\): Dual variable \\(\\alpha_i^* = 0\\)."
  },
  {
    "objectID": "pages/Kernel_PCA.html",
    "href": "pages/Kernel_PCA.html",
    "title": "Kernel PCA",
    "section": "",
    "text": "In the context of a given dataset \\(\\mathbf{X} \\in \\mathbb{R}^{d \\times n}\\), where \\(\\mathbf{C} \\in \\mathbb{R}^{d \\times d}\\) represents the covariance matrix, Principal Component Analysis (PCA) encounters two critical challenges:\n\nTime Complexity: Computing the eigenvalues and eigenvectors of \\(\\mathbf{C}\\) requires an algorithmic complexity of \\(O(d^3)\\). Consequently, as the dimensionality \\(d\\) increases, the computational time becomes prohibitively large.\nNon-Linear Dataset: In situations where the dataset resides in a non-linear subspace, PCA’s attempt to derive linear combinations of Principal Components may yield suboptimal results.\n\nTo address these issues, we propose methods for reducing the time complexity of finding eigenvalues and eigenvectors and for handling non-linear relationships in PCA."
  },
  {
    "objectID": "pages/Kernel_PCA.html#transforming-features",
    "href": "pages/Kernel_PCA.html#transforming-features",
    "title": "Kernel PCA",
    "section": "Transforming Features",
    "text": "Transforming Features\nTo address non-linear relationships, we propose mapping the dataset to higher dimensions as follows: \\[\n\\mathbf{x} \\to \\phi(\\mathbf{x}) \\quad \\mathbb{R}^d \\to \\mathbb{R}^D \\quad \\text{where } [D &gt;&gt; d]\n\\]\nTo compute \\(D\\), let \\(\\mathbf{x}=\\left [ \\begin{array} {cc}  f_1 & f_2 \\end{array} \\right ]\\) represent features of a dataset containing datapoints lying on a second-degree curve in a two-dimensional space.\nTo convert it from quadratic to linear, we map the features to: \\[\n\\phi(\\mathbf{x})=\\left [\n\\begin{array} {cccccc}\n    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2\n\\end{array}\n\\right ]\n\\]\nMapping \\(d\\) features to the polynomial power \\(p\\) results in \\(^{d+p} C_d\\) new features.\nHowever, it is essential to note that finding \\(\\phi(\\mathbf{x})\\) may be computationally demanding.\nTo overcome this issue, we present the solution in the subsequent section."
  },
  {
    "objectID": "pages/Kernel_PCA.html#kernel-functions",
    "href": "pages/Kernel_PCA.html#kernel-functions",
    "title": "Kernel PCA",
    "section": "Kernel Functions",
    "text": "Kernel Functions\nA function \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is considered a “valid” Kernel Function if it maps data points to the real numbers.\nProof of a “Valid” Kernel: There are two methods to establish the validity of a kernel:\n\nMethod 1: Explicitly exhibit the mapping to \\(\\phi\\), which may be challenging in certain cases.\nMethod 2: Utilize Mercer’s Theorem, which states that \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) is a valid kernel if and only if:\n\n\\(k\\) is symmetric, i.e., \\(k(\\mathbf{x},\\mathbf{x}') = k(\\mathbf{x}',\\mathbf{x})\\)\nFor any dataset \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\), the matrix \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), where \\(\\mathbf{K}_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)\\), is Positive Semi-Definite.\n\n\nTwo popular kernel functions are:\n\nPolynomial Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x} + 1)^p\\)\nRadial Basis Function Kernel or Gaussian Kernel: \\(k(\\mathbf{x},\\mathbf{x}') = \\exp\\left(\\displaystyle-\\frac{\\|\\mathbf{x}-\\mathbf{x}'\\|^2}{2\\sigma^2}\\right)\\)"
  },
  {
    "objectID": "pages/kkmeans.html",
    "href": "pages/kkmeans.html",
    "title": "Kernel K-means",
    "section": "",
    "text": "Smart Initialization - K-means++\nThe concept of K-means++ involves selecting centroids that are maximally distant from each other.\n\nStep 1: Randomly select \\(\\boldsymbol{\\mu} _1 ^0\\) from the dataset.\nStep 2: For \\(l \\in \\{2, 3, \\ldots, k\\}\\), choose \\(\\boldsymbol{\\mu} _l ^0\\) probabilistically proportional to the score(\\(S\\)), where \\(S\\) is defined as follows: \\[\n  S(\\mathbf{x}_i) = \\min _{\\{j=1, 2, \\ldots, l-1\\}} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{j} ^0 ||}^2 \\quad \\forall \\mathbf{x}_i \\in \\mathbf{X}\n\\] The probabilistic aspect of the algorithm provides an expected guarantee of optimal convergence in K-means. The guarantee is given by: \\[\n  \\mathbb{E} \\left[ \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||}^2 \\right ]\n  \\le O(\\log k) \\left [ \\min _{\\{z_1, z_2, \\ldots, z_n\\}} \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||}^2 \\right ]\n\\] where \\(O(\\log k)\\) is a constant of order \\(\\log k\\).\nStep 3: Once the centroids are determined, we proceed with our usual Lloyd’s Algorithm.\n\n\n\nChoice of K\nA prerequisite for K-means is determining the number of clusters, denoted as \\(k\\). However, what if the value of \\(k\\) is unknown?\nIf we were to choose \\(k\\) to be equal to \\(n\\): \\[\nF(z_1, z_2, \\dots, z_n) = \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||} ^2 = 0\n\\] However, as having as many clusters as datapoints is undesirable, we aim to minimize \\(k\\) while penalizing large values of \\(k\\). \\[\n\\underset{k}{\\arg \\min} \\left [ \\sum _{i=1} ^{n} {|| \\mathbf{x}_i - \\boldsymbol{\\mu} _{z_i} ||} ^2 + \\text{Penalty}(k) \\right ]\n\\] Two common criteria for making the above argument are:\n\n\\(\\text{\\textcolor{blue}{Akaike Information Criterion}}\\): \\(\\left [ 2K - 2\\ln(\\hat{\\mathcal{L}}(\\theta ^*)) \\right ]\\)\n\\(\\text{\\textcolor{blue}{Bayesian Information Criterion}}\\): \\(\\left [ K\\ln(n) - 2\\ln(\\hat{\\mathcal{L}}(\\theta ^*)) \\right ]\\)\n\nHowever, detailed elaboration of these criteria is not discussed here."
  },
  {
    "objectID": "pages/bayes_est.html",
    "href": "pages/bayes_est.html",
    "title": "Bayesian estimation",
    "section": "",
    "text": "Bayesian estimation is a statistical method that updates parameter estimates by incorporating prior knowledge or beliefs along with observed data to calculate the posterior probability distribution of the parameters.\nLet \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\) be a dataset where \\(\\mathbf{x}_i\\) follows a distribution with parameters \\(\\boldsymbol{\\theta}\\). We assume that the data points are independent and identically distributed, and we also consider \\(\\boldsymbol{\\theta}\\) as a random variable with its own probability distribution.\nOur objective is to update the parameters using the available data.\ni.e. \\[\nP(\\boldsymbol{\\theta})\\Rightarrow P(\\boldsymbol{\\theta}|\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})\n\\] where, employing Bayes’ Law, we find \\[\nP(\\boldsymbol{\\theta}|\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})=\\left ( \\frac{P(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}|\\boldsymbol{\\theta})}{P(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\})} \\right )*P(\\boldsymbol{\\theta})\n\\]"
  },
  {
    "objectID": "pages/bayes_est.html#bayesian-estimation-for-a-bernoulli-distribution",
    "href": "pages/bayes_est.html#bayesian-estimation-for-a-bernoulli-distribution",
    "title": "Bayesian estimation",
    "section": "Bayesian Estimation for a Bernoulli Distribution",
    "text": "Bayesian Estimation for a Bernoulli Distribution\nLet \\(\\{x_1, x_2, \\ldots, x_n\\}\\) be a dataset where \\(x_i \\in \\{0,1\\}\\) with parameter \\(\\theta\\). What distribution can be suitable for \\(P(\\theta)\\)?\nA commonly used distribution for priors is the Beta Distribution. \\[\nf(p;\\alpha,\\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{z} \\hspace{2em} \\forall p \\in [0,1] \\\\\n\\] \\[\n\\text{where $z$ is a normalizing factor}\n\\]\nHence, utilizing the Beta Distribution as the Prior, we obtain, \\[\\begin{align*}\nP(\\theta|\\{x_1, x_2, \\ldots, x_n\\}) &\\propto P(\\theta|\\{x_1, x_2, \\ldots, x_n\\})*P(\\theta) \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto \\left [ \\prod _{i=1} ^n {p^{x_i}(1-p)^{1-x_i}} \\right ]*\\left [ p^{\\alpha-1}(1-p)^{\\beta-1} \\right ] \\\\\nf_{\\theta|\\{x_1, x_2, \\ldots, x_n\\}}(p) &\\propto p^{\\sum _{i=1} ^n x_i + \\alpha - 1}(1-p)^{\\sum _{i=1} ^n(1-x_i) + \\beta - 1}\n\\end{align*}\\] i.e. we obtain, \\[\n\\text{BETA PRIOR }(\\alpha, \\beta) \\xrightarrow[Bernoulli]{\\{x_1, x_2, \\ldots, x_n\\}} \\text{BETA POSTERIOR }(\\alpha + n_h, \\beta + n_t)\n\\] \\[\n\\therefore \\hat{p_{\\text{ML}}} = \\mathbb{E}[\\text{Posterior}]=\\mathbb{E}[\\text{Beta}(\\alpha +n_h, \\beta + n_t)]= \\frac{\\alpha + n_h}{\\alpha + n_h + \\beta + n_t}\n\\]"
  },
  {
    "objectID": "pages/em.html",
    "href": "pages/em.html",
    "title": "Expectation-maximization (EM) algorithm",
    "section": "",
    "text": "Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/em.html#convexity-and-jensens-inequality",
    "href": "pages/em.html#convexity-and-jensens-inequality",
    "title": "Expectation-maximization (EM) algorithm",
    "section": "",
    "text": "Convexity is a property of a function or set that implies a unique line segment can be drawn between any two points within the function or set. For a concave function, this property can be expressed as, \\[\nf \\left (\\sum _{k=1} ^K \\lambda_k a_k \\right ) \\ge \\sum _{k=1} ^K \\lambda_k f(a_k)\n\\] where \\[\n\\sum _{k=1} ^K \\lambda _k = 1\n\\] \\[\na_k \\text{ are points of the function}\n\\] This is also known as Jensen’s Inequality."
  },
  {
    "objectID": "pages/klin_reg.html",
    "href": "pages/klin_reg.html",
    "title": "Kernel Least squares regression",
    "section": "",
    "text": "What if the data points reside in a non-linear subspace? Similar to dealing with non-linear data clustering, kernel functions are employed in this scenario as well.\nLet \\(\\mathbf{w}^* = \\mathbf{X}\\boldsymbol{\\alpha}^*\\), where \\(\\boldsymbol{\\alpha}^* \\in \\mathbb{R}^n\\). \\[\\begin{align*}\n\\mathbf{X}\\boldsymbol{\\alpha}^* &= \\mathbf{w}^* \\\\\n\\therefore \\mathbf{X}\\boldsymbol{\\alpha}^* &= (\\mathbf{X}\\mathbf{X}^T)^+\\mathbf{X}\\mathbf{y} \\\\\n(\\mathbf{X}\\mathbf{X}^T)\\mathbf{X}\\boldsymbol{\\alpha}^* &= (\\mathbf{X}\\mathbf{X}^T)(\\mathbf{X}\\mathbf{X}^T)^+\\mathbf{X}\\mathbf{y} \\\\\n(\\mathbf{X}\\mathbf{X}^T)\\mathbf{X}\\boldsymbol{\\alpha}^* &= \\mathbf{X}\\mathbf{y} \\\\\n\\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)\\mathbf{X}\\boldsymbol{\\alpha}^* &= \\mathbf{X}^T\\mathbf{X}\\mathbf{y} \\\\\n(\\mathbf{X}^T\\mathbf{X})^2\\boldsymbol{\\alpha}^* &= \\mathbf{X}^T\\mathbf{X}\\mathbf{y} \\\\\n\\mathbf{K}^2\\boldsymbol{\\alpha}^* &= \\mathbf{K}\\mathbf{y} \\\\\n\\therefore \\boldsymbol{\\alpha}^* &= \\mathbf{K}^{-1}\\mathbf{y}\n\\end{align*}\\]\nHere, \\(\\mathbf{K} \\in \\mathbb{R}^{n \\times n}\\), and it can be obtained using a kernel function such as the Polynomial Kernel or RBF Kernel.\nTo predict using \\(\\boldsymbol{\\alpha}\\) and the kernel function, let \\(\\mathbf{X}_{\\text{test}} \\in \\mathbb{R}^{d \\times m}\\) represent the test dataset. The prediction is made as follows:\n\\[\\begin{align*}\n\\mathbf{w}^*\\phi(\\mathbf{X}_{\\text{test}}) &= \\sum_{i=1}^n \\alpha_i^* k(\\mathbf{x}_i, \\mathbf{x}_{\\text{test}_i})\n\\end{align*}\\]\nHere, \\(\\alpha_i^*\\) denotes the importance of the \\(i\\)-th data point in relation to \\(\\mathbf{w}^*\\), and \\(k(\\mathbf{x}_i, \\mathbf{x}_{\\text{test}_i})\\) signifies the similarity between \\(\\mathbf{x}_{\\text{test}_i}\\) and \\(\\mathbf{x}_i\\)."
  },
  {
    "objectID": "pages/rid_lin_reg.html",
    "href": "pages/rid_lin_reg.html",
    "title": "Ridge regression",
    "section": "",
    "text": "Ridge regression is a linear regression technique that addresses multicollinearity and overfitting by adding a penalty term to the ordinary least squares method.\nThe objective function of ridge regression is given by:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum^n_{i=1}(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 + \\lambda||\\mathbf{w}||_2^2\n\\]\nHere, \\(\\lambda||\\mathbf{w}||_2^2\\) serves as the regularization term, and \\(||\\mathbf{w}||_2^2\\) represents the squared L2 norm of \\(\\mathbf{w}\\). Let us denote this equation as \\(f(\\mathbf{w})\\).\nAlternatively, we can express ridge regression as:\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\sum^n_{i=1}(\\mathbf{w}^T\\mathbf{x}_i-y_i)^2 \\hspace{1em}\\text{s.t.}\\hspace{1em}||\\mathbf{w}||_2^2\\leq\\theta\n\\]\nHere, the value of \\(\\theta\\) depends on \\(\\lambda\\). We can conclude that for any choice of \\(\\lambda&gt;0\\), there exists a corresponding \\(\\theta\\) such that optimal solutions to our objective function exist.\nThe loss function for linear regression with the maximum likelihood estimator \\(\\mathbf{w}_{\\text{ML}}\\) is defined as:\n\\[\nf(\\mathbf{w}_{\\text{ML}}) = \\sum^n_{i=1}(\\mathbf{w}_{\\text{ML}}^T\\mathbf{x}_i-y_i)^2\n\\]\nConsider the set of all \\(\\mathbf{w}\\) such that \\(f(\\mathbf{w}_{\\text{ML}}) = f(\\mathbf{w}) + c\\), where \\(c&gt;0\\). This set can be represented as:\n\\[\nS_c = \\left \\{\\mathbf{w}: f(\\mathbf{w}_{\\text{ML}}) = f(\\mathbf{w}) + c \\right \\}\n\\]\nIn other words, every \\(\\mathbf{w} \\in S_c\\) satisfies the equation:\n\\[\n||\\mathbf{X}^T\\mathbf{w}-\\mathbf{y}||^2 = ||\\mathbf{X}^T\\mathbf{w}_{\\text{ML}}-\\mathbf{y}||^2 + c\n\\]\nSimplifying the equation yields:\n\\[\n(\\mathbf{w}-\\mathbf{w}_{\\text{ML}})^T(\\mathbf{X}\\mathbf{X}^T)(\\mathbf{w}-\\mathbf{w}_{\\text{ML}}) = c'\n\\]\nThe value of \\(c'\\) depends on \\(c\\), \\(\\mathbf{X}\\mathbf{X}^T\\), and \\(\\mathbf{w}_{\\text{ML}}\\), but it does not depend on \\(\\mathbf{w}\\).\n\n\n\nPictoral Representation of what Ridge Regression does.\n\n\nIn summary, ridge regression regularizes the feature values, pushing them towards zero, but not necessarily to zero."
  },
  {
    "objectID": "pages/knn.html",
    "href": "pages/knn.html",
    "title": "K-nearest neighbors (KNN)",
    "section": "",
    "text": "Binary classification is a fundamental task in machine learning, commonly employed in various domains such as computer vision, natural language processing, and bioinformatics. Its objective is to assign objects into one of two categories based on their features.\nConsider a dataset \\(\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\), where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\), and let \\(\\{y_1, \\ldots, y_n\\}\\) be the corresponding labels, where \\(y_i \\in \\{0, 1\\}\\). The goal is to find a function \\(h: \\mathbb{R}^d \\rightarrow \\{0, 1\\}\\) that accurately predicts the labels.\nTo assess the performance of the classification function, a loss measure is employed. The loss function is defined as follows:\n\\[\n\\text{loss}(h) = \\frac{1}{n} \\sum ^n _{i=1}\\mathbb{1}\\left ( h(\\mathbf{x}_i) \\ne y_i \\right )\n\\]\nLet \\(\\mathcal{H}_{\\text{linear}}\\) denote the solution space for the linear mapping:\n\\[\n\\mathcal{H}_{\\text{linear}}=\\left\\{\\mathbf{h}_w: \\mathbb{R}^d \\rightarrow \\{1, 0\\} \\hspace{0.5em} \\text{s.t.} \\hspace{0.5em} \\mathbf{h}_w(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^T\\mathbf{x}) \\hspace{0.5em} \\forall \\mathbf{w} \\in \\mathbb{R}^d \\right\\}\n\\]\nHence, the objective function can be expressed as:\n\\[\n\\min_{h \\in \\mathcal{H}_{\\text{linear}}} \\sum_{i=1}^n \\mathbb{1}\\left ( h(\\mathbf{x}_i) \\ne y_i \\right )\n\\]\nHowever, it is important to note that this objective function presents an NP-Hard problem, making it challenging to find optimal and sufficient parameters. Therefore, improved implementations are required to address this complexity and achieve satisfactory results."
  },
  {
    "objectID": "pages/knn.html#issues-with-k-nn",
    "href": "pages/knn.html#issues-with-k-nn",
    "title": "K-nearest neighbors (KNN)",
    "section": "Issues with K-NN",
    "text": "Issues with K-NN\nThe K-NN algorithm suffers from several limitations:\n\nThe choice of distance function can yield different results. The Euclidean distance, commonly used, might not always be the best fit for all scenarios.\nComputationally, the algorithm can be demanding. When making predictions for a single test data point, the distances between that data point and all training points must be calculated and sorted. Consequently, the algorithm has a complexity of \\(O(n \\log(n))\\), where \\(n\\) represents the size of the dataset.\nThe algorithm does not learn a model but instead relies on the training dataset for making predictions."
  },
  {
    "objectID": "pages/naive_bayes.html",
    "href": "pages/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "A Generative Model Based Algorithm is an approach that seeks to model the probability distribution of the input data and generate new samples based on this distribution. The key idea involves learning the joint probability distribution of the features and labels in the training data and utilizing this learned model to make predictions for previously unseen data.\nLet us consider a dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), where \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features using the labels obtained in Step 1 through the conditional probability \\(P(\\mathbf{x}_i|y_i)\\).\n\nThe parameters in the generative model are defined as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(2^d-1\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(2^d-1\\)\n\nConsequently, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + (2^d-1) + (2^d-1) \\\\\n    & = 1 + 2(2^d-1) \\\\\n    & = 2^{d+1}-1\n\\end{align*}\\]\nIssues:\n\nToo many parameters, which may lead to overfitting.\nThe model may not be practically viable due to the assumption made in the generative process.\n\n\n\nAn alternative generative model starts with the class conditional independence assumption, which is a common assumption in various machine learning algorithms. This assumption states that the features of an object are conditionally independent given its class label.\nLet us again consider the dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), with \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm under this alternative model are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features for \\(\\mathbf{x}\\) given \\(y\\) using the following conditional probability: \\[\nP(\\mathbf{x} = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^{y_i}_i)^{f_i}(1-p^{y_i}_i)^{1-f_i}\n\\]\n\nThe parameters in this alternative model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(d\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(d\\)\n\nThus, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + d + d \\\\\n    & = 2d + 1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/naive_bayes.html#alternate-generative-model",
    "href": "pages/naive_bayes.html#alternate-generative-model",
    "title": "Naive Bayes",
    "section": "",
    "text": "An alternative generative model starts with the class conditional independence assumption, which is a common assumption in various machine learning algorithms. This assumption states that the features of an object are conditionally independent given its class label.\nLet us again consider the dataset \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\), with \\(\\mathbf{x}_i \\in \\{0, 1\\}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nThe general steps of the algorithm under this alternative model are as follows:\n\nDecide the labels by tossing a coin with \\(P(y_i=1)=p\\).\nDetermine the features for \\(\\mathbf{x}\\) given \\(y\\) using the following conditional probability: \\[\nP(\\mathbf{x} = [f_1, f_2, \\ldots, f_d]|y) = \\prod_{i=1}^d(p^{y_i}_i)^{f_i}(1-p^{y_i}_i)^{1-f_i}\n\\]\n\nThe parameters in this alternative model are as follows:\n\nParameter \\(\\hat{p}\\) to decide the label: 1\nParameters for \\(P(\\mathbf{x}|y=1)\\): \\(d\\)\nParameters for \\(P(\\mathbf{x}|y=0)\\): \\(d\\)\n\nThus, the total number of parameters is given by: \\[\\begin{align*}\n    & = 1 + d + d \\\\\n    & = 2d + 1\n\\end{align*}\\]\nThe parameters are estimated using Maximum Likelihood Estimation."
  },
  {
    "objectID": "pages/naive_bayes.html#prediction-using-the-parameters",
    "href": "pages/naive_bayes.html#prediction-using-the-parameters",
    "title": "Naive Bayes",
    "section": "Prediction using the parameters",
    "text": "Prediction using the parameters\nGiven \\(\\mathbf{x}^{test}\\in\\{0,1\\}^d\\), the prediction for \\(\\hat{y}^{test}\\) is done using the following criterion:\n\\[\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) \\ge P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\n\\]\nIf the above inequality holds, then \\(\\hat{y}^{test}=1\\), otherwise \\(\\hat{y}^{test}=0\\).\nUsing Bayes’ rule, we can express \\(P(\\hat{y}^{test}=1|\\mathbf{x}^{test})\\) and \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})\\) as follows:\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)}{P(\\mathbf{x}^{test})} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test}) & = \\frac{P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)}{P(\\mathbf{x}^{test})}\n\\end{align*}\\]\nHowever, since we are only interested in the comparison of these probabilities, we can avoid calculating \\(P(\\mathbf{x}^{test})\\).\nBy solving for \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=1)*P(\\hat{y}^{test}=1)\\), we find:\n\\[\\begin{align*}\n&=P(\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]|y^{test}=1)*P(\\hat{y}^{test}=1) \\\\\n&=\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p}\n\\end{align*}\\]\nSimilarly, we can obtain \\(P(\\mathbf{x}^{test}|\\hat{y}^{test}=0)*P(\\hat{y}^{test}=0)\\).\nTherefore, we predict \\(\\hat{y}^{test}=1\\) if:\n\\[\n\\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\ge \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\]\nOtherwise, we predict \\(\\hat{y}^{test}=0\\).\nThe Naive Bayes algorithm employs two main techniques:\n\nThe Class Conditional Independence Assumption.\nUtilizing Bayes’ Rule.\n\nAs a result, this algorithm is commonly referred to as Naive Bayes.\nIn summary, Naive Bayes is a classification algorithm based on Bayes’ theorem, which assumes that the features are independent of each other given the class label. It estimates the conditional probabilities of features given the class and uses these probabilities to make predictions for new data. Despite its naive assumption, Naive Bayes has demonstrated good performance across various applications, particularly when dealing with high-dimensional data and limited training examples."
  },
  {
    "objectID": "pages/naive_bayes.html#pitfalls-of-naive-bayes",
    "href": "pages/naive_bayes.html#pitfalls-of-naive-bayes",
    "title": "Naive Bayes",
    "section": "Pitfalls of Naive Bayes",
    "text": "Pitfalls of Naive Bayes\nOne prominent issue with Naive Bayes is that if a feature is not observed in the training set but present in the testing set, the prediction probabilities for both classes become zero.\n\\[\\begin{align*}\nP(\\hat{y}^{test}=1|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^1_i)^{f_i}(1-\\hat{p}^1_i)^{1-f_i}\\right)*\\hat{p} \\\\\nP(\\hat{y}^{test}=0|\\mathbf{x}^{test} = [f_1, f_2, \\ldots, f_d]) & \\propto \\left(\\prod_{i=1}^d(\\hat{p}^0_i)^{f_i}(1-\\hat{p}^0_i)^{1-f_i}\\right)*(1-\\hat{p})\n\\end{align*}\\]\nIf any feature \\(f_i\\) was absent in the training set, it results in \\(\\hat{p}^1_i=\\hat{p}^0_i=0\\), leading to \\(P(\\hat{y}^{test}=0|\\mathbf{x}^{test})=P(\\hat{y}^{test}=1|\\mathbf{x}^{test})=0\\).\nA popular remedy for this issue is to introduce two “pseudo” data points with labels 1 and 0, respectively, into the dataset, where all their features are set to 1. This technique is also known as Laplace smoothing.\nIn brief, Laplace smoothing is a technique employed to address the zero-frequency problem in probabilistic models, particularly in text classification. It involves adding a small constant value to the count of each feature and the number of unique classes to avoid zero probability estimates, which can cause problems during model training and prediction. By incorporating this smoothing term, the model becomes more robust and better suited for handling unseen data."
  },
  {
    "objectID": "pages/naive_bayes.html#prediction-using-bayes-rule",
    "href": "pages/naive_bayes.html#prediction-using-bayes-rule",
    "title": "Naive Bayes",
    "section": "Prediction using Bayes’ Rule",
    "text": "Prediction using Bayes’ Rule\nPrediction is based on the following equation: \\[\nP(y_{test}=1|\\mathbf{x}_{test})\\propto P(\\mathbf{x}_{test}|y_{test})*P(y_{test})\n\\]\nWhere \\(P(\\mathbf{x}_{test}|y_{test})\\equiv f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_{y_{test}}, \\hat{\\boldsymbol{\\Sigma}})\\) and \\(P(y_{test})\\equiv \\hat{p}\\).\nTo predict \\(y_{test}=1\\), we compare the probabilities:\n\\[\\begin{align*}\nf(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{1} ,\\hat{\\boldsymbol{\\Sigma} }_{1} )\\hat{p} & \\geq f(\\mathbf{x}_{i} ;\\hat{\\boldsymbol{\\mu} }_{0} ,\\hat{\\boldsymbol{\\Sigma} }_{0} )(1-\\hat{p} )\\\\\ne^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )}\\hat{p} & \\geq e^{-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )} (1-\\hat{p} )\\\\\n-(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{1}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{1} )+\\log (\\hat{p} ) & \\geq -(\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )^{T}\\hat{\\boldsymbol{\\Sigma} }_{0}^{-1} (\\mathbf{x}_{i} -\\hat{\\boldsymbol{\\mu} }_{0} )+\\log (1-\\hat{p} )\n\\end{align*}\\]\nThis inequality can be expressed as a linear decision function:\n\\[\n\\left( (\\hat{\\boldsymbol{\\mu}}_1-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}^{-1} \\right)\\mathbf{x}_{test} + \\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_0 - \\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}^{-1}\\hat{\\boldsymbol{\\mu}}_1 + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\]\nThus, the decision function of Gaussian Naive Bayes is linear."
  },
  {
    "objectID": "pages/naive_bayes.html#decision-boundaries-for-different-covariances",
    "href": "pages/naive_bayes.html#decision-boundaries-for-different-covariances",
    "title": "Naive Bayes",
    "section": "Decision Boundaries for Different Covariances",
    "text": "Decision Boundaries for Different Covariances\n\nWhen the covariance matrices are equal for both classes: As previously discussed, the decision boundary is linear.\n\n\n\n\nWhen the covariance matrices are equal for both classes\n\n\n\nWhen the covariance matrices are Identity matrices for both classes: The decision boundary is both linear and the perpendicular bisector of the line drawn from \\(\\hat{\\boldsymbol{\\mu}}_1\\) to \\(\\hat{\\boldsymbol{\\mu}}_0\\).\n\n\n\n\nWhen the covariance matrices are Identity matrices for both classes\n\n\n\nWhen the covariance matrices are not equal for both classes: Let \\(\\hat{\\boldsymbol{\\Sigma}}_1\\) and \\(\\hat{\\boldsymbol{\\Sigma}}_0\\) be the covariance matrices for classes 1 and 0, respectively. They are given by, \\[\\begin{align*}\n\\hat{\\boldsymbol{\\Sigma}}_1 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)(\\mathbb{1}(y_i=1)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_1)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=1)} \\\\\n\\hat{\\boldsymbol{\\Sigma}}_0 &= \\frac{\\displaystyle \\sum_{i=1}^n(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)(\\mathbb{1}(y_i=0)*\\mathbf{x}_i-\\hat{\\boldsymbol{\\mu}}_0)^T}{\\displaystyle \\sum_{i=1}^n\\mathbb{1}(y_i=0)}\n\\end{align*}\\] To predict \\(y_{test}=1\\), we compare the probabilities: \\[\\begin{align*}\nf(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}}_1)*\\hat{p}&\\ge f(\\mathbf{x}_{test};\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\Sigma}}_0)*(1-\\hat{p}) \\\\\ne^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)}*\\hat{p}&\\ge e^{-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)}*(1-\\hat{p}) \\\\\n-(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)^T\\hat{\\boldsymbol{\\Sigma}}_1(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_1)+\\log(\\hat{p})&\\ge -(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0)^T\\hat{\\boldsymbol{\\Sigma}}_0(\\mathbf{x}_{test}-\\hat{\\boldsymbol{\\mu}}_0) + \\log(1-\\hat{p}) \\\\\n\\end{align*}\\] This inequality leads to a quadratic decision function: \\[\n\\mathbf{x}_{test}^T(\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}-2(\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}-\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1})\\mathbf{x}_{test}+(\\hat{\\boldsymbol{\\mu}}_0^T\\hat{\\boldsymbol{\\Sigma}}_0^{-1}\\hat{\\boldsymbol{\\mu}}_0-\\hat{\\boldsymbol{\\mu}}_1^T\\hat{\\boldsymbol{\\Sigma}}_1^{-1}\\hat{\\boldsymbol{\\mu}}_1) + \\log(\\frac{1-\\hat{p}}{\\hat{p}}) \\ge 0\n\\] Hence, the decision boundary is a quadratic function when the covariance matrices are not equal for both classes.\n\n\n\n\nWhen the covariance matrices are not equal for both classes"
  },
  {
    "objectID": "pages/log_reg.html",
    "href": "pages/log_reg.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Until now, we have utilized the \\(\\text{sign}\\) function to determine the class for the output. However, what if we also wish to obtain the probabilities associated with these outputs?\nLet \\(z=\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\), where \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? To address this, we introduce the Sigmoid Function, defined as follows:\n\\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is commonly employed in machine learning as an activation function for neural networks. It exhibits an S-shaped curve, making it well-suited for modeling processes with a threshold or saturation point, such as logistic growth or binary classification problems.\nFor large positive input values, the sigmoid function approaches 1, while for large negative input values, it approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” is derived from the Greek word “sigmoides,” meaning “shaped like the letter sigma” (\\(\\Sigma\\)). The sigmoid function’s characteristic S-shaped curve resembles the shape of the letter sigma, which likely influenced the function’s name."
  },
  {
    "objectID": "pages/log_reg.html#sigmoid-function",
    "href": "pages/log_reg.html#sigmoid-function",
    "title": "Logistic regression",
    "section": "",
    "text": "Until now, we have utilized the \\(\\text{sign}\\) function to determine the class for the output. However, what if we also wish to obtain the probabilities associated with these outputs?\nLet \\(z=\\mathbf{w}^\\mathbf{T}\\mathbf{x}\\), where \\(z \\in \\mathbb{R}\\). How can we map \\([-\\infty, \\infty]\\rightarrow[0,1]\\)? To address this, we introduce the Sigmoid Function, defined as follows:\n\\[\ng(z) = \\frac{1}{1+e^{-z}}\n\\]\n\n\n\nSigmoid Function\n\n\nThe sigmoid function is commonly employed in machine learning as an activation function for neural networks. It exhibits an S-shaped curve, making it well-suited for modeling processes with a threshold or saturation point, such as logistic growth or binary classification problems.\nFor large positive input values, the sigmoid function approaches 1, while for large negative input values, it approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.\nThe term “sigmoid” is derived from the Greek word “sigmoides,” meaning “shaped like the letter sigma” (\\(\\Sigma\\)). The sigmoid function’s characteristic S-shaped curve resembles the shape of the letter sigma, which likely influenced the function’s name."
  },
  {
    "objectID": "pages/log_reg.html#logistic-regression",
    "href": "pages/log_reg.html#logistic-regression",
    "title": "Logistic regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables. The independent variables can be either continuous or categorical. The main objective of logistic regression is to estimate the probability that the dependent variable belongs to one of the two possible values, given the independent variable values.\nIn logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic (sigmoid) function. This function generates an S-shaped curve ranging between 0 and 1. By transforming the output of a linear combination of the independent variables using the logistic function, logistic regression provides a probability estimate that can be used for classifying new observations.\nLet \\(D=\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n,y_n)\\}\\) denote the dataset, where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{0, 1\\}\\).\nWe know that:\n\\[\nP(y=1|\\mathbf{x}) = g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) = \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}}}\n\\]\nUsing the maximum likelihood approach, we can derive the following expression:\n\\[\\begin{align*}\n\\mathcal{L}(\\mathbf{w};\\text{Data}) &= \\prod _{i=1} ^{n} (g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{y_i}(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))^{1-y_i} \\\\\n\\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} y_i\\log(g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i))+(1-y_i)\\log(1- g(\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i)) \\\\\n&= \\sum _{i=1} ^{n} y_i\\log\\left(\\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right)+(1-y_i)\\log\\left(\\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}\\right) \\\\\n&= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\end{align*}\\]\nTherefore, our objective, which involves maximizing the log-likelihood function, can be formulated as follows:\n\\[\n\\max _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i) - \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) \\right ]\n\\]\nHowever, a closed-form solution for this problem does not exist. Therefore, we resort to using gradient descent for convergence.\nThe gradient of the log-likelihood function is computed as follows:\n\\[\\begin{align*}\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ (1-y_i)(-\\mathbf{x}_i) - \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) (-\\mathbf{x}_i) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ -\\mathbf{x}_i + \\mathbf{x}_iy_i + \\mathbf{x}_i \\left( \\frac{e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n&= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_iy_i - \\mathbf{x}_i \\left( \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ] \\\\\n\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) &= \\sum _{i=1} ^{n} \\left [ \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right ]\n\\end{align*}\\]\nUtilizing the gradient descent update rule, we obtain:\n\\[\\begin{align*}\n\\mathbf{w}_{t+1} &= \\mathbf{w}_t + \\eta_t\\nabla \\log(\\mathcal{L}(\\mathbf{w};\\text{Data})) \\\\\n&= \\mathbf{w}_t + \\eta_t  \\left ( \\sum _{i=1} ^{n} \\mathbf{x}_i \\left(y_i - \\frac{1}{1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}} \\right ) \\right )\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/log_reg.html#kernel-and-regularized-versions",
    "href": "pages/log_reg.html#kernel-and-regularized-versions",
    "title": "Logistic regression",
    "section": "Kernel and Regularized Versions",
    "text": "Kernel and Regularized Versions\nIt is possible to argue that \\(\\mathbf{w}^*=\\displaystyle\\sum _{i=1} ^{n}\\alpha_i\\mathbf{x}_i\\), thereby allowing for kernelization. For additional information, please refer to this link.\nThe regularized version of logistic regression can be expressed as follows:\n\\[\n\\min _{\\mathbf{w}}\\sum _{i=1} ^{n} \\left [ \\log(1+e^{-\\mathbf{w}^\\mathbf{T}\\mathbf{x}_i}) + \\mathbf{w}^\\mathbf{T}\\mathbf{x}_i(1-y_i) \\right ] + \\frac{\\lambda}{2}||\\mathbf{w}||^2\n\\]\nHere, \\(\\frac{\\lambda}{2}||\\mathbf{w}||^2\\) serves as the regularizer, and \\(\\lambda\\) is determined through cross-validation."
  },
  {
    "objectID": "pages/ffn.html",
    "href": "pages/ffn.html",
    "title": "Forward Propagation",
    "section": "",
    "text": "The forward pass in a neural network involves processing input data through the network’s layers to generate predictions or outputs by sequentially applying weights, biases, and activation functions.\nMathematical Representation:\nFor one training example \\(x^{(i)}\\):\n\nHidden Layer Computations:\n\nWeighted Sum Calculation:\n\n\\(z^{[l] (i)} = W^{[l]} a^{[l-1](i)} + b^{[l]}\\)\n\nCompute the weighted sum of inputs (\\(a^{[l-1](i)}\\)) by the weights (\\(W^{[l]}\\)) and add the bias (\\(b^{[l]}\\)) for layer \\(l\\).\n\n\nActivation Calculation:\n\n\\(a^{[l] (i)} = g(z^{[l] (i)})\\)\n\nApply an activation function \\(g\\) (such as sigmoid, tanh, ReLU, etc.) to the computed weighted sum to get the activation of layer \\(l\\).\n\n\n\nOutput Layer Computations:\n\nWeighted Sum Calculation:\n\n\\(z^{[L] (i)} = W^{[L]} a^{[L-1] (i)} + b^{[L]}\\)\n\nCompute the weighted sum of inputs (\\(a^{[L-1] (i)}\\)) by the weights (\\(W^{[L]}\\)) and add the bias (\\(b^{[L]}\\)) for the output layer.\n\n\nActivation Calculation:\n\n\\(\\hat{y}^{(i)} = a^{[L] (i)} = \\sigma(z^{ [L] (i)})\\)\n\nApply an appropriate activation function (e.g., softmax for multiclass classification, sigmoid for binary classification) to the computed weighted sum to obtain the predicted output (\\(\\hat{y}^{(i)}\\)).\n\n\n\nCost Function:\n\n\\(J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\left(y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right)\\right)\\)\n\nCompute the appropriate cost function (e.g., cross-entropy, mean squared error) to evaluate the difference between the predicted output (\\(a^{[L] (i)}\\)) and the actual output (\\(y^{(i)}\\)) for all training examples (\\(m\\)).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nimport numpy as np\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n    \n    Z1 = W1@X + b1\n    A1 = np.tanh(Z1)\n    Z2 = W2@A1 + b2\n    A2 = 1/(1+np.exp(-Z2))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache"
  },
  {
    "objectID": "pages/gradient.html",
    "href": "pages/gradient.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Optimizing Model Parameters: Gradient Descent is used to update the parameters (weights and biases) of a neural network in the direction that minimizes the loss function.\nGeneral Gradient Descent Rule: The general update rule for a parameter \\(\\theta\\) using Gradient Descent is: \\[\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }\\] where \\(\\alpha\\) is the learning rate and \\(\\frac{\\partial J }{ \\partial \\theta }\\) denotes the partial derivative of the cost function \\(J\\) with respect to the parameter \\(\\theta\\).\nParameter Update for Weights and Biases:\n\nOutput Layer:\n\nUpdate the weights and biases of the output layer using the gradients calculated during backpropagation:\n\n\\(W^{[L]} = W^{[L]} - \\alpha \\frac{\\partial J }{ \\partial W^{[L]} }\\)\n\\(b^{[L]} = b^{[L]} - \\alpha \\frac{\\partial J }{ \\partial b^{[L]} }\\)\n\n\nHidden Layers:\n\nUpdate the weights and biases of the hidden layers using the gradients computed during backpropagation:\n\nFor \\(l\\)th hidden layer:\n\n\\(W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial J }{ \\partial W^{[l]} }\\)\n\\(b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial J }{ \\partial b^{[l]} }\\)\n\n\n\n\nImpact of Learning Rate (\\(\\alpha\\)):\n\nThe learning rate \\(\\alpha\\) determines the step size during parameter updates.\nA larger \\(\\alpha\\) might lead to faster convergence but can overshoot the minimum.\nA smaller \\(\\alpha\\) might converge slowly but can be more precise.\n\nIteration and Convergence:\n\nIterate through multiple epochs (iterations) of forward propagation, backpropagation, and parameter updates until the model’s performance converges or the predefined number of epochs is reached.\nMonitor the decrease in the cost function to assess convergence.\n\nImportance of Gradient Descent:\n\nGradient Descent is crucial in adjusting model parameters to minimize the loss function, thereby enhancing the accuracy of predictions in a neural network.\nIt enables the network to learn from the data by iteratively updating weights and biases to improve the overall performance of the model.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nimport numpy as np\n\ndef compute_cost(A2, Y):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of examples\n\n    logprobs = np.multiply(Y, np.log(A2)) + np.multiply((1-Y), np.log(1-A2))\n    cost = - np.sum(logprobs)/Y.shape[1]\n        \n    cost = float(np.squeeze(cost)) \n\n    return cost\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    W1, b1, W2, b2 = copy.deepcopy(parameters[\"W1\"]), copy.deepcopy(parameters[\"b1\"]), copy.deepcopy(parameters[\"W2\"]), copy.deepcopy(parameters[\"b2\"])\n    \n    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n    \n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2\n        \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n        \n    parameters = initialize_parameters(n_x, n_h, n_y)\n        \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads)\n                \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters"
  },
  {
    "objectID": "pages/activations.html",
    "href": "pages/activations.html",
    "title": "Weight Initializations",
    "section": "",
    "text": "Activation functions play a crucial role in introducing non-linearity to neural networks, enabling them to model complex relationships and learn intricate patterns from the data. Here are common activation functions used in neural networks:\n\n\n\n\n\n\n\n\n\nActivation\nFormula\nGraph\nDescription\n\n\n\n\nSigmoid\n\\(a = \\frac{1}{1 + e^{-z}}\\)\n\nThe sigmoid function produces outputs between 0 and 1, resembling an S-shaped curve. It’s suitable for binary classification tasks where the output needs to be in the range [0, 1].\n\n\nTanh\n\\(a = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\)\n\nTanh, or hyperbolic tangent, produces outputs between -1 and 1. It is similar to the sigmoid function but with outputs centered around zero, making it suitable for hidden layers.\n\n\nReLU (Rectified Linear Unit)\n\\(a = \\max(0, z)\\)\n\nReLU is a simple and widely used activation function that outputs zero for negative inputs and linearly increases for positive inputs, efficiently mitigating the vanishing gradient problem.\n\n\nLeaky ReLU\n\\(a = \\max(0.01z, z)\\)\n\nLeaky ReLU is an improved version of ReLU, allowing a small gradient for negative inputs to address the “dying ReLU” problem encountered with the standard ReLU function.\n\n\n\n\nImportance of Non-linear Activation Functions:\nNeural networks with non-linear activation functions can approximate complex functions and relationships between inputs and outputs. Linear activation functions result in neural networks that are essentially just linear transformations, limiting their ability to capture intricate patterns present in real-world data.\nA detailed explanation on Stack Overflow provides insights into the necessity of non-linear activation functions in enabling neural networks to learn complex mappings.\n\n\nDerivatives of Activation Functions:\nThe derivatives of activation functions are essential for backpropagation during the training of neural networks. Here are the derivatives of common activation functions:\n\n\n\nActivation\nFormula\nDerivative\n\n\n\n\nSigmoid\n\\(a = \\frac{1}{1 + e^{-z}}\\)\n\\(a(1 - a)\\)\n\n\nTanh\n\\(a = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\)\n\\(1 - a^2\\)\n\n\nReLU\n\\(a = \\max(0, z)\\)\n0 if \\(z &lt; 0\\); 1 if \\(z \\geq 0\\)\n\n\nLeaky ReLU\n\\(a = \\max(0.01z, z)\\)\n0.01 if \\(z &lt; 0\\); 1 if \\(z \\geq 0\\)\n\n\n\nUnderstanding these derivatives is crucial in computing gradients during backpropagation, facilitating the adjustment of weights and biases for effective training of neural networks.\nThese diverse activation functions empower neural networks to handle various types of data and learn complex relationships, contributing to their adaptability and efficiency in modeling intricate patterns within datasets."
  }
]